Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Liu2014,
abstract = {{\textcopyright} 2014 The Author(s). Licensee InTech.Pose estimation methods in robotics applications frequently suffer from inaccuracy due to a lack of correspondence and real-time constraints, and instability from a wide range of viewpoints, etc. In this paper, we present a novel approach for estimating the poses of all the cameras in a multi-camera system in which each camera is placed rigidly using only a few coplanar points simultaneously. Instead of solving the orientation and translation for the multi-camera system from the overlapping point correspondences among all the cameras directly, we employ homography, which can map image points with 3D coplanar-referenced points. In our method, we first establish the corresponding relations between each camera by their Euclidean geometries and optimize the homographies of the cameras; then, we solve the orientation and translation for the optimal homographies. The results from simulations and real case experiments show that our approach is accurate and robust for implementation in robotics applications. Finally, a practical implementation in a ping-pong robot is described in order to confirm the validity of our approach.},
author = {Liu, Yong and Xiong, Rong and Li, Yi},
doi = {10.5772/58868},
file = {:home/aufar/Documents/FinalYearProject/Background Research/58868.pdf:pdf},
issn = {17298814},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Coplanar points,Multi-camera system,Ping-pong robot,Pose estimation},
title = {{Robust and accurate multiple-camera pose estimation toward robotic applications}},
volume = {11},
year = {2014}
}
@article{Carlson2012,
abstract = {Powered wheelchair users often struggle to drive safely and effectively and, in more critical cases, can only get around when accompanied by an assistant. To address these issues, we propose a collaborative control mechanism that assists users as and when they require help. The system uses a multiple-hypothesis method to predict the driver's intentions and, if necessary, adjusts the control signals to achieve the desired goal safely. The main emphasis of this paper is on a comprehensive evaluation, where we not only look at the system performance but also, perhaps more importantly, characterize the user performance in an experiment that combines eye tracking with a secondary task. Without assistance, participants experienced multiple collisions while driving around the predefined route. Conversely, when they were assisted by the collaborative controller, not only did they drive more safely but also they were able to pay less attention to their driving, resulting in a reduced cognitive workload. We discuss the importance of these results and their implications for other applications of shared control, such as brain-machine interfaces, where it could be used to compensate for both the low frequency and the low resolution of the user input.},
author = {Carlson, Tom and Demiris, Yiannis},
doi = {10.1109/TSMCB.2011.2181833},
file = {:home/aufar/Documents/FinalYearProject/Background Research/CarlsonDe2012.pdf:pdf},
isbn = {1083-4419 VO  - 42},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Collision avoidance,human factors,human robot interaction,intelligent robots,rehabilitation robotics,wheelchairs},
number = {3},
pages = {876--888},
pmid = {22275718},
title = {{Collaborative control for a robotic wheelchair: Evaluation of performance, attention, and workload}},
volume = {42},
year = {2012}
}
@article{Kairy2014,
abstract = {Power wheelchairs (PWCs) can have a positive impact on user well-being, self-esteem, pain, activity and participation. Newly developed intelligent power wheelchairs (IPWs), allowing autonomous or collaboratively-controlled navigation, could enhance mobility of individuals not able to use, or having difficulty using, standard PWCs. The objective of this study was to explore the perspectives of PWC users (PWUs) and their caregivers regarding if and how IPWs could impact on current challenges faced by PWUs, as well as inform current development of IPWs. A qualitative exploratory study using individual interviews was conducted with PWUs (n = 12) and caregivers (n = 4). A semi-structured interview guide and video were used to facilitate informed discussion regarding IPWs. Thematic analysis revealed three main themes: (1) "challenging situations that may be overcome by an IPW" described how the IPW features of obstacle avoidance, path following, and target following could alleviate PWUs' identified mobility difficulties; (2) "cautious optimism concerning IPW use revealed participants" addresses concerns regarding using an IPW as well as technological suggestions; (3) "defining the potential IPW user" revealed characteristics of PWUs that would benefit from IPW use. Findings indicate how IPW use may help overcome PWC difficulties and confirm the importance of user input in the ongoing development of IPWs.},
author = {Kairy, Dahlia and Rushton, Paula W. and Archambault, Philippe and Pituch, Evelina and Torkia, Caryne and {El Fathi}, Anas and Stone, Paula and Routhier, Fran{\c{c}}ois and Forget, Robert and Demers, Louise and Pineau, Joelle and Gourdeau, Richard},
doi = {10.3390/ijerph110202244},
file = {:home/aufar/Documents/FinalYearProject/Background Research/ijerph-11-02244.pdf:pdf},
isbn = {1660-4601},
issn = {16617827},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Disability,Intelligent power wheelchair,Mobility,Navigation,Obstacle-avoidance,Path following,Safety,User-centered design},
number = {2},
pages = {2244--2261},
pmid = {24566051},
title = {{Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair use: A qualitative study}},
volume = {11},
year = {2014}
}
@article{Biswas2012,
abstract = {The sheer volume of data generated by depth cameras provides a challenge to process in real time, in particular when used for indoor mobile robot localization and navigation. We introduce the Fast Sampling Plane Filtering (FSPF) algorithm to reduce the volume of the 3D point cloud by sampling points from the depth image, and classifying local grouped sets of points as belonging to planes in 3D (the “plane filtered” points) or points that do not correspond to planes within a specified error margin (the “outlier” points). We then introduce a localization algorithm based on an observation model that down-projects the plane filtered points on to 2D, and assigns correspondences for each point to lines in the 2D map. The full sampled point cloud (consisting of both plane filtered as well as outlier points) is processed for obstacle avoidance for autonomous navigation. All our algorithms process only the depth information, and do not require additional RGB data. The FSPF, localization and obstacle avoidance algorithms run in real time at full camera frame rates (30Hz) with low CPU requirements (16$\backslash${\%}). We provide experimental results demonstrating the effectiveness of our approach for indoor mobile robot localization and navigation. We further compare the accuracy and robustness in localization using depth cam- eras with FSPF vs. alternative approaches that simulate laser rangefinder scans from the 3D data.},
author = {Biswas, Joydeep and Veloso, Manuela},
doi = {10.1109/ICRA.2012.6224766},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}icra.2012.6224766.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1697--1702},
pmid = {7084453},
publisher = {IEEE},
title = {{Depth camera based indoor mobile robot localization and navigation}},
year = {2012}
}
@inproceedings{Wang2013,
abstract = {Under the structure of robot technology middleware(RTM), this paper presents a distributed method for mobile robot simultaneous localization and mapping(SLAM) to address the problem of 3D modeling in complex indoor environment.We integrate the image feature and depth information to establish the correspondence-based iterative closest point (ICP) algorithm for localizing the robot precisely. With the introduction of keyframe selection mechanism, a vision-based loop closure detect algorithm and tree-based network optimizer(TORO) are used to efficiently achieve globally consistent and accuracy maps during the map building. Experimental results verify the feasibility and effectiveness of the proposed algorithm in the indoor environment. {\textcopyright} 2013 IEEE.},
author = {Wang, Ke and Jia, Songmin and Guo, Bing and Li, Yuchen},
booktitle = {2013 IEEE International Conference on Information and Automation, ICIA 2013},
doi = {10.1109/ICInfA.2013.6720481},
file = {:home/aufar/Documents/FinalYearProject/Background Research/06720481.pdf:pdf},
isbn = {9781479913343},
keywords = {3D map building,RTM,SLAM,keyframe,loop closure,mobile robot},
pages = {1224--1229},
title = {{Mobile robot 3D map building based on RTM}},
year = {2013}
}
@article{Bailey2006,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Bailey, Tim and Durrant-Whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
eprint = {there is not},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Bailey-2006-Simultaneous-localization-and-mappi.pdf:pdf},
isbn = {1610-7438},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {108--117},
pmid = {1638022},
publisher = {IEEE},
title = {{Simultaneous localization and mapping (SLAM): Part II}},
volume = {13},
year = {2006}
}
@article{Mather2015,
abstract = {The Population Reference Bureau INFORMS people around the world about population, health, and the environment, and EMPOWERS them to use that information to ADVANCE the well-being of current and future generations.},
author = {Mather, Mark and Jacobsen, Linda A. and Pollard, Kelvin M.},
doi = {10.1093/oxfordhb/9780199640935.013.0031},
file = {:home/aufar/Documents/FinalYearProject/Background Research/aging-us-population-bulletin-1.pdf:pdf},
isbn = {9780191749506},
issn = {00987921},
journal = {Population Bulletin},
keywords = {Deregulation,Financial innovation,Large banks,Small banks,Technological change,Us banking industry},
number = {2},
title = {{Aging in the United States}},
volume = {70},
year = {2015}
}
@article{Zolotas2018,
author = {Zolotas, Mark and Elsdon, Joshua and Demiris, Yiannis},
file = {:home/aufar/Documents/FinalYearProject/Background Research/hololens{\_}wheelchair/hololens{\_}wheelchair{\_}iros18{\_}stamped.pdf:pdf},
title = {{Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance}},
year = {2018}
}
@article{Taketomi2017,
abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
doi = {10.1186/s41074-017-0027-2},
file = {:home/aufar/Documents/FinalYearProject/Background Research/s41074-017-0027-2.pdf:pdf},
isbn = {81-7319-221-9},
issn = {1882-6695},
journal = {IPSJ Transactions on Computer Vision and Applications},
keywords = {augmented reality,computer vision,robotics,survey,visual slam},
number = {1},
pages = {16},
publisher = {IPSJ Transactions on Computer Vision and Applications},
title = {{Visual SLAM algorithms: a survey from 2010 to 2016}},
url = {http://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2},
volume = {9},
year = {2017}
}
@article{Wang2011,
abstract = {This paper presents algorithms for improving the detection of moving objects in robot visual simultaneous localization and mapping (SLAM). The method of speeded-up robust feature (SURF) is employed in the algorithm to provide a robust detection for image features as well as a better description of landmarks in the map of a visual SLAM system. Meanwhile, a moving object detection (MOD) is designed based on the correspondence constraint of the essential matrix for the feature points on image plane. Experiments are carried out on a handheld camera sensor to verify the performances of the proposed algorithms. The results show that the integration of SURF and MOD is efficient to improve the robustness of robot SLAM.},
author = {Wang, Yin Tien and Feng, Ying Chieh and Hung, Duen Yan},
doi = {10.1109/IMTC.2011.5944059},
file = {:home/aufar/Documents/FinalYearProject/Background Research/05944059.pdf:pdf},
isbn = {9781424479351},
issn = {10915281},
journal = {Conference Record - IEEE Instrumentation and Measurement Technology Conference},
keywords = {Moving Object Detection (MOD),Simultaneous Localization and Mapping (SLAM),Speeded Up Robust Features (SURF),Vision Sensor},
pages = {1078--1082},
publisher = {IEEE},
title = {{Detection and tracking of moving objects in SLAM using vision sensors}},
year = {2011}
}
@article{Fox1997,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fox, Dieter and Burgard, Wolfram and Thrun, Sebastian},
doi = {10.1109/100.580977},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/Documents/FinalYearProject/Background Research/00580977.pdf:pdf},
isbn = {1070-9932 VO - 4},
issn = {10709932},
journal = {IEEE Robotics {\&} Automation Magazine ( Volume: 4 , Issue: 1 , Mar 1997 )},
number = {March},
pages = {23--33},
pmid = {568982},
title = {{The Dynamic Window Approach to Collision Avoidance}},
url = {https://ieeexplore.ieee.org/abstract/document/580977/authors{\#}authors},
volume = {4},
year = {1997}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that canbe usedtoperformreliablematchingbetweendifferent views of an object or scene. The features are invariant to image scale and rotation, and are shown toprovide robust matching across a a substantial range ofaffine dis- tortion, change in 3Dviewpoint, addition ofnoise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be cor- rectly matched with high probability against a large database offeatures from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual fea- tures to a database offeatures fromknown objects using a fast nearest-neighbor algorithm, followedbyaHoughtransformtoidentifyclustersbelonging toasin- gle object, andfinallyperforming verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects amongclutter andocclusionwhileachievingnear real-timeperformance},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G},
doi = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
eprint = {0112017},
file = {:home/aufar/Documents/FinalYearProject/Background Research/ijcv04.pdf:pdf},
isbn = {1568811012},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {SIFT},
pages = {1--28},
pmid = {20064111},
primaryClass = {cs},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
year = {2004}
}
@article{Chacon-Quesada,
abstract = {In this paper we present a novel augmented reality head mounted display user interface for controlling a robotic wheelchair for people with limited mobility. To lower the cognitive requirements needed to control the wheelchair, we propose integration of a smart wheelchair with an eye-tracking enabled head-mounted display. We propose a novel platform that integrates multiple user interface interaction methods for aiming at and selecting affordances derived by on-board perception capabilities such as laser-scanner readings and cameras. We demonstrate the effectiveness of the approach by evaluating our platform in two realistic scenarios: 1) Door detection, where the affordance corresponds to a Door object and the Go-Through action and 2) People detection, where the affordance corresponds to a Person and the Approach action. To the best of our knowledge, this is the first demonstration of a augmented reality head-mounted display user interface for controlling a smart wheelchair.},
author = {Chac{\'{o}}n-Quesada, Rodrigo and Demiris, Yiannis},
file = {:home/aufar/Documents/FinalYearProject/Background Research/14f-augmented-reality-control.pdf:pdf},
keywords = {Index terms-Affordances,augmented reality,head mounted display,smart wheelchairs,user interface},
pages = {1--4},
title = {{Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances}},
url = {http://www.imperial.ac.uk/personal-robotics/robots/}
}
@article{Hasanuzzaman2006,
abstract = {Achieving natural interactions by means of vision and speech between humans and robots is one of the major goals that many researchers are working on. This paper aims to describe a gesture-based human-robot interaction (HRI) system using a knowledge-based software platform. A frame-based knowledge model is defined for the gesture interpretation and HRI. In this knowledge model, necessary frames are defined for the known users, robots, poses, gestures and robot behaviors. First, the system identifies the user using the eigenface method. Then, face and hand poses are segmented from the camera frame buffer using the person's specific skin color information and classified by the subspace method. The system is capable of recognizing static gestures comprised of the face and hand poses, and dynamic gestures of face in motion. The system combines computer vision and knowledge-based approaches in order to improve the adaptability to different people.},
author = {Hasanuzzaman, Md and Zhang, T. and Ampornaramveth, V. and Ueno, H.},
doi = {10.1108/01439910610638216},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Gesture-based human-robot interaction using a
knowledge-based software platform.pdf:pdf},
issn = {0143991X},
journal = {Industrial Robot},
keywords = {Man machine interface,Robotics},
number = {1 SPEC. ISS.},
pages = {37--49},
title = {{Gesture-based human-robot interaction using a knowledge-based software platform}},
volume = {33},
year = {2006}
}
@article{Emharraf2015,
abstract = {this paper presents an approach for unknown indoor environment exploration using a simultaneous localization and mapping system. The approach addresses the problem of unknown indoor environments exploration, based on robot mobile moving and sonar scanning. The measurements given by the localization system (odometry for the test system), update for the robot self-localization. The map building process maintaining two grid maps: (1) map grid models the environment occupancy (OM), (2) map grid memorize the robot trajectory(TM). The use of two grid maps provides an efficacy description and use of the environment information over time. Results in simulation and real robots experiments using random exploration show the fusibility of our approach.},
author = {Emharraf, Mohamed and Rahmoun, Mohammed and Saber, Mohammed and Azizi, Mostafa},
doi = {10.1109/EITech.2015.7162945},
file = {:home/aufar/Documents/FinalYearProject/Background Research/07162945.pdf:pdf},
isbn = {9781479974795},
journal = {Proceedings of 2015 International Conference on Electrical and Information Technologies, ICEIT 2015},
keywords = {autonomous robot mobile,map building,self-localization,unknown indoor environment},
pages = {1--6},
publisher = {IEEE},
title = {{Mobile robot: Simultaneous localization and mapping of unknown indoor environment}},
year = {2015}
}
@article{WorldHealthOrganisation&TheWorldbank2011,
abstract = {http://www.larchetoronto.org/wordpress/wp-content/uploads/2012/01/launch-of-World-Report-on-Disability-Jan-27-121.pdf},
archivePrefix = {arXiv},
arxivId = {NBK304079},
author = {{World Health Organisation {\&} The World bank}},
doi = {10.1080/09687599.2011.589198},
eprint = {NBK304079},
file = {:home/aufar/Documents/FinalYearProject/Background Research/9789240685215{\_}eng.pdf:pdf},
isbn = {9241564180},
issn = {09687599},
journal = {Disability and Society},
keywords = {Convention on the rights of persons with disabilit,Disability data,Disability politics,World health organization,World report on disability},
number = {5},
pages = {655--658},
pmid = {21780912},
title = {{The world report on disability}},
volume = {26},
year = {2011}
}
@article{Bailey2006a,
author = {Bailey, T and Durrant-Whyte, H},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Durrant-Whyte{\_}Bailey{\_}SLAM-tutorial-I.pdf:pdf},
journal = {IEEE Robotics and Automation Magazine},
number = {2},
pages = {99--108},
title = {{Simultaneous localisation and mapping (SLAM): Part I - The essential algorithms.}},
volume = {13},
year = {2006}
}
@article{Cadena2016,
abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
archivePrefix = {arXiv},
arxivId = {1606.05830},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830},
file = {:home/aufar/Documents/FinalYearProject/Background Research/1606.05830v3.pdf:pdf},
isbn = {9781479936847},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
pmid = {6576973927449638915},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
volume = {32},
year = {2016}
}
@article{Quattoni2009,
abstract = {Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task.},
author = {Quattoni, Ariadna and Torralba, Antonio},
doi = {10.1109/CVPR.2009.5206537},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}CVPR.2009.5206537.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {00208868},
journal = {International Surgery},
number = {3},
pages = {182--186},
publisher = {IEEE},
title = {{Recognizing Indoor Scenes}},
volume = {56},
year = {2009}
}
@article{Fransen2010,
abstract = {Human-robot interaction necessitates more than robust people detection and tracking. It relies on the integration of disparate scene information from tracking and recognition systems combined and infused with current and prior knowledge to facililtate robotic understanding and interaction with humans and the environment. In this work we will discuss our efforts in the development and integration of visual scene processing systems for the purpose of enhancing human robotic interaction. Our latest efforts in integrating 3D scene information for the production of novel information sources will be discussed and demonstrated. We show the integration of facial pose and pointing gestures to localize the diectic gesture to a single point in space. Additionally, we will discuss our efforts in integrating Markov logic networks for high level reasoning with computer vision systems to facilitate scene understanding.},
author = {Fransen, Benjamin R. and Lawson, Wallace E. and Bugajska, Magdalena D.},
doi = {10.1109/CVPRW.2010.5543749},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Integrating Vision for Human-Robot Interaction/05543749.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
pages = {9--16},
publisher = {IEEE},
title = {{Integrating vision for human-robot interaction}},
year = {2010}
}
@article{Versini2017,
author = {Versini, Alessandro},
file = {:home/aufar/Documents/FinalYearProject/Background Research/av2013.pdf:pdf},
journal = {Electronic Engineering},
pages = {1--67},
title = {{Augmented Reality for Driving Applications}},
year = {2017}
}
@inproceedings{Leonard,
abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of `which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning},
author = {Leonard, J.J. and Durrant-Whyte, H.F.},
booktitle = {Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91},
doi = {10.1109/IROS.1991.174711},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}iros.1991.174711.pdf:pdf},
isbn = {0-7803-0067-X},
issn = {1476-5535},
pages = {1442--1447},
pmid = {21086107},
title = {{Simultaneous map building and localization for an autonomous mobile robot}},
url = {http://ieeexplore.ieee.org/document/174711/}
}
@article{Nister2004,
abstract = {DAVID AND HENRIK STEWENIUS Department of Computer Science, Center for Visualization and Virtual Environments, University of Kentucky, Lexington, KY},
author = {Nist{\'{e}}r, David},
doi = {10.1109/CVPR.2004.1315081},
file = {:home/aufar/Documents/FinalYearProject/Background Research/01315081.pdf:pdf},
isbn = {0769521584},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
number = {1},
pages = {560--567},
title = {{A Minimal Solution to the Generalised 3-Point Pose Problem}},
volume = {27},
year = {2004}
}
@unpublished{Leutenegger2019,
author = {Leutenegger, Stefan},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Representations and Sensors.pdf:pdf},
institution = {Imperial College London},
isbn = {9780470517062},
title = {{Representations and Sensors}},
year = {2019}
}
@article{Martinez-Martin2017,
abstract = {Technological advances are being made to assist humans in performing ordinary tasks in everyday settings. A key issue is the interaction with objects of varying size, shape, and degree of mobility. Autonomous assistive robots must be provided with the ability to process visual data in real time so that they can react adequately for quickly adapting to changes in the environment. Reliable object detection and recognition is usually a necessary early step to achieve this goal. In spite of significant research achievements, this issue still remains a challenge when real-life scenarios are considered. In this article, we present a vision system for assistive robots that is able to detect and recognize objects from a visual input in ordinary environments in real time. The system computes color, motion, and shape cues, combining them in a probabilistic manner to accurately achieve object detection and recognition, taking some inspiration from vision science. In addition, with the purpose of processing the input visual data in real time, a graphical processing unit (GPU) has been employed. The presented approach has been implemented and evaluated on a humanoid robot torso located at realistic scenarios. For further experimental validation, a public image repository for object recognition has been used, allowing a quantitative comparison with respect to other state-of-the-art techniques when realworld scenes are considered. Finally, a temporal analysis of the performance is provided with respect to image resolution and the number of target objects in the scene. {\textcopyright} 1994-2011 IEEE.},
author = {Martinez-Martin, Ester and {Del Pobil}, Angel P.},
doi = {10.1109/MRA.2016.2615329},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Object Detection and Recognition for Assistive Robots.pdf:pdf},
isbn = {9780470976371},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {123--138},
title = {{Object detection and recognition for assistive robots: Experimentation and implementation}},
volume = {24},
year = {2017}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Redmon{\_}You{\_}Only{\_}Look{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Baldini2017,
author = {Baldini, Filippo},
file = {:home/aufar/Documents/FinalYearProject/Background Research/fb1713.pdf:pdf},
journal = {Electronic Engineering},
pages = {1--67},
title = {{Machine Learning for Humanoid Robot Programming through Virtual Reality Headsets}},
year = {2017}
}
@misc{Vidanapathirana,
author = {Vidanapathirana, Madhawa},
title = {{Real-time Human Detection in Computer Vision — Part 2}},
url = {https://medium.com/@madhawavidanapathirana/real-time-human-detection-in-computer-vision-part-2-c7eda27115c6},
urldate = {2019-01-25},
year = {2018}
}
@article{Shinde2018,
abstract = {Human action recognition in video analytics has been widely studied in recent years. Yet, most of these methods assign a single action label to video after either analyzing a complete video or using classifier for each frame. But when compared to human vision strategy, it can be deduced that we (human) require just an instance of visual data for recognition of scene. It turns out that small group of frames or even single frame from the video are enough for precise recognition. In this paper, we present an approach to detect, localize and recognize actions of interest in almost real-time from frames obtained by a continuous stream of video data that can be captured from a surveillance camera. The model takes input frames after a specified period and is able to give action label based on a single frame. Combining results over specific time we predicted the action label for the stream of video. We demonstrate that YOLO is effective method and comparatively fast for recognition and localization in Liris Human Activities dataset.},
author = {Shinde, Shubham and Kothari, Ashwin and Gupta, Vikram},
doi = {10.1016/j.procs.2018.07.112},
file = {:home/aufar/Documents/FinalYearProject/Background Research/1-s2.0-S1877050918310652-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Convolutional Neural Network,Liris Human Activities dataset,You Only Look Once (YOLO),action label,human action recognition,video analytics},
number = {2018},
pages = {831--838},
publisher = {Elsevier B.V.},
title = {{YOLO based Human Action Recognition and Localization}},
url = {https://doi.org/10.1016/j.procs.2018.07.112},
volume = {133},
year = {2018}
}
@article{Dalal2004,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, Navneet and Triggs, William},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
file = {:home/aufar/Documents/FinalYearProject/Background Research/01467360.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {1063-6919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR05},
number = {3},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of Oriented Gradients for Human Detection}},
volume = {1},
year = {2004}
}
@article{Kanaujia,
abstract = {Recent studies based upon the output of the ECMWF operational forecasting model indicates that if, after the first day of a forecast, a perfect model could be substituted for the present model, forecasts as good as these presently produced at seven days would be realized at ten days. These studies do not reveal how much improvement in one-day forecasting is possible. The author hypothesises that if all other imperfections in the forecasting procedure could be removed, the inevitable initial uncertainties in observing the small-scale features would, after D days, lead to error fields with amplitudes and spectra resembling those of the errors in present one-day forecasts. The appropriate value of D is highly dependent upon the spectrum of actual atmospheric motions. Estimates with a crude model place D at about four days, thereby implying that the present forecasting success at one week may some day be realized at nearly two weeks (6 Refs.)},
author = {Kanaujia, Atul and Wang, Ping and Haering, Niels},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Markov{\_}Logic{\_}Networks{\_}For{\_}Scene{\_}Interpretation{\_}And{\_}Complex{\_}Event{\_}Recognition{\_}In{\_}Videos.pdf:pdf},
isbn = {0094-243X},
title = {{Markov Logic Networks for Scene Interpretation and Complex Event Recognition in Videos}}
}
@article{Espinace2010,
abstract = {Scene recognition is a highly valuable perceptual ability for an indoor$\backslash$nmobile robot, however, current approaches for scene recognition present$\backslash$na significant drop in performance for the case of indoor scenes.$\backslash$nWe believe that this can be explained by the high appearance variability$\backslash$nof indoor environments. This stresses the need to include high-level$\backslash$nsemantic information in the recognition process. In this work we$\backslash$npropose a new approach for indoor scene recognition based on a generative$\backslash$nprobabilistic hierarchical model that uses common objects as an intermediate$\backslash$nsemantic representation. Under this model, we use object classifiers$\backslash$nto associate low-level visual features to objects, and at the same$\backslash$ntime, we use contextual relations to associate objects to scenes.$\backslash$nAs a further contribution, we improve the performance of current$\backslash$nstate-of-the-art category-level object classifiers by including geometrical$\backslash$ninformation obtained from a 3D range sensor that facilitates the$\backslash$nimplementation of a focus of attention mechanism within a Monte Carlo$\backslash$nsampling scheme. We test our approach using real data, showing significant$\backslash$nadvantages with respect to previous state-of-the-art methods.},
author = {Espinace, P. and Kollar, T. and Soto, A. and Roy, N.},
doi = {10.1109/ROBOT.2010.5509682},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}robot.2010.5509682.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1406--1413},
publisher = {IEEE},
title = {{Indoor scene recognition through object detection}},
year = {2010}
}
@article{Thrun2005,
abstract = {Probabilistic robotics is a new and growing area in robotics, concerned with perception and control in the face of uncertainty. Building on the field of mathematical statistics, probabilistic robotics endows robots with a new level of robustness in real-world situations.{\textless}br {\textless}br This book introduces the reader to a wealth of techniques and algorithms in the field. All algorithms are based on a single overarching mathematical foundation. Each chapter provides example implementations in pseudo code, detailed mathematical derivations, discussions from a practitioner's perspective, and extensive lists of exercises and class projects. The book's Web site, http://www.probabilistic-robotics.org, has additional material.{\textless}br {\textless}br The book is relevant for anyone involved in robotic software development and scientific research. It will also be of interest to applied statisticians and engineers dealing with real-world sensor data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
doi = {10.1145/504729.504754},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/Documents/FinalYearProject/Background Research/ProbabilisticRobotics.pdf:pdf},
isbn = {0262201623},
issn = {00010782},
journal = {Intelligent robotics and autonomous agents, The MIT {\ldots}},
number = {3},
pages = {52},
pmid = {20926156},
title = {{Probabilistic robotics (intelligent robotics and autonomous agents series)}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Probabilistic+Robotics+(Intelligent+Robotics+and+Autonomous+Agents){\#}0},
volume = {45},
year = {2005}
}
