Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Carlson2012,
abstract = {Powered wheelchair users often struggle to drive safely and effectively and, in more critical cases, can only get around when accompanied by an assistant. To address these issues, we propose a collaborative control mechanism that assists users as and when they require help. The system uses a multiple-hypothesis method to predict the driver's intentions and, if necessary, adjusts the control signals to achieve the desired goal safely. The main emphasis of this paper is on a comprehensive evaluation, where we not only look at the system performance but also, perhaps more importantly, characterize the user performance in an experiment that combines eye tracking with a secondary task. Without assistance, participants experienced multiple collisions while driving around the predefined route. Conversely, when they were assisted by the collaborative controller, not only did they drive more safely but also they were able to pay less attention to their driving, resulting in a reduced cognitive workload. We discuss the importance of these results and their implications for other applications of shared control, such as brain-machine interfaces, where it could be used to compensate for both the low frequency and the low resolution of the user input.},
author = {Carlson, Tom and Demiris, Yiannis},
doi = {10.1109/TSMCB.2011.2181833},
file = {:home/aufar/Documents/FinalYearProject/Background Research/CarlsonDe2012.pdf:pdf},
isbn = {1083-4419 VO  - 42},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Collision avoidance,human factors,human robot interaction,intelligent robots,rehabilitation robotics,wheelchairs},
number = {3},
pages = {876--888},
pmid = {22275718},
title = {{Collaborative control for a robotic wheelchair: Evaluation of performance, attention, and workload}},
volume = {42},
year = {2012}
}
@article{Kairy2014,
abstract = {Power wheelchairs (PWCs) can have a positive impact on user well-being, self-esteem, pain, activity and participation. Newly developed intelligent power wheelchairs (IPWs), allowing autonomous or collaboratively-controlled navigation, could enhance mobility of individuals not able to use, or having difficulty using, standard PWCs. The objective of this study was to explore the perspectives of PWC users (PWUs) and their caregivers regarding if and how IPWs could impact on current challenges faced by PWUs, as well as inform current development of IPWs. A qualitative exploratory study using individual interviews was conducted with PWUs (n = 12) and caregivers (n = 4). A semi-structured interview guide and video were used to facilitate informed discussion regarding IPWs. Thematic analysis revealed three main themes: (1) "challenging situations that may be overcome by an IPW" described how the IPW features of obstacle avoidance, path following, and target following could alleviate PWUs' identified mobility difficulties; (2) "cautious optimism concerning IPW use revealed participants" addresses concerns regarding using an IPW as well as technological suggestions; (3) "defining the potential IPW user" revealed characteristics of PWUs that would benefit from IPW use. Findings indicate how IPW use may help overcome PWC difficulties and confirm the importance of user input in the ongoing development of IPWs.},
author = {Kairy, Dahlia and Rushton, Paula W. and Archambault, Philippe and Pituch, Evelina and Torkia, Caryne and {El Fathi}, Anas and Stone, Paula and Routhier, Fran{\c{c}}ois and Forget, Robert and Demers, Louise and Pineau, Joelle and Gourdeau, Richard},
doi = {10.3390/ijerph110202244},
file = {:home/aufar/Documents/FinalYearProject/Background Research/ijerph-11-02244.pdf:pdf},
isbn = {1660-4601},
issn = {16617827},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Disability,Intelligent power wheelchair,Mobility,Navigation,Obstacle-avoidance,Path following,Safety,User-centered design},
number = {2},
pages = {2244--2261},
pmid = {24566051},
title = {{Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair use: A qualitative study}},
volume = {11},
year = {2014}
}
@article{Biswas2012,
abstract = {The sheer volume of data generated by depth cameras provides a challenge to process in real time, in particular when used for indoor mobile robot localization and navigation. We introduce the Fast Sampling Plane Filtering (FSPF) algorithm to reduce the volume of the 3D point cloud by sampling points from the depth image, and classifying local grouped sets of points as belonging to planes in 3D (the “plane filtered” points) or points that do not correspond to planes within a specified error margin (the “outlier” points). We then introduce a localization algorithm based on an observation model that down-projects the plane filtered points on to 2D, and assigns correspondences for each point to lines in the 2D map. The full sampled point cloud (consisting of both plane filtered as well as outlier points) is processed for obstacle avoidance for autonomous navigation. All our algorithms process only the depth information, and do not require additional RGB data. The FSPF, localization and obstacle avoidance algorithms run in real time at full camera frame rates (30Hz) with low CPU requirements (16$\backslash${\%}). We provide experimental results demonstrating the effectiveness of our approach for indoor mobile robot localization and navigation. We further compare the accuracy and robustness in localization using depth cam- eras with FSPF vs. alternative approaches that simulate laser rangefinder scans from the 3D data.},
author = {Biswas, Joydeep and Veloso, Manuela},
doi = {10.1109/ICRA.2012.6224766},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}icra.2012.6224766.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1697--1702},
pmid = {7084453},
publisher = {IEEE},
title = {{Depth camera based indoor mobile robot localization and navigation}},
year = {2012}
}
@inproceedings{Wang2013,
abstract = {Under the structure of robot technology middleware(RTM), this paper presents a distributed method for mobile robot simultaneous localization and mapping(SLAM) to address the problem of 3D modeling in complex indoor environment.We integrate the image feature and depth information to establish the correspondence-based iterative closest point (ICP) algorithm for localizing the robot precisely. With the introduction of keyframe selection mechanism, a vision-based loop closure detect algorithm and tree-based network optimizer(TORO) are used to efficiently achieve globally consistent and accuracy maps during the map building. Experimental results verify the feasibility and effectiveness of the proposed algorithm in the indoor environment. {\textcopyright} 2013 IEEE.},
author = {Wang, Ke and Jia, Songmin and Guo, Bing and Li, Yuchen},
booktitle = {2013 IEEE International Conference on Information and Automation, ICIA 2013},
doi = {10.1109/ICInfA.2013.6720481},
file = {:home/aufar/Documents/FinalYearProject/Background Research/06720481.pdf:pdf},
isbn = {9781479913343},
keywords = {3D map building,RTM,SLAM,keyframe,loop closure,mobile robot},
pages = {1224--1229},
title = {{Mobile robot 3D map building based on RTM}},
year = {2013}
}
@article{Bailey2006,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Bailey, Tim and Durrant-Whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
eprint = {there is not},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Bailey-2006-Simultaneous-localization-and-mappi.pdf:pdf},
isbn = {1610-7438},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {108--117},
pmid = {1638022},
publisher = {IEEE},
title = {{Simultaneous localization and mapping (SLAM): Part II}},
volume = {13},
year = {2006}
}
@article{Zolotas2018,
author = {Zolotas, Mark and Elsdon, Joshua and Demiris, Yiannis},
file = {:home/aufar/Documents/FinalYearProject/Background Research/hololens{\_}wheelchair/hololens{\_}wheelchair{\_}iros18{\_}stamped.pdf:pdf},
title = {{Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance}},
year = {2018}
}
@article{Fox1997,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fox, Dieter and Burgard, Wolfram and Thrun, Sebastian},
doi = {10.1109/100.580977},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/Documents/FinalYearProject/Background Research/00580977.pdf:pdf},
isbn = {1070-9932 VO - 4},
issn = {10709932},
journal = {IEEE Robotics {\&} Automation Magazine ( Volume: 4 , Issue: 1 , Mar 1997 )},
number = {March},
pages = {23--33},
pmid = {568982},
title = {{The Dynamic Window Approach to Collision Avoidance}},
url = {https://ieeexplore.ieee.org/abstract/document/580977/authors{\#}authors},
volume = {4},
year = {1997}
}
@article{Chacon-Quesada,
abstract = {In this paper we present a novel augmented reality head mounted display user interface for controlling a robotic wheelchair for people with limited mobility. To lower the cognitive requirements needed to control the wheelchair, we propose integration of a smart wheelchair with an eye-tracking enabled head-mounted display. We propose a novel platform that integrates multiple user interface interaction methods for aiming at and selecting affordances derived by on-board perception capabilities such as laser-scanner readings and cameras. We demonstrate the effectiveness of the approach by evaluating our platform in two realistic scenarios: 1) Door detection, where the affordance corresponds to a Door object and the Go-Through action and 2) People detection, where the affordance corresponds to a Person and the Approach action. To the best of our knowledge, this is the first demonstration of a augmented reality head-mounted display user interface for controlling a smart wheelchair.},
author = {Chac{\'{o}}n-Quesada, Rodrigo and Demiris, Yiannis},
file = {:home/aufar/Documents/FinalYearProject/Background Research/14f-augmented-reality-control.pdf:pdf},
keywords = {Index terms-Affordances,augmented reality,head mounted display,smart wheelchairs,user interface},
pages = {1--4},
title = {{Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances}},
url = {http://www.imperial.ac.uk/personal-robotics/robots/}
}
@article{Hasanuzzaman2006,
abstract = {Achieving natural interactions by means of vision and speech between humans and robots is one of the major goals that many researchers are working on. This paper aims to describe a gesture-based human-robot interaction (HRI) system using a knowledge-based software platform. A frame-based knowledge model is defined for the gesture interpretation and HRI. In this knowledge model, necessary frames are defined for the known users, robots, poses, gestures and robot behaviors. First, the system identifies the user using the eigenface method. Then, face and hand poses are segmented from the camera frame buffer using the person's specific skin color information and classified by the subspace method. The system is capable of recognizing static gestures comprised of the face and hand poses, and dynamic gestures of face in motion. The system combines computer vision and knowledge-based approaches in order to improve the adaptability to different people.},
author = {Hasanuzzaman, Md and Zhang, T. and Ampornaramveth, V. and Ueno, H.},
doi = {10.1108/01439910610638216},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Gesture-based human-robot interaction using a
knowledge-based software platform.pdf:pdf},
issn = {0143991X},
journal = {Industrial Robot},
keywords = {Man machine interface,Robotics},
number = {1 SPEC. ISS.},
pages = {37--49},
title = {{Gesture-based human-robot interaction using a knowledge-based software platform}},
volume = {33},
year = {2006}
}
@article{Emharraf2015,
abstract = {this paper presents an approach for unknown indoor environment exploration using a simultaneous localization and mapping system. The approach addresses the problem of unknown indoor environments exploration, based on robot mobile moving and sonar scanning. The measurements given by the localization system (odometry for the test system), update for the robot self-localization. The map building process maintaining two grid maps: (1) map grid models the environment occupancy (OM), (2) map grid memorize the robot trajectory(TM). The use of two grid maps provides an efficacy description and use of the environment information over time. Results in simulation and real robots experiments using random exploration show the fusibility of our approach.},
author = {Emharraf, Mohamed and Rahmoun, Mohammed and Saber, Mohammed and Azizi, Mostafa},
doi = {10.1109/EITech.2015.7162945},
file = {:home/aufar/Documents/FinalYearProject/Background Research/07162945.pdf:pdf},
isbn = {9781479974795},
journal = {Proceedings of 2015 International Conference on Electrical and Information Technologies, ICEIT 2015},
keywords = {autonomous robot mobile,map building,self-localization,unknown indoor environment},
pages = {1--6},
publisher = {IEEE},
title = {{Mobile robot: Simultaneous localization and mapping of unknown indoor environment}},
year = {2015}
}
@article{Cadena2016,
abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
archivePrefix = {arXiv},
arxivId = {1606.05830},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830},
file = {:home/aufar/Documents/FinalYearProject/Background Research/1606.05830v3.pdf:pdf},
isbn = {9781479936847},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
pmid = {6576973927449638915},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
volume = {32},
year = {2016}
}
@article{Quattoni2009,
abstract = {Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task.},
author = {Quattoni, Ariadna and Torralba, Antonio},
doi = {10.1109/CVPR.2009.5206537},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}CVPR.2009.5206537.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {00208868},
journal = {International Surgery},
number = {3},
pages = {182--186},
publisher = {IEEE},
title = {{Recognizing Indoor Scenes}},
volume = {56},
year = {2009}
}
@article{Fransen2010,
abstract = {Human-robot interaction necessitates more than robust people detection and tracking. It relies on the integration of disparate scene information from tracking and recognition systems combined and infused with current and prior knowledge to facililtate robotic understanding and interaction with humans and the environment. In this work we will discuss our efforts in the development and integration of visual scene processing systems for the purpose of enhancing human robotic interaction. Our latest efforts in integrating 3D scene information for the production of novel information sources will be discussed and demonstrated. We show the integration of facial pose and pointing gestures to localize the diectic gesture to a single point in space. Additionally, we will discuss our efforts in integrating Markov logic networks for high level reasoning with computer vision systems to facilitate scene understanding.},
author = {Fransen, Benjamin R. and Lawson, Wallace E. and Bugajska, Magdalena D.},
doi = {10.1109/CVPRW.2010.5543749},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Integrating Vision for Human-Robot Interaction/05543749.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
pages = {9--16},
publisher = {IEEE},
title = {{Integrating vision for human-robot interaction}},
year = {2010}
}
@article{Versini2017,
author = {Versini, Alessandro},
file = {:home/aufar/Documents/FinalYearProject/Background Research/av2013.pdf:pdf},
journal = {Electronic Engineering},
pages = {1--67},
title = {{Augmented Reality for Driving Applications}},
year = {2017}
}
@inproceedings{Leonard,
abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of `which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning},
author = {Leonard, J.J. and Durrant-Whyte, H.F.},
booktitle = {Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91},
doi = {10.1109/IROS.1991.174711},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}iros.1991.174711.pdf:pdf},
isbn = {0-7803-0067-X},
issn = {1476-5535},
pages = {1442--1447},
pmid = {21086107},
title = {{Simultaneous map building and localization for an autonomous mobile robot}},
url = {http://ieeexplore.ieee.org/document/174711/}
}
@article{Martinez-Martin2017,
abstract = {Technological advances are being made to assist humans in performing ordinary tasks in everyday settings. A key issue is the interaction with objects of varying size, shape, and degree of mobility. Autonomous assistive robots must be provided with the ability to process visual data in real time so that they can react adequately for quickly adapting to changes in the environment. Reliable object detection and recognition is usually a necessary early step to achieve this goal. In spite of significant research achievements, this issue still remains a challenge when real-life scenarios are considered. In this article, we present a vision system for assistive robots that is able to detect and recognize objects from a visual input in ordinary environments in real time. The system computes color, motion, and shape cues, combining them in a probabilistic manner to accurately achieve object detection and recognition, taking some inspiration from vision science. In addition, with the purpose of processing the input visual data in real time, a graphical processing unit (GPU) has been employed. The presented approach has been implemented and evaluated on a humanoid robot torso located at realistic scenarios. For further experimental validation, a public image repository for object recognition has been used, allowing a quantitative comparison with respect to other state-of-the-art techniques when realworld scenes are considered. Finally, a temporal analysis of the performance is provided with respect to image resolution and the number of target objects in the scene. {\textcopyright} 1994-2011 IEEE.},
author = {Martinez-Martin, Ester and {Del Pobil}, Angel P.},
doi = {10.1109/MRA.2016.2615329},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Object Detection and Recognition for Assistive Robots.pdf:pdf},
isbn = {9780470976371},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {123--138},
title = {{Object detection and recognition for assistive robots: Experimentation and implementation}},
volume = {24},
year = {2017}
}
@article{Baldini2017,
author = {Baldini, Filippo},
file = {:home/aufar/Documents/FinalYearProject/Background Research/fb1713.pdf:pdf},
journal = {Electronic Engineering},
pages = {1--67},
title = {{Machine Learning for Humanoid Robot Programming through Virtual Reality Headsets}},
year = {2017}
}
@article{Kanaujia,
abstract = {Recent studies based upon the output of the ECMWF operational forecasting model indicates that if, after the first day of a forecast, a perfect model could be substituted for the present model, forecasts as good as these presently produced at seven days would be realized at ten days. These studies do not reveal how much improvement in one-day forecasting is possible. The author hypothesises that if all other imperfections in the forecasting procedure could be removed, the inevitable initial uncertainties in observing the small-scale features would, after D days, lead to error fields with amplitudes and spectra resembling those of the errors in present one-day forecasts. The appropriate value of D is highly dependent upon the spectrum of actual atmospheric motions. Estimates with a crude model place D at about four days, thereby implying that the present forecasting success at one week may some day be realized at nearly two weeks (6 Refs.)},
author = {Kanaujia, Atul and Wang, Ping and Haering, Niels},
file = {:home/aufar/Documents/FinalYearProject/Background Research/Markov{\_}Logic{\_}Networks{\_}For{\_}Scene{\_}Interpretation{\_}And{\_}Complex{\_}Event{\_}Recognition{\_}In{\_}Videos.pdf:pdf},
isbn = {0094-243X},
title = {{Markov Logic Networks for Scene Interpretation and Complex Event Recognition in Videos}}
}
@article{Espinace2010,
abstract = {Scene recognition is a highly valuable perceptual ability for an indoor$\backslash$nmobile robot, however, current approaches for scene recognition present$\backslash$na significant drop in performance for the case of indoor scenes.$\backslash$nWe believe that this can be explained by the high appearance variability$\backslash$nof indoor environments. This stresses the need to include high-level$\backslash$nsemantic information in the recognition process. In this work we$\backslash$npropose a new approach for indoor scene recognition based on a generative$\backslash$nprobabilistic hierarchical model that uses common objects as an intermediate$\backslash$nsemantic representation. Under this model, we use object classifiers$\backslash$nto associate low-level visual features to objects, and at the same$\backslash$ntime, we use contextual relations to associate objects to scenes.$\backslash$nAs a further contribution, we improve the performance of current$\backslash$nstate-of-the-art category-level object classifiers by including geometrical$\backslash$ninformation obtained from a 3D range sensor that facilitates the$\backslash$nimplementation of a focus of attention mechanism within a Monte Carlo$\backslash$nsampling scheme. We test our approach using real data, showing significant$\backslash$nadvantages with respect to previous state-of-the-art methods.},
author = {Espinace, P. and Kollar, T. and Soto, A. and Roy, N.},
doi = {10.1109/ROBOT.2010.5509682},
file = {:home/aufar/Documents/FinalYearProject/Background Research/10.1109{\_}robot.2010.5509682.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1406--1413},
publisher = {IEEE},
title = {{Indoor scene recognition through object detection}},
year = {2010}
}
@article{Thrun2005,
abstract = {Probabilistic robotics is a new and growing area in robotics, concerned with perception and control in the face of uncertainty. Building on the field of mathematical statistics, probabilistic robotics endows robots with a new level of robustness in real-world situations.{\textless}br {\textless}br This book introduces the reader to a wealth of techniques and algorithms in the field. All algorithms are based on a single overarching mathematical foundation. Each chapter provides example implementations in pseudo code, detailed mathematical derivations, discussions from a practitioner's perspective, and extensive lists of exercises and class projects. The book's Web site, http://www.probabilistic-robotics.org, has additional material.{\textless}br {\textless}br The book is relevant for anyone involved in robotic software development and scientific research. It will also be of interest to applied statisticians and engineers dealing with real-world sensor data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
doi = {10.1145/504729.504754},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/Documents/FinalYearProject/Background Research/ProbabilisticRobotics.pdf:pdf},
isbn = {0262201623},
issn = {00010782},
journal = {Intelligent robotics and autonomous agents, The MIT {\ldots}},
number = {3},
pages = {52},
pmid = {20926156},
title = {{Probabilistic robotics (intelligent robotics and autonomous agents series)}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Probabilistic+Robotics+(Intelligent+Robotics+and+Autonomous+Agents){\#}0},
volume = {45},
year = {2005}
}
