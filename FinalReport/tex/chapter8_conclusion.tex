\chapter{Conclusion and Further Work}

\section{Conclusion}
Throughout this report, we have discussed the implementation of an augmented reality system to aid PWUs in navigating areas with people as potential collision risks. We outlined the requirements for the system to handle this scenario in the Requirement Capture section of the report. From our tests, we can show that we have implemented a working proof of concept, using the Microsoft Hololens as the augmented reality device, and front-facing camera as the visual input for people detection and analysis.

\paragraph{}One of the contributions of this project is the development of a Unity application that streams the front-facing camera of the Hololens to a partner PC. The ability to process the frames on a computer with a GPU opens up a whole avenue of research for computer vision techniques using the Hololens. This report explores the use of these techniques in the Human Detection \& Direction system, which uses the YOLO object detector, Deep SORT tracker, and OpenPose body pose estimation networks. To make this accessible to future researchers, we make available the Unity framework, which implements the video streaming across ROS topics. However, this report also highlights the limitations of using the Unity engine for video streaming, and suggest that any future researchers should develop a UWP application that accesses the Hololens video stream, to make use of the multi-threading capabilities of the device.

\paragraph{}Secondly, we contribute a version of Darknet that has been modified to run as a ROS node, allowing seamless integration with other ROS packages. We achieved this by wrapping the core Darknet libraries using Python and providing an interface between the neural network framework and ROS topics to pass messages containing the images and bounding box detections. In addition to this, we have also trained the YOLO object detector using the CrowdHuman dataset. We provide the pre-trained weights that can be run on the ROS Darknet framework. We also explored the use of body pose estimation as a method of determining the direction a detected individual is walking in. We have developed Python wrappers around the network to allow it to accept ROS topic messages as input and a way for it to output its keypoint estimations.  

\paragraph{}Thirdly, in addition to the video streaming capabilities of the Unity application, we have also developed an augmented reality experience for PWU which visualizes the direction people are walking in, based on the results of the HDD system. These holograms are rendered by the Hololens and are used as visual aids by the PWU to avoid collisions. Due to the frame rate limitations of the application, the holographic visualizations are not completely stable but provide a reasonable indicator of direction to the PWU.

\paragraph{}Finally, we have developed a simple collision avoidance system that utilizes the spatial mapping capabilities of the Hololens to determine the distance between the PWU and the detected people. The reactive control system monitors the input velocity commands sent by the PWU using the joystick on ARTA and selectively controls the final velocity of the wheelchair when it detects a collision.

\section{Future Work}

\subsection{Utilizing ARTA Sensors}
To achieve a more accurate mapping of the surroundings, the system should not rely on a single sensor input in the form of the front-facing camera. Instead, the visual input and positions of the object obtained from the front-facing camera should be used in conjunction with other sensors, such as the laser scanners attached to the base of ARTA. Furthermore, the PRL has already developed ROS packages that implement this on ARTA. The future work would involve comparing the positions of the detected persons in the Hololens frame and ARTA frame to estimate a better world position.

\subsection{GameObject Tracking in Unity}
In the current Unity application, we create a new GameObject representing a detected person and delete it every frame. We do this since we determine the direction a person is walking in from the output of the OpenPose network. As such, the tracker ID is not used beyond visualization.

\paragraph{} An alternative approach would be to create a single GameObject for each tracking ID. These GameObjects are then compared across frames, and if the tracker ID exists in the new frame, the previously instantiated GameObject is rendered. When the tracker ID is not in the frame, we set the mesh of the hologram to transparent, and the hologram is not visible to the PWU. This would allow the Unity application to keep track of the positions of the detected objects across frames, and reduce the error in the accuracy of hologram placement. This would also mean that the Hololens was aware of detected persons, even when the user turns their head to the side and the objects are no longer in the FOV of the front-facing camera.

\subsection{Advanced Reactive Control}
Rather than have the reactive control system reduce the speed of ARTA when it detects a collision risk, a more advanced technique would be a system that takes over and navigates the wheelchair out of the way of the detected person. This system would be implemented using either the additional ARTA sensors or GameObject tracking we mentioned in the previous sections as a way of keeping a better mapping of the surroundings of the Hololens. We could then use the object avoidance libraries developed by the PRL to control ARTA when it detects objects, and calculate the best path that avoids a collision.

\subsection{Video Streaming}
The major limitation of the Unity Game engine is the fact that image compression can only be done in the main thread of the application. This limits the frame rate the scene can be displayed at, resulting in latency when rendering holograms in the surroundings. This was why we chose to instantiate and delete holograms for each detection instead of having them persist across frames. 

\paragraph{}To solve this problem, one would have to develop a way of compressing the images outside of the main thread. This is a difficult problem to solve since Unity does not allow access to textures and GameObjects outside of the main thread due to thread safety issues. As such, an alternative approach would be to somehow access the video streams outside of Unity, perhaps using the HololensForCV library provided by Microsoft, to stream the front facing camera to a partner PC. The partner PC would then stream the captured images back to the Hololens and the detections, into the Unity application. This, however, would involve writing a UWP application that could run in the background of the Hololens, as well as using Windows networking protocols to stream the video data from the Hololens.
