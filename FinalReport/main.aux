\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{Acknowledgements}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction and Requirements}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{5}}
\citation{Hou2010}
\citation{Zeng2017}
\citation{Zeng2017}
\citation{Redmon}
\citation{Redmon}
\citation{Piccardi2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Human Detection}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Direction of Research}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Review of Existing Methodologies}{6}}
\@writefile{toc}{\contentsline {paragraph}{}{6}}
\citation{Hirabayashi}
\citation{Viola2001}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Comparison of Foreground Detection and Scanning Windows. The number of bounding boxes drawn by the scanning window shows the computational complexity of the method.\relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:foregroundVsScanning}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Foreground Detection}{7}}
\@writefile{toc}{\contentsline {paragraph}{}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Scanning Windows}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Classical Object Detection}{7}}
\@writefile{toc}{\contentsline {paragraph}{Haar Cascades}{7}}
\citation{Dalal2005}
\citation{Girshick2014}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{toc}{\contentsline {paragraph}{Histograms of Oriented Gradients}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Deep Learning Object Detection}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the R-CNN approach. The number of region proposals contributes to the latency of this method.\relax }}{8}}
\citation{Redmon}
\@writefile{toc}{\contentsline {paragraph}{R-CNN}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\newlabel{sec:backYOLO}{{2.1.2}{9}}
\@writefile{toc}{\contentsline {paragraph}{YOLO}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces YOLO algorithm dividing a square image into grid-cells for bounding box prediction.\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Comments}{9}}
\newlabel{sec:detector}{{2.1.3}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\citation{Dicle2013}
\citation{Bewley2016}
\citation{Kalman1961}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Tracking}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Direction of Research}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Review of Exisiting Methodologies}{10}}
\newlabel{sec:objectTrack}{{2.2.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{SORT}{10}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\citation{Wojke2018}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Deep SORT}{11}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{11}}
\@writefile{toc}{\contentsline {paragraph}{}{11}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Comments}{11}}
\newlabel{sec:objectTrackComments}{{2.2.3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Head and Body Pose Estimation}{11}}
\citation{Valenti2012}
\citation{Murphy-Chutorian2009}
\citation{Kazemi2014}
\citation{Papandreou2017}
\citation{Ren2017}
\citation{He2016}
\citation{Pishchulin}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Direction of Research}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Review of Existing Methodologies}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Head Pose Estimation}{12}}
\newlabel{sec:backHeadPoseEstimation}{{2.3.2}{12}}
\@writefile{toc}{\contentsline {paragraph}{Facial Landmark Detection}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Body Pose Estimation}{12}}
\newlabel{sec:backBodyPoseEstimation}{{2.3.2}{12}}
\@writefile{toc}{\contentsline {paragraph}{PoseNet}{12}}
\citation{Cao2017}
\citation{Cao2017}
\citation{Cao2017}
\citation{Bailey2006a}
\@writefile{toc}{\contentsline {paragraph}{OpenPose}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces OpenPose body pose detection \cite  {Cao2017}. The network is able to determine an estimate of the key-points hidden by objects.\relax }}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Comments}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}SLAM}{13}}
\citation{Cadena2016}
\citation{Taketomi2017}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Direction of Research}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Review of Existing Methodologies}{14}}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Visual SLAM}{14}}
\@writefile{toc}{\contentsline {paragraph}{Initialization}{14}}
\citation{Nister2004}
\citation{Milgram1994}
\@writefile{toc}{\contentsline {paragraph}{Tracking}{15}}
\@writefile{toc}{\contentsline {paragraph}{Mapping}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Comments}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Augmented Reality Headsets}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Direction of Research}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Review of Existing Methods}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Microsoft Hololens}{15}}
\newlabel{back:holo}{{2.5.2}{15}}
\citation{Microsofta}
\citation{Microsoft2015}
\citation{Zolotas2018}
\citation{Chacon-Quesada}
\@writefile{toc}{\contentsline {paragraph}{Holograms}{16}}
\@writefile{toc}{\contentsline {paragraph}{}{16}}
\@writefile{toc}{\contentsline {paragraph}{Hardware Specifications}{16}}
\@writefile{toc}{\contentsline {paragraph}{}{16}}
\@writefile{toc}{\contentsline {paragraph}{Personal Robotics Lab}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Comments}{16}}
\citation{Chacon-Quesada}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Requirements Capture}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Project Deliverable}{17}}
\@writefile{toc}{\contentsline {paragraph}{}{17}}
\@writefile{toc}{\contentsline {paragraph}{}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Human Detection and Direction}{17}}
\@writefile{toc}{\contentsline {paragraph}{}{17}}
\@writefile{toc}{\contentsline {paragraph}{Features}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Obstacle Mapping \& Visualization }{18}}
\@writefile{toc}{\contentsline {paragraph}{Features}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Reactive Control}{18}}
\newlabel{sec:reactive}{{3.4}{18}}
\@writefile{toc}{\contentsline {paragraph}{Features}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Analysis and Design}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:4}{{4}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Design Overview}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces High level system diagram showing the flow of messages from ARTA and the HDD system through the intermediary device, the Hololens.\relax }}{19}}
\newlabel{fig:simplifiedHL}{{4.1}{19}}
\citation{Bewley2016,Jin2017}
\citation{Insafutdinov}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Hardware}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hardware description of devices each system runs on.\relax }}{20}}
\newlabel{tab:hardware}{{4.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}System Communication}{20}}
\newlabel{sec:systemComms}{{4.1.2}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Robotic Operating System}{20}}
\@writefile{toc}{\contentsline {paragraph}{ROS Topics}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Human Detection \& Direction System}{20}}
\citation{Chacon-Quesada,Detectron2018,Rena}
\citation{Redmon}
\citation{Redmon2018}
\citation{Lin}
\citation{Shao}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}YOLO Object Detector}{21}}
\newlabel{sec:yolo}{{4.2.1}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of Detector}{21}}
\@writefile{toc}{\contentsline {paragraph}{}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Pre-trained Model vs Training}{21}}
\newlabel{sec:designYOLO}{{4.2.1}{21}}
\@writefile{toc}{\contentsline {paragraph}{Comparing Models}{21}}
\@writefile{toc}{\contentsline {paragraph}{Pedestrian Dataset}{21}}
\citation{Wojke2018}
\citation{Cao2017}
\citation{Bewley2016}
\citation{Wojke2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Model comparison on a video from Sherfield Walkway at Imperial College London. The trained model is able to better detect figures at a distance.\relax }}{22}}
\newlabel{fig:yoloCHvsCoco}{{4.2}{22}}
\@writefile{toc}{\contentsline {paragraph}{Analysis}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}YACHT: Yet Another Crowd Human Tracker}{22}}
\@writefile{toc}{\contentsline {paragraph}{}{22}}
\@writefile{toc}{\contentsline {subsubsection}{YACHT Tracker: Object Tracking}{22}}
\newlabel{sec:YACHT}{{4.2.2}{22}}
\@writefile{toc}{\contentsline {paragraph}{SORT}{22}}
\citation{Milan}
\citation{Milan}
\@writefile{toc}{\contentsline {paragraph}{Deep SORT}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces MOT16 benchmark \cite  {Milan} (L) using our YOLO model (M) for Deep SORT (R). The increased detection accuracy results in slightly better initial detections, so tracking is more accurate.\relax }}{23}}
\newlabel{fig:deepSortMOT}{{4.3}{23}}
\@writefile{toc}{\contentsline {paragraph}{Analysis}{23}}
\citation{Leutenegger2019}
\citation{Leutenegger2019}
\@writefile{toc}{\contentsline {subsubsection}{Object Tracking for Direction Inference}{24}}
\newlabel{sec:objecTrackingDirection}{{4.2.2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Linear extrapolation works for objects that move across the frame, but it becomes difficult to determine the direction when mostly vertical motion occurs\relax }}{24}}
\newlabel{fig:linExProblem}{{4.4}{24}}
\@writefile{toc}{\contentsline {paragraph}{Algorithm}{24}}
\@writefile{toc}{\contentsline {paragraph}{Issues}{24}}
\citation{Cao2017}
\citation{Cao2017}
\citation{Cao2017}
\citation{Patacchiola2017a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Pinhole Camera projection and the loss of the z-axis \cite  {Leutenegger2019}. As such, it is not possible to determine the exact distance to an object without a depth camera.\relax }}{25}}
\newlabel{fig:pinhole}{{4.5}{25}}
\@writefile{toc}{\contentsline {paragraph}{}{25}}
\@writefile{toc}{\contentsline {subsubsection}{YACHT Direction: Body Pose Estimation}{25}}
\newlabel{des:YACHTBody}{{4.2.2}{25}}
\@writefile{toc}{\contentsline {paragraph}{Object Detectors \& Bottom-Up Approaches}{25}}
\newlabel{des:body_25}{{4.2.2}{25}}
\@writefile{toc}{\contentsline {paragraph}{Keypoint Estimation}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Keypoints produced by the BODY\_25 model \cite  {Cao2017}.\relax }}{26}}
\newlabel{fig:bodyKeyPoints}{{4.6}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Head Pose Estimation}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Initial HDD System with the HeadPose Node before removal.\relax }}{26}}
\newlabel{fig:headPoseHDD}{{4.7}{26}}
\@writefile{toc}{\contentsline {paragraph}{Head Detection}{26}}
\@writefile{toc}{\contentsline {paragraph}{Reasons for removal}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces DeepGaze head pose estimator on sample images. We notice in (b) that DeepGaze predicts everyone to be looking in the same direction, despite the differences in head poses.\relax }}{27}}
\newlabel{fig:deepGaze}{{4.8}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hololens Unity Application}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}ROS Node}{27}}
\newlabel{sec:rossharp}{{4.3.1}{27}}
\@writefile{toc}{\contentsline {paragraph}{}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}HoloCamera}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces ROS\# acts as a ROS wrapper around the Unity application, allowing for seamless communication with other ROS nodes.\relax }}{28}}
\newlabel{fig:holoROSWrapper}{{4.9}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Video Streaming Choices}{28}}
\newlabel{sec:videoStreaming}{{4.3.2}{28}}
\@writefile{toc}{\contentsline {paragraph}{Windows Device Portal}{28}}
\@writefile{toc}{\contentsline {paragraph}{Microsoft HoloLensForCV}{28}}
\@writefile{toc}{\contentsline {paragraph}{Unity Camera Stream}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Module Description}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}HoloWorld}{29}}
\@writefile{toc}{\contentsline {subsubsection}{World Manager}{29}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA Manager}{29}}
\newlabel{sec:alignment}{{4.3.3}{29}}
\@writefile{toc}{\contentsline {paragraph}{Alignment}{29}}
\@writefile{toc}{\contentsline {paragraph}{}{29}}
\citation{Zolotas2018,Chacon-Quesada}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces We need a way of knowing if a detected object is in front of the wheelchair, or if the PWU is looking to the side. \relax }}{30}}
\newlabel{fig:holoArtaAlignment}{{4.10}{30}}
\@writefile{toc}{\contentsline {paragraph}{Reactive Control}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}ARTA}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}ROS Packages}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Reactive Control}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces A PWU wearing the Hololens controlling ARTA using the joystick.\relax }}{31}}
\newlabel{fig:zihanARTA}{{4.11}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Removal of Exotic Control Interfaces}{31}}
\@writefile{toc}{\contentsline {paragraph}{}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces System diagram detailing individual components of the programs running on each device.\relax }}{32}}
\newlabel{fig:detailedHL}{{5.1}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Human Detection \& Direction System}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The HDD System is sub-divided into two ROS packages, YOLO and YACHT. We show that YACHT has two sub-nodes which both depend on the outputs of the detector.\relax }}{33}}
\newlabel{fig:detailedHDD}{{5.2}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Hardware \& Software Dependencies}{33}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{33}}
\citation{darknet13}
\@writefile{toc}{\contentsline {subsubsection}{Software}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}YOLO Object Detector}{34}}
\@writefile{toc}{\contentsline {subsubsection}{Darknet}{34}}
\@writefile{toc}{\contentsline {subsubsection}{Darknet in ROS}{35}}
\@writefile{toc}{\contentsline {paragraph}{}{35}}
\@writefile{toc}{\contentsline {paragraph}{}{35}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Darknet IMAGE Python wrappers for seamless ROS integration.}{35}}
\citation{Shao}
\@writefile{toc}{\contentsline {subsubsection}{YOLO Detection Algorithm}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization of the YOLO person detection algorithm dividing a resized square image into grid cells.\relax }}{36}}
\newlabel{fig:yoloViz}{{5.3}{36}}
\@writefile{toc}{\contentsline {subsubsection}{YOLOv3 Tiny vs YOLOv3}{36}}
\@writefile{toc}{\contentsline {subsubsection}{Training YOLOv3 Tiny}{36}}
\@writefile{toc}{\contentsline {paragraph}{CrowdHuman}{36}}
\citation{Shao}
\citation{Shao}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces A comparison of CrowdHuman to other person image datasets \cite  {Shao}, showing the increase in number of people and diversity in the images.\relax }}{37}}
\newlabel{fig:crowdHumanStats}{{5.4}{37}}
\@writefile{toc}{\contentsline {paragraph}{Annotations}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{Converting Annotations}{37}}
\@writefile{toc}{\contentsline {paragraph}{Training Parameters}{38}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}Training parameters used for YOLOv3 Tiny on the CrowdHuman dataset}{38}}
\@writefile{toc}{\contentsline {paragraph}{Training Process}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluating Trained Model}{38}}
\@writefile{toc}{\contentsline {paragraph}{Hololens Videos}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Trained model detects people and heads at different ranges. We also see it can detect people far away and accurately detect their heads.\relax }}{38}}
\newlabel{fig:yoloRange}{{5.5}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Evaluating the model on Sherfield Walkway test video. We notice that it is able to small figures close together at a distance.\relax }}{39}}
\newlabel{fig:yoloSherfield}{{5.6}{39}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Node}{39}}
\newlabel{sec:nodeYOLO}{{5.1.2}{39}}
\@writefile{toc}{\contentsline {paragraph}{ROS Topics}{39}}
\@writefile{toc}{\contentsline {paragraph}{}{39}}
\citation{Wojke2018}
\newlabel{bbmsg}{{5.3}{40}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}BoundingBox.msg}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}YACHT Package}{40}}
\newlabel{sec:YACHT}{{5.1.3}{40}}
\@writefile{toc}{\contentsline {paragraph}{ROS Communication}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}YACHT: Tracker}{40}}
\@writefile{toc}{\contentsline {subsubsection}{Deep SORT}{40}}
\@writefile{toc}{\contentsline {paragraph}{Modifications}{40}}
\@writefile{toc}{\contentsline {paragraph}{Deep Association Metric}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Visualization of delay on CPU bound Deep SORT\relax }}{41}}
\newlabel{fig:deepSortCPU}{{5.7}{41}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}Deep SORT Tensorflow GPU modifications}{41}}
\@writefile{toc}{\contentsline {paragraph}{Trackers \& Tracking}{41}}
\citation{Shao}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Visualization of Deep SORT matching. IOU matching is used as a final check for tracks that are unmatched by feature vector Nearest Neighbour matching.\relax }}{42}}
\newlabel{fig:deepSortMatch}{{5.8}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Linear Extrapolation}{42}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topic}{42}}
\newlabel{sec:yachtTrackROS}{{5.1.4}{42}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.5}ROS message structure for BoundingBoxID.msg}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}YACHT: Direction}{42}}
\@writefile{toc}{\contentsline {paragraph}{}{42}}
\citation{Jia}
\@writefile{toc}{\contentsline {subsubsection}{OpenPose}{43}}
\@writefile{toc}{\contentsline {paragraph}{Installation \& Setup}{43}}
\@writefile{toc}{\contentsline {paragraph}{Model}{43}}
\@writefile{toc}{\contentsline {subsubsection}{KeyPoint Estimation}{43}}
\newlabel{sec:bottomUp}{{5.1.5}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison of keypoint estimation at different scales\relax }}{44}}
\newlabel{fig:openposeKP}{{5.9}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Defining Direction}{44}}
\newlabel{sec:keypointEstimate}{{5.1.5}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Body keypoint estimation of different views. These images show that the network can differentiate between left and right limbs.\relax }}{44}}
\newlabel{fig:keypointShrey}{{5.10}{44}}
\@writefile{toc}{\contentsline {paragraph}{Method}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Significant key-points for determining whether a person is facing the camera.\relax }}{45}}
\newlabel{tab:keypoints}{{5.1}{45}}
\@writefile{toc}{\contentsline {subsubsection}{Implementation \& Detection Matching}{45}}
\newlabel{lst:matchDetPose}{{5.6}{45}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.6}Direction and Detection Matching code in people\_direction.py}{45}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topic}{45}}
\newlabel{sec:yachtDirROS}{{5.1.5}{45}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.7}ROS message structure for BoundingBoxDirection.msg}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hololens Unity Application}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Components of the Unity application running on the Hololens. We divide the program into two sub-programs, HoloCamera and HoloWorld.\relax }}{46}}
\newlabel{fig:detailedHololens}{{5.11}{46}}
\@writefile{toc}{\contentsline {paragraph}{}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Hardware \& Software Dependencies}{46}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{46}}
\@writefile{toc}{\contentsline {paragraph}{Microsoft Hololens}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The Hololens is larger compared to its competition. However, the increased number of sensors and wider spread usage makes it an ideal AR device for this project.\relax }}{47}}
\newlabel{fig:holodevice}{{5.12}{47}}
\@writefile{toc}{\contentsline {subsubsection}{Software}{47}}
\@writefile{toc}{\contentsline {paragraph}{Universal Windows Platform}{47}}
\@writefile{toc}{\contentsline {paragraph}{Unity}{47}}
\@writefile{toc}{\contentsline {paragraph}{Development Tools}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Hololens Locatable Camera}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Specifications}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces These images were taken from the same position. The Hololens has a reduced FOV and is unable to capture the whole desk.\relax }}{48}}
\newlabel{fig:holoVsIphone}{{5.13}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Hololens Video Camera Stream}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Image Capture}{49}}
\@writefile{toc}{\contentsline {paragraph}{Vulcan Technologies}{49}}
\@writefile{toc}{\contentsline {paragraph}{Image Capture Pipeline}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The image capture pipeline involves an intermediate Texture2D Unity state.\relax }}{49}}
\newlabel{fig:imgProcPipeline}{{5.14}{49}}
\@writefile{toc}{\contentsline {paragraph}{}{50}}
\@writefile{toc}{\contentsline {subsubsection}{Image Compression}{50}}
\@writefile{toc}{\contentsline {paragraph}{Unity Threading}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Image compression is a time consuming operation that must be done in the main thread. Only after compression is done can the holograms be rendered, limiting the application to 5 FPS.\relax }}{50}}
\newlabel{fig:unityThreads}{{5.15}{50}}
\@writefile{toc}{\contentsline {paragraph}{Limited Frame-rate}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}ROS Communication}{51}}
\@writefile{toc}{\contentsline {subsubsection}{Modifications}{51}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topics}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces ROS topics in and out of the Unity application. Each topic carries data used by different parts of the overall system.\relax }}{51}}
\newlabel{fig:unityComms}{{5.16}{51}}
\@writefile{toc}{\contentsline {paragraph}{Compressed Images}{51}}
\@writefile{toc}{\contentsline {paragraph}{HDD Topics}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Hololens World}{52}}
\@writefile{toc}{\contentsline {subsubsection}{Hololens World Manager}{52}}
\newlabel{sec:holoworld}{{5.2.5}{52}}
\@writefile{toc}{\contentsline {paragraph}{Communication}{52}}
\@writefile{toc}{\contentsline {paragraph}{Spatial Mapping}{52}}
\@writefile{toc}{\contentsline {subsubsection}{HDD Data to GameObjects}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces The Intersection-over-Union metric is used to compare bounding boxes for matching. The higher the metric, the more overlap between the boxes.\relax }}{53}}
\newlabel{fig:ioumatching}{{5.17}{53}}
\@writefile{toc}{\contentsline {paragraph}{Matching Bounding Boxes}{53}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.8}IOU metric C\# implementation}{53}}
\@writefile{toc}{\contentsline {paragraph}{Creating GameObjects}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces The front facing camera projects the surroundings onto a 2D plane. We can determine the real world position of a pixel by reversing the projection and using frame transforms.\relax }}{54}}
\newlabel{fig:cameraProjection}{{5.18}{54}}
\@writefile{toc}{\contentsline {subsubsection}{Hologram Manager}{54}}
\@writefile{toc}{\contentsline {paragraph}{Holograms}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces The Green and Red Arrow prefabs. The number under the arrow is the associated tracker ID of the object. This allows us to track objects in the surroundings.\relax }}{55}}
\newlabel{fig:holograms}{{5.19}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Images captured from the Hololens showing what the PWU would see when wearing the device.\relax }}{55}}
\newlabel{fig:greenredrender}{{5.20}{55}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA Manager}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}ARTA Powered Wheelchair}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces The ROS nodes running on ARTA. We utilize PRL ROS packages as a base and communicate with the Hololens for reactive control.\relax }}{56}}
\newlabel{fig:detailedARTA}{{5.21}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Hardware \& Software Dependencies}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Software}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}PRL ROS Packages}{57}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA}{57}}
\@writefile{toc}{\contentsline {subsubsection}{Localisation}{57}}
\@writefile{toc}{\contentsline {subsubsection}{Navigation}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Hololens Communication}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Reactive Control}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces The Reactive Control system spans both ARTA and the Hololens, due to its dependency on ARTA sensor data as well as the Unity converted HDD system GameObjects.\relax }}{58}}
\newlabel{fig:detailedReactive}{{5.22}{58}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA}{59}}
\@writefile{toc}{\contentsline {paragraph}{Manual Control}{59}}
\@writefile{toc}{\contentsline {paragraph}{ARTA Positioning}{59}}
\@writefile{toc}{\contentsline {subsubsection}{Hololens}{59}}
\@writefile{toc}{\contentsline {paragraph}{World Manager}{59}}
\@writefile{toc}{\contentsline {paragraph}{ARTA Manager}{59}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA Hololens Alignment}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces The alignment of the ARTA and Hololens. We note that both frames can rotate about the z-axis.\relax }}{60}}
\newlabel{fig:holoArtaFrames}{{5.23}{60}}
\@writefile{toc}{\contentsline {paragraph}{Alignment Process}{60}}
\@writefile{toc}{\contentsline {subsubsection}{Unity Distance to Objects}{60}}
\@writefile{toc}{\contentsline {subsubsection}{Reactive Control Algorithm}{60}}
\@writefile{toc}{\contentsline {paragraph}{}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces A visualization of the Reactive Control system and the corresponding distance speed scaling functions.\relax }}{61}}
\newlabel{fig:reactiveFlow}{{5.24}{61}}
\@writefile{toc}{\contentsline {paragraph}{}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces The velocity scaling functions relative to the distance to the object. We note that the red function begins decreasing when the detected person is further away. This is to compensate for the person walking towards the PWU.\relax }}{62}}
\newlabel{fig:distanceScalingFunctions}{{5.25}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}System Summary}{62}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Testing \& Results}{63}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Human Detection and Distance}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Test System Description}{63}}
\newlabel{sec:hddSys}{{6.1.1}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Test Setup}{63}}
\newlabel{sec:testSetup}{{6.1.2}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The experimental setup used to test the HDD and Hololens spatial mapping accuracy. We asked the people acting as targets to stand at 1 meter intervals while a person sitting down wearing the Hololens looked at them using the front facing camera.\relax }}{64}}
\newlabel{fig:hddTestSetup}{{6.1}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Test Procedure}{64}}
\@writefile{toc}{\contentsline {paragraph}{}{64}}
\@writefile{toc}{\contentsline {paragraph}{}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Results}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The graph shows that there is no significant difference between the orientation of the target. We note the decrease in accuracy past $4$m, and the lack of holograms at $6$m.\relax }}{65}}
\newlabel{fig:hddResults}{{6.2}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Target person at different distances viewed through the front-camera of the Hololens. We note that the accuracy of the hologram placement decreases as the target is further away.\relax }}{66}}
\newlabel{fig:marek}{{6.3}{66}}
\@writefile{toc}{\contentsline {paragraph}{}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Discussion}{66}}
\newlabel{sec:distanceDiscussion}{{6.1.5}{66}}
\@writefile{toc}{\contentsline {paragraph}{}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Gazebo Simulation}{67}}
\newlabel{sec:gazeboSimTest}{{6.2}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Test System Description}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Test Setup}{67}}
\@writefile{toc}{\contentsline {subsubsection}{Gazebo Simulation}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The Gazebo Simulation of the lab hallway outside the PRL. The blue mesh represents the range of the simulated laser scanner.\relax }}{67}}
\newlabel{fig:greenredrender}{{6.4}{67}}
\@writefile{toc}{\contentsline {subsubsection}{Real World Lab Setup}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces We emulate the real world position of the Gazebo model position in the ICRS lab. We monitor the simulated velocity of the wheelchair as we vary the distance the target person is from the Hololens.\relax }}{68}}
\newlabel{fig:greenredrender}{{6.5}{68}}
\@writefile{toc}{\contentsline {subsubsection}{Monitoring ROS Topics}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Test Procedure}{68}}
\@writefile{toc}{\contentsline {paragraph}{}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Results}{69}}
\@writefile{toc}{\contentsline {paragraph}{}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Reactive Control System velocity outputs at different distances.\relax }}{70}}
\newlabel{fig:reactiveResults}{{6.6}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}Discussion}{71}}
\newlabel{sec:gazeboDiscussion}{{6.2.5}{71}}
\@writefile{toc}{\contentsline {paragraph}{}{71}}
\@writefile{toc}{\contentsline {paragraph}{}{71}}
\@writefile{toc}{\contentsline {paragraph}{}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Full System}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Test Setup}{72}}
\@writefile{toc}{\contentsline {paragraph}{}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Test Procedure}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces The experimental setup used to test the full system. We ask the target person to walk down the hallway for Scenarios 1 \& 2. For Scenario 3, the target person stands still.\relax }}{73}}
\newlabel{fig:fullSystemTest}{{6.7}{73}}
\@writefile{toc}{\contentsline {paragraph}{Target Away}{73}}
\@writefile{toc}{\contentsline {paragraph}{Target Towards}{73}}
\@writefile{toc}{\contentsline {paragraph}{Looking Away}{73}}
\@writefile{toc}{\contentsline {paragraph}{}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces The path taken by the Target Person along the hallway outside the PRL.\relax }}{74}}
\newlabel{fig:zihanBackForward}{{6.8}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Results}{74}}
\@writefile{toc}{\contentsline {subsubsection}{Target Away}{74}}
\@writefile{toc}{\contentsline {subsubsection}{Target Towards}{75}}
\@writefile{toc}{\contentsline {subsubsection}{Looking Away}{75}}
\@writefile{toc}{\contentsline {subsubsection}{Summary of Scenarios}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Discussion}{75}}
\@writefile{toc}{\contentsline {paragraph}{}{75}}
\@writefile{toc}{\contentsline {paragraph}{}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Overall System Discussion}{76}}
\bibstyle{unsrt}
\bibdata{fyp_report.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Evaluation}{77}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{Hou2010}{{1}{}{{}}{{}}}
\bibcite{Zeng2017}{{2}{}{{}}{{}}}
\bibcite{Redmon}{{3}{}{{}}{{}}}
\bibcite{Piccardi2004}{{4}{}{{}}{{}}}
\bibcite{Hirabayashi}{{5}{}{{}}{{}}}
\bibcite{Viola2001}{{6}{}{{}}{{}}}
\bibcite{Dalal2005}{{7}{}{{}}{{}}}
\bibcite{Girshick2014}{{8}{}{{}}{{}}}
\bibcite{Dicle2013}{{9}{}{{}}{{}}}
\bibcite{Bewley2016}{{10}{}{{}}{{}}}
\bibcite{Kalman1961}{{11}{}{{}}{{}}}
\bibcite{Wojke2018}{{12}{}{{}}{{}}}
\bibcite{Valenti2012}{{13}{}{{}}{{}}}
\bibcite{Murphy-Chutorian2009}{{14}{}{{}}{{}}}
\bibcite{Kazemi2014}{{15}{}{{}}{{}}}
\bibcite{Papandreou2017}{{16}{}{{}}{{}}}
\bibcite{Ren2017}{{17}{}{{}}{{}}}
\bibcite{He2016}{{18}{}{{}}{{}}}
\bibcite{Pishchulin}{{19}{}{{}}{{}}}
\bibcite{Cao2017}{{20}{}{{}}{{}}}
\bibcite{Bailey2006a}{{21}{}{{}}{{}}}
\bibcite{Cadena2016}{{22}{}{{}}{{}}}
\bibcite{Taketomi2017}{{23}{}{{}}{{}}}
\bibcite{Nister2004}{{24}{}{{}}{{}}}
\bibcite{Milgram1994}{{25}{}{{}}{{}}}
\bibcite{Microsofta}{{26}{}{{}}{{}}}
\bibcite{Microsoft2015}{{27}{}{{}}{{}}}
\bibcite{Zolotas2018}{{28}{}{{}}{{}}}
\bibcite{Chacon-Quesada}{{29}{}{{}}{{}}}
\bibcite{Jin2017}{{30}{}{{}}{{}}}
\bibcite{Insafutdinov}{{31}{}{{}}{{}}}
\bibcite{Detectron2018}{{32}{}{{}}{{}}}
\bibcite{Rena}{{33}{}{{}}{{}}}
\bibcite{Redmon2018}{{34}{}{{}}{{}}}
\bibcite{Lin}{{35}{}{{}}{{}}}
\bibcite{Shao}{{36}{}{{}}{{}}}
\bibcite{Milan}{{37}{}{{}}{{}}}
\bibcite{Leutenegger2019}{{38}{}{{}}{{}}}
\bibcite{Patacchiola2017a}{{39}{}{{}}{{}}}
\bibcite{darknet13}{{40}{}{{}}{{}}}
\bibcite{Jia}{{41}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\providecommand\totalcount@set[2]{}
\totalcount@set{page}{82}
