\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {paragraph}{Abstract}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction and Requirements}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{4}}
\citation{Hou2010}
\citation{Zeng2017}
\citation{Zeng2017}
\citation{Redmon}
\citation{Redmon}
\citation{Piccardi2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Human Detection}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Direction of Research}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Review of Existing Methodologies}{5}}
\@writefile{toc}{\contentsline {paragraph}{}{5}}
\citation{Hirabayashi}
\citation{Viola2001}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Comparison of Foreground Detection and Scanning Windows. The number of bounding boxes drawn by the scanning window shows the computational complexity of the method.\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:foregroundVsScanning}{{2.1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Foreground Detection}{6}}
\@writefile{toc}{\contentsline {paragraph}{}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Scanning Windows}{6}}
\@writefile{toc}{\contentsline {subsubsection}{Classical Object Detection}{6}}
\@writefile{toc}{\contentsline {paragraph}{Haar Cascades}{6}}
\citation{Dalal2005}
\citation{Girshick2014}
\@writefile{toc}{\contentsline {paragraph}{}{7}}
\@writefile{toc}{\contentsline {paragraph}{Histograms of Oriented Gradients}{7}}
\@writefile{toc}{\contentsline {paragraph}{}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Deep Learning Object Detection}{7}}
\@writefile{toc}{\contentsline {paragraph}{}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the R-CNN approach. The number of region proposals contributes to the latency of this method.\relax }}{7}}
\citation{Redmon}
\@writefile{toc}{\contentsline {paragraph}{R-CNN}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\newlabel{sec:backYOLO}{{2.1.2}{8}}
\@writefile{toc}{\contentsline {paragraph}{YOLO}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces YOLO algorithm dividing a square image into grid-cells for bounding box prediction.\relax }}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Comments}{8}}
\newlabel{sec:detector}{{2.1.3}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\citation{Dicle2013}
\citation{Bewley2016}
\citation{Kalman1961}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Tracking}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Direction of Research}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Review of Exisiting Methodologies}{9}}
\newlabel{sec:objectTrack}{{2.2.2}{9}}
\@writefile{toc}{\contentsline {subsubsection}{SORT}{9}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\citation{Wojke2018}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Deep SORT}{10}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Comments}{10}}
\newlabel{sec:objectTrackComments}{{2.2.3}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Head and Body Pose Estimation}{10}}
\citation{Valenti2012}
\citation{Murphy-Chutorian2009}
\citation{Kazemi2014}
\citation{Papandreou2017}
\citation{Ren2017}
\citation{He2016}
\citation{Pishchulin}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Direction of Research}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Review of Existing Methodologies}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Head Pose Estimation}{11}}
\newlabel{sec:backHeadPoseEstimation}{{2.3.2}{11}}
\@writefile{toc}{\contentsline {paragraph}{Facial Landmark Detection}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Body Pose Estimation}{11}}
\newlabel{sec:backBodyPoseEstimation}{{2.3.2}{11}}
\@writefile{toc}{\contentsline {paragraph}{PoseNet}{11}}
\citation{Cao2017}
\citation{Cao2017}
\citation{Cao2017}
\citation{Bailey2006a}
\@writefile{toc}{\contentsline {paragraph}{OpenPose}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces OpenPose body pose detection \cite  {Cao2017}. The network is able to determine an estimate of the key-points hidden by objects.\relax }}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Comments}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}SLAM}{12}}
\citation{Cadena2016}
\citation{Taketomi2017}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Direction of Research}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Review of Existing Methodologies}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Visual SLAM}{13}}
\@writefile{toc}{\contentsline {paragraph}{Initialization}{13}}
\citation{Nister2004}
\citation{Milgram1994}
\@writefile{toc}{\contentsline {paragraph}{Tracking}{14}}
\@writefile{toc}{\contentsline {paragraph}{Mapping}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Comments}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Augmented Reality Headsets}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Direction of Research}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Review of Existing Methods}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Microsoft Hololens}{14}}
\newlabel{back:holo}{{2.5.2}{14}}
\citation{Microsofta}
\citation{Microsoft2015}
\citation{Zolotas2018}
\citation{Chacon-Quesada}
\citation{Chacon-Quesada}
\citation{Chacon-Quesada}
\@writefile{toc}{\contentsline {paragraph}{Holograms}{15}}
\@writefile{toc}{\contentsline {paragraph}{}{15}}
\@writefile{toc}{\contentsline {paragraph}{Hardware Specifications}{15}}
\@writefile{toc}{\contentsline {paragraph}{}{15}}
\@writefile{toc}{\contentsline {paragraph}{Personal Robotics Lab}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Comments}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces AR visualizations and markers for interaction \cite  {Chacon-Quesada}\relax }}{16}}
\citation{Chacon-Quesada}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Requirements Capture}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Project Deliverable}{17}}
\@writefile{toc}{\contentsline {paragraph}{}{17}}
\@writefile{toc}{\contentsline {paragraph}{}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Human Detection and Direction}{17}}
\@writefile{toc}{\contentsline {paragraph}{}{17}}
\@writefile{toc}{\contentsline {paragraph}{Features}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Obstacle Mapping \& Visualization }{18}}
\@writefile{toc}{\contentsline {paragraph}{Features}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Reactive Control}{18}}
\@writefile{toc}{\contentsline {paragraph}{Features}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Analysis and Design}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:4}{{4}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Design Overview}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces High level system diagram showing the flow of messages from ARTA and the HDD system through the intermediary device, the Hololens.\relax }}{19}}
\newlabel{fig:simplifiedHL}{{4.1}{19}}
\citation{Bewley2016,Jin2017}
\citation{Insafutdinov}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Hardware}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hardware description of devices each system runs on.\relax }}{20}}
\newlabel{tab:hardware}{{4.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}System Communication}{20}}
\newlabel{sec:systemComms}{{4.1.2}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Robotic Operating System}{20}}
\@writefile{toc}{\contentsline {paragraph}{ROS Topics}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Human Detection \& Direction System}{20}}
\citation{Chacon-Quesada,Detectron2018,Rena}
\citation{Redmon}
\citation{Redmon2018}
\citation{Lin}
\citation{Shao}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}YOLO Object Detector}{21}}
\newlabel{sec:yolo}{{4.2.1}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of Detector}{21}}
\@writefile{toc}{\contentsline {paragraph}{}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Pre-trained Model vs Training}{21}}
\newlabel{sec:designYOLO}{{4.2.1}{21}}
\@writefile{toc}{\contentsline {paragraph}{Comparing Models}{21}}
\@writefile{toc}{\contentsline {paragraph}{Pedestrian Dataset}{21}}
\citation{Wojke2018}
\citation{Cao2017}
\citation{Bewley2016}
\citation{Wojke2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Model comparison on a video from Sherfield Walkway at Imperial College London. The trained model is able to better detect figures at a distance.\relax }}{22}}
\newlabel{fig:yoloCHvsCoco}{{4.2}{22}}
\@writefile{toc}{\contentsline {paragraph}{Analysis}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}YACHT: Yet Another Crowd Human Tracker}{22}}
\@writefile{toc}{\contentsline {paragraph}{}{22}}
\@writefile{toc}{\contentsline {subsubsection}{YACHT Tracker: Object Tracking}{22}}
\newlabel{sec:YACHT}{{4.2.2}{22}}
\@writefile{toc}{\contentsline {paragraph}{SORT}{22}}
\citation{Milan}
\citation{Milan}
\@writefile{toc}{\contentsline {paragraph}{Deep SORT}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces MOT16 benchmark \cite  {Milan} (L) using our YOLO model (M) for Deep SORT (R). The increased detection accuracy results in slightly better initial detections, so tracking is more accurate.\relax }}{23}}
\newlabel{fig:deepSortMOT}{{4.3}{23}}
\@writefile{toc}{\contentsline {paragraph}{Analysis}{23}}
\citation{Leutenegger2019}
\citation{Leutenegger2019}
\@writefile{toc}{\contentsline {subsubsection}{Object Tracking for Direction Inference}{24}}
\newlabel{sec:objecTrackingDirection}{{4.2.2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Linear extrapolation works for objects that move across the frame, but it becomes difficult to determine the direction when mostly vertical motion occurs\relax }}{24}}
\newlabel{fig:linExProblem}{{4.4}{24}}
\@writefile{toc}{\contentsline {paragraph}{Algorithm}{24}}
\@writefile{toc}{\contentsline {paragraph}{Issues}{24}}
\citation{Cao2017}
\citation{Cao2017}
\citation{Cao2017}
\citation{Patacchiola2017a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Pinhole Camera projection and the loss of the z-axis \cite  {Leutenegger2019}. As such, it is not possible to determine the exact distance to an object without a depth camera.\relax }}{25}}
\newlabel{fig:pinhole}{{4.5}{25}}
\@writefile{toc}{\contentsline {paragraph}{}{25}}
\@writefile{toc}{\contentsline {subsubsection}{YACHT Direction: Body Pose Estimation}{25}}
\newlabel{des:YACHTBody}{{4.2.2}{25}}
\@writefile{toc}{\contentsline {paragraph}{Object Detectors \& Bottom-Up Approaches}{25}}
\newlabel{des:body_25}{{4.2.2}{25}}
\@writefile{toc}{\contentsline {paragraph}{Keypoint Estimation}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Keypoints produced by the BODY\_25 model \cite  {Cao2017}.\relax }}{26}}
\newlabel{fig:bodyKeyPoints}{{4.6}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Head Pose Estimation}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Initial HDD System with the HeadPose Node before removal.\relax }}{26}}
\newlabel{fig:headPoseHDD}{{4.7}{26}}
\@writefile{toc}{\contentsline {paragraph}{Head Detection}{26}}
\@writefile{toc}{\contentsline {paragraph}{Reasons for removal}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces DeepGaze head pose estimator on sample images. We notice in (b) that DeepGaze predicts everyone to be looking in the same direction, despite the differences in head poses.\relax }}{27}}
\newlabel{fig:deepGaze}{{4.8}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hololens Unity Application}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}ROS Node}{27}}
\@writefile{toc}{\contentsline {paragraph}{}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}HoloCamera}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces ROS\# acts as a ROS wrapper around the Unity application, allowing for seamless communication with other ROS nodes.\relax }}{28}}
\newlabel{fig:holoROSWrapper}{{4.9}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Video Streaming Choices}{28}}
\newlabel{sec:videoStreaming}{{4.3.2}{28}}
\@writefile{toc}{\contentsline {paragraph}{Windows Device Portal}{28}}
\@writefile{toc}{\contentsline {paragraph}{Microsoft HoloLensForCV}{28}}
\@writefile{toc}{\contentsline {paragraph}{Unity Camera Stream}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Module Description}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}HoloWorld}{29}}
\@writefile{toc}{\contentsline {subsubsection}{World Manager}{29}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA Manager}{29}}
\@writefile{toc}{\contentsline {paragraph}{Alignment}{29}}
\@writefile{toc}{\contentsline {paragraph}{}{29}}
\citation{Zolotas2018,Chacon-Quesada}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces We need a way of knowing if a detected object is in front of the wheelchair, or if the PWU is looking to the side. \relax }}{30}}
\newlabel{fig:holoArtaAlignment}{{4.10}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}ARTA}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Breakdown}{30}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces System diagram detailing individual components of the programs running on each device.\relax }}{31}}
\newlabel{fig:detailedHL}{{5.1}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Human Detection \& Direction System}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The HDD System is sub-divided into two ROS packages, YOLO and YACHT. We show that YACHT has two sub-nodes which both depend on the outputs of the detector.\relax }}{32}}
\newlabel{fig:detailedHDD}{{5.2}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Hardware \& Software Dependencies}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{32}}
\citation{darknet13}
\@writefile{toc}{\contentsline {subsubsection}{Software}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}YOLO Object Detector}{33}}
\@writefile{toc}{\contentsline {subsubsection}{Darknet}{33}}
\@writefile{toc}{\contentsline {subsubsection}{Darknet in ROS}{34}}
\@writefile{toc}{\contentsline {paragraph}{}{34}}
\@writefile{toc}{\contentsline {paragraph}{}{34}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Darknet IMAGE Python wrappers for seamless ROS integration.}{34}}
\citation{Shao}
\@writefile{toc}{\contentsline {subsubsection}{YOLO Detection Algorithm}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization of the YOLO person detection algorithm dividing a resized square image into grid cells.\relax }}{35}}
\newlabel{fig:yoloViz}{{5.3}{35}}
\@writefile{toc}{\contentsline {subsubsection}{YOLOv3 Tiny vs YOLOv3}{35}}
\@writefile{toc}{\contentsline {subsubsection}{Training YOLOv3 Tiny}{35}}
\@writefile{toc}{\contentsline {paragraph}{CrowdHuman}{35}}
\citation{Shao}
\citation{Shao}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces A comparison of CrowdHuman to other person image datasets \cite  {Shao}, showing the increase in number of people and diversity in the images.\relax }}{36}}
\newlabel{fig:crowdHumanStats}{{5.4}{36}}
\@writefile{toc}{\contentsline {paragraph}{Annotations}{36}}
\@writefile{toc}{\contentsline {paragraph}{}{36}}
\@writefile{toc}{\contentsline {paragraph}{}{36}}
\@writefile{toc}{\contentsline {paragraph}{}{36}}
\@writefile{toc}{\contentsline {paragraph}{}{36}}
\@writefile{toc}{\contentsline {paragraph}{Converting Annotations}{36}}
\@writefile{toc}{\contentsline {paragraph}{Training Parameters}{37}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}Training parameters used for YOLOv3 Tiny on the CrowdHuman dataset}{37}}
\@writefile{toc}{\contentsline {paragraph}{Training Process}{37}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluating Trained Model}{37}}
\@writefile{toc}{\contentsline {paragraph}{Hololens Videos}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Trained model detects people and heads at different ranges. We also see it can detect people far away and accurately detect their heads.\relax }}{37}}
\newlabel{fig:yoloRange}{{5.5}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Evaluating the model on Sherfield Walkway test video. We notice that it is able to small figures close together at a distance.\relax }}{38}}
\newlabel{fig:yoloSherfield}{{5.6}{38}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Node}{38}}
\newlabel{sec:nodeYOLO}{{5.1.2}{38}}
\@writefile{toc}{\contentsline {paragraph}{ROS Topics}{38}}
\@writefile{toc}{\contentsline {paragraph}{}{38}}
\citation{Wojke2018}
\newlabel{bbmsg}{{5.3}{39}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}BoundingBox.msg}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}YACHT Package}{39}}
\@writefile{toc}{\contentsline {paragraph}{ROS Communication}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}YACHT: Tracker}{39}}
\@writefile{toc}{\contentsline {subsubsection}{Deep SORT}{39}}
\@writefile{toc}{\contentsline {paragraph}{Modifications}{39}}
\@writefile{toc}{\contentsline {paragraph}{Deep Association Metric}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Visualization of delay on CPU bound Deep SORT\relax }}{40}}
\newlabel{fig:deepSortCPU}{{5.7}{40}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}Deep SORT Tensorflow GPU modifications}{40}}
\@writefile{toc}{\contentsline {paragraph}{Trackers \& Tracking}{40}}
\citation{Shao}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Visualization of Deep SORT matching. IOU matching is used as a final check for tracks that are unmatched by feature vector Nearest Neighbour matching.\relax }}{41}}
\newlabel{fig:deepSortMatch}{{5.8}{41}}
\@writefile{toc}{\contentsline {subsubsection}{Linear Extrapolation}{41}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topic}{41}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.5}ROS message structure for BoundingBoxID.msg}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}YACHT: Direction}{41}}
\@writefile{toc}{\contentsline {paragraph}{}{41}}
\citation{Jia}
\@writefile{toc}{\contentsline {subsubsection}{OpenPose}{42}}
\@writefile{toc}{\contentsline {paragraph}{Installation \& Setup}{42}}
\@writefile{toc}{\contentsline {paragraph}{Model}{42}}
\@writefile{toc}{\contentsline {subsubsection}{KeyPoint Estimation}{42}}
\newlabel{sec:bottomUp}{{5.1.5}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison of keypoint estimation at different scales\relax }}{43}}
\newlabel{fig:openposeKP}{{5.9}{43}}
\@writefile{toc}{\contentsline {subsubsection}{Defining Direction}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Body keypoint estimation of different views. These images show that the network can differentiate between left and right limbs.\relax }}{43}}
\newlabel{fig:keypointShrey}{{5.10}{43}}
\@writefile{toc}{\contentsline {paragraph}{Method}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Significant key-points for determining whether a person is facing the camera.\relax }}{44}}
\newlabel{tab:keypoints}{{5.1}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Implementation \& Detection Matching}{44}}
\newlabel{lst:matchDetPose}{{5.6}{44}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.6}Direction and Detection Matching code in people\_direction.py}{44}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topic}{44}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.7}ROS message structure for BoundingBoxDirection.msg}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hololens Unity Application}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Components of the Unity application running on the Hololens. We divide the program into two sub-programs, HoloCamera and HoloWorld.\relax }}{45}}
\newlabel{fig:detailedHololens}{{5.11}{45}}
\@writefile{toc}{\contentsline {paragraph}{}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Hardware \& Software Dependencies}{45}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{45}}
\@writefile{toc}{\contentsline {paragraph}{Microsoft Hololens}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The Hololens is larger compared to its competition. However, the increased number of sensors and wider spread usage makes it an ideal AR device for this project.\relax }}{46}}
\newlabel{fig:holodevice}{{5.12}{46}}
\@writefile{toc}{\contentsline {subsubsection}{Software}{46}}
\@writefile{toc}{\contentsline {paragraph}{Universal Windows Platform}{46}}
\@writefile{toc}{\contentsline {paragraph}{Unity}{46}}
\@writefile{toc}{\contentsline {paragraph}{Development Tools}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Hololens Locatable Camera}{47}}
\@writefile{toc}{\contentsline {subsubsection}{Specifications}{47}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces These images were taken from the same position. The Hololens has a reduced FOV and is unable to capture the whole desk.\relax }}{47}}
\newlabel{fig:holoVsIphone}{{5.13}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Hololens Video Camera Stream}{47}}
\bibstyle{unsrt}
\bibdata{fyp_report.bib}
\bibcite{Hou2010}{{1}{}{{}}{{}}}
\bibcite{Zeng2017}{{2}{}{{}}{{}}}
\bibcite{Redmon}{{3}{}{{}}{{}}}
\bibcite{Piccardi2004}{{4}{}{{}}{{}}}
\bibcite{Hirabayashi}{{5}{}{{}}{{}}}
\bibcite{Viola2001}{{6}{}{{}}{{}}}
\bibcite{Dalal2005}{{7}{}{{}}{{}}}
\bibcite{Girshick2014}{{8}{}{{}}{{}}}
\bibcite{Dicle2013}{{9}{}{{}}{{}}}
\bibcite{Bewley2016}{{10}{}{{}}{{}}}
\bibcite{Kalman1961}{{11}{}{{}}{{}}}
\bibcite{Wojke2018}{{12}{}{{}}{{}}}
\bibcite{Valenti2012}{{13}{}{{}}{{}}}
\bibcite{Murphy-Chutorian2009}{{14}{}{{}}{{}}}
\bibcite{Kazemi2014}{{15}{}{{}}{{}}}
\bibcite{Papandreou2017}{{16}{}{{}}{{}}}
\bibcite{Ren2017}{{17}{}{{}}{{}}}
\bibcite{He2016}{{18}{}{{}}{{}}}
\bibcite{Pishchulin}{{19}{}{{}}{{}}}
\bibcite{Cao2017}{{20}{}{{}}{{}}}
\bibcite{Bailey2006a}{{21}{}{{}}{{}}}
\bibcite{Cadena2016}{{22}{}{{}}{{}}}
\bibcite{Taketomi2017}{{23}{}{{}}{{}}}
\bibcite{Nister2004}{{24}{}{{}}{{}}}
\bibcite{Milgram1994}{{25}{}{{}}{{}}}
\bibcite{Microsofta}{{26}{}{{}}{{}}}
\bibcite{Microsoft2015}{{27}{}{{}}{{}}}
\bibcite{Zolotas2018}{{28}{}{{}}{{}}}
\bibcite{Chacon-Quesada}{{29}{}{{}}{{}}}
\bibcite{Jin2017}{{30}{}{{}}{{}}}
\bibcite{Insafutdinov}{{31}{}{{}}{{}}}
\bibcite{Detectron2018}{{32}{}{{}}{{}}}
\bibcite{Rena}{{33}{}{{}}{{}}}
\bibcite{Redmon2018}{{34}{}{{}}{{}}}
\bibcite{Lin}{{35}{}{{}}{{}}}
\bibcite{Shao}{{36}{}{{}}{{}}}
\bibcite{Milan}{{37}{}{{}}{{}}}
\bibcite{Leutenegger2019}{{38}{}{{}}{{}}}
\bibcite{Patacchiola2017a}{{39}{}{{}}{{}}}
\bibcite{darknet13}{{40}{}{{}}{{}}}
\bibcite{Jia}{{41}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\providecommand\totalcount@set[2]{}
\totalcount@set{page}{53}
