\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {paragraph}{Abstract}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{Acknowledgements}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction and Requirements}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{5}}
\citation{Hou2010}
\citation{Zeng2017}
\citation{Zeng2017}
\citation{Redmon}
\citation{Redmon}
\citation{Piccardi2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Human Detection}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Direction of Research}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Review of Existing Methodologies}{6}}
\@writefile{toc}{\contentsline {paragraph}{}{6}}
\citation{Hirabayashi}
\citation{Viola2001}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Comparison of Foreground Detection and Scanning Windows. The number of bounding boxes drawn by the scanning window shows the computational complexity of the method.\relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:foregroundVsScanning}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Foreground Detection}{7}}
\@writefile{toc}{\contentsline {paragraph}{}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Scanning Windows}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Classical Object Detection}{7}}
\@writefile{toc}{\contentsline {paragraph}{Haar Cascades}{7}}
\citation{Dalal2005}
\citation{Girshick2014}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{toc}{\contentsline {paragraph}{Histograms of Oriented Gradients}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Deep Learning Object Detection}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the R-CNN approach. The number of region proposals contributes to the latency of this method.\relax }}{8}}
\citation{Redmon}
\@writefile{toc}{\contentsline {paragraph}{R-CNN}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\newlabel{sec:backYOLO}{{2.1.2}{9}}
\@writefile{toc}{\contentsline {paragraph}{YOLO}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces YOLO algorithm dividing a square image into grid-cells for bounding box prediction.\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Comments}{9}}
\newlabel{sec:detector}{{2.1.3}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\citation{Dicle2013}
\citation{Bewley2016}
\citation{Kalman1961}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Tracking}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Direction of Research}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Review of Exisiting Methodologies}{10}}
\newlabel{sec:objectTrack}{{2.2.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{SORT}{10}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\citation{Wojke2018}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Deep SORT}{11}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{11}}
\@writefile{toc}{\contentsline {paragraph}{}{11}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Comments}{11}}
\newlabel{sec:objectTrackComments}{{2.2.3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Head and Body Pose Estimation}{11}}
\citation{Valenti2012}
\citation{Murphy-Chutorian2009}
\citation{Kazemi2014}
\citation{Papandreou2017}
\citation{Ren2017}
\citation{He2016}
\citation{Pishchulin}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Direction of Research}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Review of Existing Methodologies}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Head Pose Estimation}{12}}
\newlabel{sec:backHeadPoseEstimation}{{2.3.2}{12}}
\@writefile{toc}{\contentsline {paragraph}{Facial Landmark Detection}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Body Pose Estimation}{12}}
\newlabel{sec:backBodyPoseEstimation}{{2.3.2}{12}}
\@writefile{toc}{\contentsline {paragraph}{PoseNet}{12}}
\citation{Cao2017}
\citation{Cao2017}
\citation{Cao2017}
\citation{Bailey2006a}
\@writefile{toc}{\contentsline {paragraph}{OpenPose}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces OpenPose body pose detection \cite  {Cao2017}. The network is able to determine an estimate of the key-points hidden by objects.\relax }}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Comments}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}SLAM}{13}}
\citation{Cadena2016}
\citation{Taketomi2017}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Direction of Research}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Review of Existing Methodologies}{14}}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Visual SLAM}{14}}
\@writefile{toc}{\contentsline {paragraph}{Initialization}{14}}
\citation{Nister2004}
\citation{Milgram1994}
\@writefile{toc}{\contentsline {paragraph}{Tracking}{15}}
\@writefile{toc}{\contentsline {paragraph}{Mapping}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Comments}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Augmented Reality Headsets}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Direction of Research}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Review of Existing Methods}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Microsoft Hololens}{15}}
\newlabel{back:holo}{{2.5.2}{15}}
\citation{Microsofta}
\citation{Microsoft2015}
\citation{Zolotas2018}
\citation{Chacon-Quesada}
\citation{Chacon-Quesada}
\citation{Chacon-Quesada}
\@writefile{toc}{\contentsline {paragraph}{Holograms}{16}}
\@writefile{toc}{\contentsline {paragraph}{}{16}}
\@writefile{toc}{\contentsline {paragraph}{Hardware Specifications}{16}}
\@writefile{toc}{\contentsline {paragraph}{}{16}}
\@writefile{toc}{\contentsline {paragraph}{Personal Robotics Lab}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Comments}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces AR visualizations and markers for interaction \cite  {Chacon-Quesada}\relax }}{17}}
\citation{Chacon-Quesada}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Requirements Capture}{18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Project Deliverable}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Human Detection and Direction}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{toc}{\contentsline {paragraph}{Features}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Obstacle Mapping \& Visualization }{19}}
\@writefile{toc}{\contentsline {paragraph}{Features}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Reactive Control}{19}}
\@writefile{toc}{\contentsline {paragraph}{Features}{19}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Analysis and Design}{20}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:4}{{4}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Design Overview}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces High level system diagram showing the flow of messages from ARTA and the HDD system through the intermediary device, the Hololens.\relax }}{20}}
\newlabel{fig:simplifiedHL}{{4.1}{20}}
\citation{Bewley2016,Jin2017}
\citation{Insafutdinov}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Hardware}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hardware description of devices each system runs on.\relax }}{21}}
\newlabel{tab:hardware}{{4.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}System Communication}{21}}
\newlabel{sec:systemComms}{{4.1.2}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Robotic Operating System}{21}}
\@writefile{toc}{\contentsline {paragraph}{ROS Topics}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Human Detection \& Direction System}{21}}
\citation{Chacon-Quesada,Detectron2018,Rena}
\citation{Redmon}
\citation{Redmon2018}
\citation{Lin}
\citation{Shao}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}YOLO Object Detector}{22}}
\newlabel{sec:yolo}{{4.2.1}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of Detector}{22}}
\@writefile{toc}{\contentsline {paragraph}{}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Pre-trained Model vs Training}{22}}
\newlabel{sec:designYOLO}{{4.2.1}{22}}
\@writefile{toc}{\contentsline {paragraph}{Comparing Models}{22}}
\@writefile{toc}{\contentsline {paragraph}{Pedestrian Dataset}{22}}
\citation{Wojke2018}
\citation{Cao2017}
\citation{Bewley2016}
\citation{Wojke2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Model comparison on a video from Sherfield Walkway at Imperial College London. The trained model is able to better detect figures at a distance.\relax }}{23}}
\newlabel{fig:yoloCHvsCoco}{{4.2}{23}}
\@writefile{toc}{\contentsline {paragraph}{Analysis}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}YACHT: Yet Another Crowd Human Tracker}{23}}
\@writefile{toc}{\contentsline {paragraph}{}{23}}
\@writefile{toc}{\contentsline {subsubsection}{YACHT Tracker: Object Tracking}{23}}
\newlabel{sec:YACHT}{{4.2.2}{23}}
\@writefile{toc}{\contentsline {paragraph}{SORT}{23}}
\citation{Milan}
\citation{Milan}
\@writefile{toc}{\contentsline {paragraph}{Deep SORT}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces MOT16 benchmark \cite  {Milan} (L) using our YOLO model (M) for Deep SORT (R). The increased detection accuracy results in slightly better initial detections, so tracking is more accurate.\relax }}{24}}
\newlabel{fig:deepSortMOT}{{4.3}{24}}
\@writefile{toc}{\contentsline {paragraph}{Analysis}{24}}
\citation{Leutenegger2019}
\citation{Leutenegger2019}
\@writefile{toc}{\contentsline {subsubsection}{Object Tracking for Direction Inference}{25}}
\newlabel{sec:objecTrackingDirection}{{4.2.2}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Linear extrapolation works for objects that move across the frame, but it becomes difficult to determine the direction when mostly vertical motion occurs\relax }}{25}}
\newlabel{fig:linExProblem}{{4.4}{25}}
\@writefile{toc}{\contentsline {paragraph}{Algorithm}{25}}
\@writefile{toc}{\contentsline {paragraph}{Issues}{25}}
\citation{Cao2017}
\citation{Cao2017}
\citation{Cao2017}
\citation{Patacchiola2017a}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Pinhole Camera projection and the loss of the z-axis \cite  {Leutenegger2019}. As such, it is not possible to determine the exact distance to an object without a depth camera.\relax }}{26}}
\newlabel{fig:pinhole}{{4.5}{26}}
\@writefile{toc}{\contentsline {paragraph}{}{26}}
\@writefile{toc}{\contentsline {subsubsection}{YACHT Direction: Body Pose Estimation}{26}}
\newlabel{des:YACHTBody}{{4.2.2}{26}}
\@writefile{toc}{\contentsline {paragraph}{Object Detectors \& Bottom-Up Approaches}{26}}
\newlabel{des:body_25}{{4.2.2}{26}}
\@writefile{toc}{\contentsline {paragraph}{Keypoint Estimation}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Keypoints produced by the BODY\_25 model \cite  {Cao2017}.\relax }}{27}}
\newlabel{fig:bodyKeyPoints}{{4.6}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Head Pose Estimation}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Initial HDD System with the HeadPose Node before removal.\relax }}{27}}
\newlabel{fig:headPoseHDD}{{4.7}{27}}
\@writefile{toc}{\contentsline {paragraph}{Head Detection}{27}}
\@writefile{toc}{\contentsline {paragraph}{Reasons for removal}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces DeepGaze head pose estimator on sample images. We notice in (b) that DeepGaze predicts everyone to be looking in the same direction, despite the differences in head poses.\relax }}{28}}
\newlabel{fig:deepGaze}{{4.8}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hololens Unity Application}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}ROS Node}{28}}
\newlabel{sec:rossharp}{{4.3.1}{28}}
\@writefile{toc}{\contentsline {paragraph}{}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}HoloCamera}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces ROS\# acts as a ROS wrapper around the Unity application, allowing for seamless communication with other ROS nodes.\relax }}{29}}
\newlabel{fig:holoROSWrapper}{{4.9}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Video Streaming Choices}{29}}
\newlabel{sec:videoStreaming}{{4.3.2}{29}}
\@writefile{toc}{\contentsline {paragraph}{Windows Device Portal}{29}}
\@writefile{toc}{\contentsline {paragraph}{Microsoft HoloLensForCV}{29}}
\@writefile{toc}{\contentsline {paragraph}{Unity Camera Stream}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Module Description}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}HoloWorld}{30}}
\@writefile{toc}{\contentsline {subsubsection}{World Manager}{30}}
\@writefile{toc}{\contentsline {subsubsection}{ARTA Manager}{30}}
\@writefile{toc}{\contentsline {paragraph}{Alignment}{30}}
\@writefile{toc}{\contentsline {paragraph}{}{30}}
\citation{Zolotas2018,Chacon-Quesada}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces We need a way of knowing if a detected object is in front of the wheelchair, or if the PWU is looking to the side. \relax }}{31}}
\newlabel{fig:holoArtaAlignment}{{4.10}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}ARTA}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Breakdown}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Implementation}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces System diagram detailing individual components of the programs running on each device.\relax }}{32}}
\newlabel{fig:detailedHL}{{5.1}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Human Detection \& Direction System}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The HDD System is sub-divided into two ROS packages, YOLO and YACHT. We show that YACHT has two sub-nodes which both depend on the outputs of the detector.\relax }}{33}}
\newlabel{fig:detailedHDD}{{5.2}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Hardware \& Software Dependencies}{33}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{33}}
\citation{darknet13}
\@writefile{toc}{\contentsline {subsubsection}{Software}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}YOLO Object Detector}{34}}
\@writefile{toc}{\contentsline {subsubsection}{Darknet}{34}}
\@writefile{toc}{\contentsline {subsubsection}{Darknet in ROS}{35}}
\@writefile{toc}{\contentsline {paragraph}{}{35}}
\@writefile{toc}{\contentsline {paragraph}{}{35}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Darknet IMAGE Python wrappers for seamless ROS integration.}{35}}
\citation{Shao}
\@writefile{toc}{\contentsline {subsubsection}{YOLO Detection Algorithm}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Visualization of the YOLO person detection algorithm dividing a resized square image into grid cells.\relax }}{36}}
\newlabel{fig:yoloViz}{{5.3}{36}}
\@writefile{toc}{\contentsline {subsubsection}{YOLOv3 Tiny vs YOLOv3}{36}}
\@writefile{toc}{\contentsline {subsubsection}{Training YOLOv3 Tiny}{36}}
\@writefile{toc}{\contentsline {paragraph}{CrowdHuman}{36}}
\citation{Shao}
\citation{Shao}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces A comparison of CrowdHuman to other person image datasets \cite  {Shao}, showing the increase in number of people and diversity in the images.\relax }}{37}}
\newlabel{fig:crowdHumanStats}{{5.4}{37}}
\@writefile{toc}{\contentsline {paragraph}{Annotations}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{}{37}}
\@writefile{toc}{\contentsline {paragraph}{Converting Annotations}{37}}
\@writefile{toc}{\contentsline {paragraph}{Training Parameters}{38}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}Training parameters used for YOLOv3 Tiny on the CrowdHuman dataset}{38}}
\@writefile{toc}{\contentsline {paragraph}{Training Process}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluating Trained Model}{38}}
\@writefile{toc}{\contentsline {paragraph}{Hololens Videos}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Trained model detects people and heads at different ranges. We also see it can detect people far away and accurately detect their heads.\relax }}{38}}
\newlabel{fig:yoloRange}{{5.5}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Evaluating the model on Sherfield Walkway test video. We notice that it is able to small figures close together at a distance.\relax }}{39}}
\newlabel{fig:yoloSherfield}{{5.6}{39}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Node}{39}}
\newlabel{sec:nodeYOLO}{{5.1.2}{39}}
\@writefile{toc}{\contentsline {paragraph}{ROS Topics}{39}}
\@writefile{toc}{\contentsline {paragraph}{}{39}}
\citation{Wojke2018}
\newlabel{bbmsg}{{5.3}{40}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}BoundingBox.msg}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}YACHT Package}{40}}
\@writefile{toc}{\contentsline {paragraph}{ROS Communication}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}YACHT: Tracker}{40}}
\@writefile{toc}{\contentsline {subsubsection}{Deep SORT}{40}}
\@writefile{toc}{\contentsline {paragraph}{Modifications}{40}}
\@writefile{toc}{\contentsline {paragraph}{Deep Association Metric}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Visualization of delay on CPU bound Deep SORT\relax }}{41}}
\newlabel{fig:deepSortCPU}{{5.7}{41}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}Deep SORT Tensorflow GPU modifications}{41}}
\@writefile{toc}{\contentsline {paragraph}{Trackers \& Tracking}{41}}
\citation{Shao}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Visualization of Deep SORT matching. IOU matching is used as a final check for tracks that are unmatched by feature vector Nearest Neighbour matching.\relax }}{42}}
\newlabel{fig:deepSortMatch}{{5.8}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Linear Extrapolation}{42}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topic}{42}}
\newlabel{sec:yachtTrackROS}{{5.1.4}{42}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.5}ROS message structure for BoundingBoxID.msg}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}YACHT: Direction}{42}}
\@writefile{toc}{\contentsline {paragraph}{}{42}}
\citation{Jia}
\@writefile{toc}{\contentsline {subsubsection}{OpenPose}{43}}
\@writefile{toc}{\contentsline {paragraph}{Installation \& Setup}{43}}
\@writefile{toc}{\contentsline {paragraph}{Model}{43}}
\@writefile{toc}{\contentsline {subsubsection}{KeyPoint Estimation}{43}}
\newlabel{sec:bottomUp}{{5.1.5}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison of keypoint estimation at different scales\relax }}{44}}
\newlabel{fig:openposeKP}{{5.9}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Defining Direction}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Body keypoint estimation of different views. These images show that the network can differentiate between left and right limbs.\relax }}{44}}
\newlabel{fig:keypointShrey}{{5.10}{44}}
\@writefile{toc}{\contentsline {paragraph}{Method}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Significant key-points for determining whether a person is facing the camera.\relax }}{45}}
\newlabel{tab:keypoints}{{5.1}{45}}
\@writefile{toc}{\contentsline {subsubsection}{Implementation \& Detection Matching}{45}}
\newlabel{lst:matchDetPose}{{5.6}{45}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.6}Direction and Detection Matching code in people\_direction.py}{45}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topic}{45}}
\newlabel{sec:yachtDirROS}{{5.1.5}{45}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.7}ROS message structure for BoundingBoxDirection.msg}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hololens Unity Application}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Components of the Unity application running on the Hololens. We divide the program into two sub-programs, HoloCamera and HoloWorld.\relax }}{46}}
\newlabel{fig:detailedHololens}{{5.11}{46}}
\@writefile{toc}{\contentsline {paragraph}{}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Hardware \& Software Dependencies}{46}}
\@writefile{toc}{\contentsline {subsubsection}{Hardware}{46}}
\@writefile{toc}{\contentsline {paragraph}{Microsoft Hololens}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The Hololens is larger compared to its competition. However, the increased number of sensors and wider spread usage makes it an ideal AR device for this project.\relax }}{47}}
\newlabel{fig:holodevice}{{5.12}{47}}
\@writefile{toc}{\contentsline {subsubsection}{Software}{47}}
\@writefile{toc}{\contentsline {paragraph}{Universal Windows Platform}{47}}
\@writefile{toc}{\contentsline {paragraph}{Unity}{47}}
\@writefile{toc}{\contentsline {paragraph}{Development Tools}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Hololens Locatable Camera}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Specifications}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces These images were taken from the same position. The Hololens has a reduced FOV and is unable to capture the whole desk.\relax }}{48}}
\newlabel{fig:holoVsIphone}{{5.13}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Hololens Video Camera Stream}{48}}
\@writefile{toc}{\contentsline {subsubsection}{Image Capture}{49}}
\@writefile{toc}{\contentsline {paragraph}{Vulcan Technologies}{49}}
\@writefile{toc}{\contentsline {paragraph}{Image Capture Pipeline}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The image capture pipeline involves an intermediate Texture2D Unity state.\relax }}{49}}
\newlabel{fig:imgProcPipeline}{{5.14}{49}}
\@writefile{toc}{\contentsline {paragraph}{}{50}}
\@writefile{toc}{\contentsline {subsubsection}{Image Compression}{50}}
\@writefile{toc}{\contentsline {paragraph}{Unity Threading}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Image compression is a time consuming operation that must be done in the main thread. Only after compression is done can the holograms be rendered, limiting the application to 5 FPS.\relax }}{50}}
\newlabel{fig:unityThreads}{{5.15}{50}}
\@writefile{toc}{\contentsline {paragraph}{Limited Frame-rate}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}ROS Communication}{51}}
\@writefile{toc}{\contentsline {subsubsection}{Modifications}{51}}
\@writefile{toc}{\contentsline {subsubsection}{ROS Topics}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Image compression is a time consuming operation that must be done in the main thread. Only after compression is done can the holograms be rendered, limiting the application to 5 FPS.\relax }}{51}}
\newlabel{fig:unityThreads}{{5.16}{51}}
\@writefile{toc}{\contentsline {paragraph}{Compressed Images}{51}}
\bibstyle{unsrt}
\bibdata{fyp_report.bib}
\@writefile{toc}{\contentsline {paragraph}{HDD Topics}{52}}
\bibcite{Hou2010}{{1}{}{{}}{{}}}
\bibcite{Zeng2017}{{2}{}{{}}{{}}}
\bibcite{Redmon}{{3}{}{{}}{{}}}
\bibcite{Piccardi2004}{{4}{}{{}}{{}}}
\bibcite{Hirabayashi}{{5}{}{{}}{{}}}
\bibcite{Viola2001}{{6}{}{{}}{{}}}
\bibcite{Dalal2005}{{7}{}{{}}{{}}}
\bibcite{Girshick2014}{{8}{}{{}}{{}}}
\bibcite{Dicle2013}{{9}{}{{}}{{}}}
\bibcite{Bewley2016}{{10}{}{{}}{{}}}
\bibcite{Kalman1961}{{11}{}{{}}{{}}}
\bibcite{Wojke2018}{{12}{}{{}}{{}}}
\bibcite{Valenti2012}{{13}{}{{}}{{}}}
\bibcite{Murphy-Chutorian2009}{{14}{}{{}}{{}}}
\bibcite{Kazemi2014}{{15}{}{{}}{{}}}
\bibcite{Papandreou2017}{{16}{}{{}}{{}}}
\bibcite{Ren2017}{{17}{}{{}}{{}}}
\bibcite{He2016}{{18}{}{{}}{{}}}
\bibcite{Pishchulin}{{19}{}{{}}{{}}}
\bibcite{Cao2017}{{20}{}{{}}{{}}}
\bibcite{Bailey2006a}{{21}{}{{}}{{}}}
\bibcite{Cadena2016}{{22}{}{{}}{{}}}
\bibcite{Taketomi2017}{{23}{}{{}}{{}}}
\bibcite{Nister2004}{{24}{}{{}}{{}}}
\bibcite{Milgram1994}{{25}{}{{}}{{}}}
\bibcite{Microsofta}{{26}{}{{}}{{}}}
\bibcite{Microsoft2015}{{27}{}{{}}{{}}}
\bibcite{Zolotas2018}{{28}{}{{}}{{}}}
\bibcite{Chacon-Quesada}{{29}{}{{}}{{}}}
\bibcite{Jin2017}{{30}{}{{}}{{}}}
\bibcite{Insafutdinov}{{31}{}{{}}{{}}}
\bibcite{Detectron2018}{{32}{}{{}}{{}}}
\bibcite{Rena}{{33}{}{{}}{{}}}
\bibcite{Redmon2018}{{34}{}{{}}{{}}}
\bibcite{Lin}{{35}{}{{}}{{}}}
\bibcite{Shao}{{36}{}{{}}{{}}}
\bibcite{Milan}{{37}{}{{}}{{}}}
\bibcite{Leutenegger2019}{{38}{}{{}}{{}}}
\bibcite{Patacchiola2017a}{{39}{}{{}}{{}}}
\bibcite{darknet13}{{40}{}{{}}{{}}}
\bibcite{Jia}{{41}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\providecommand\totalcount@set[2]{}
\totalcount@set{page}{57}
