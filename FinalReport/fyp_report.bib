Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Cao2017,
abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
archivePrefix = {arXiv},
arxivId = {1812.08008v2},
author = {Cao, Zhe and Simon, Tomas and Wei, Shih En and Sheikh, Yaser},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.143},
eprint = {1812.08008v2},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - Unknown - OpenPose Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields.pdf:pdf},
isbn = {9781538604571},
pages = {1302--1310},
title = {{Realtime multi-person 2D pose estimation using part affinity fields}},
url = {https://gineshidalgo.com},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Bewley2016,
abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
archivePrefix = {arXiv},
arxivId = {1602.00763v2},
author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2016.7533003},
eprint = {1602.00763v2},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bewley et al. - 2017 - SIMPLE ONLINE AND REALTIME TRACKING.pdf:pdf},
isbn = {9781467399616},
issn = {15224880},
keywords = {Computer Vision,Data Association,Detection,Multiple Object Tracking},
pages = {3464--3468},
title = {{Simple online and realtime tracking}},
url = {https://github.com/abewley/sort},
volume = {2016-Augus},
year = {2016}
}
@inproceedings{Pishchulin,
abstract = {This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation. Models and code available at http://pose.mpi-inf.mpg.de.},
archivePrefix = {arXiv},
arxivId = {1511.06645v2},
author = {Pishchulin, Leonid and Insafutdinov, Eldar and Tang, Siyu and Andres, Bjoern and Andriluka, Mykhaylo and Gehler, Peter and Schiele, Bernt},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.533},
eprint = {1511.06645v2},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pishchulin et al. - Unknown - DeepCut Joint Subset Partition and Labeling for Multi Person Pose Estimation.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {4929--4937},
title = {{DeepCut: Joint subset partition and labeling for multi person pose estimation}},
url = {http://pose.mpi-inf.mpg.de},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{Insafutdinov,
abstract = {The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation. Models and code available at http://pose.mpi-inf.mpg.de},
archivePrefix = {arXiv},
arxivId = {1605.03170v3},
author = {Insafutdinov, Eldar and Pishchulin, Leonid and Andres, Bjoern and Andriluka, Mykhaylo and Schiele, Bernt},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46466-4_3},
eprint = {1605.03170v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Insafutdinov et al. - Unknown - DeeperCut A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model.pdf:pdf},
isbn = {9783319464657},
issn = {16113349},
pages = {34--50},
title = {{Deepercut: A deeper, stronger, and faster multi-person pose estimation model}},
url = {http://pose.mpi-inf.mpg.de},
volume = {9910 LNCS},
year = {2016}
}
@inproceedings{Dicle2013,
abstract = {code},
author = {Dicle, Caglayan and Camps, Octavia I and Sznaier, Mario},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.286},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dicle, Camps, Sznaier - 2013 - The Way They Move Tracking Multiple Targets with Similar Appearance.pdf:pdf},
isbn = {9781479928392},
keywords = {Generalized Linear Assignment,Hankel,Multitarget tracking,Rank Estimation,motion dynamics,tracking},
pages = {2304--2311},
title = {{The way they move: Tracking multiple targets with similar appearance}},
url = {http://robustsystems.coe.neu.edu},
year = {2013}
}
@article{Ren,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497v3},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks(3).pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Convolutional Neural Network !,Index Terms-Object Detection,Region Proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.}},
url = {http://image-net.org/challenges/LSVRC/2015/results http://www.ncbi.nlm.nih.gov/pubmed/27295650},
volume = {39},
year = {2017}
}
@techreport{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP 50 in 51 ms on a Titan X, compared to 57.5 AP 50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
author = {Redmon, Joseph and Farhadi, Ali},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - Unknown - YOLOv3 An Incremental Improvement.pdf:pdf},
pages = {1--6},
title = {{YOLOv3: An Incremental Improvement}},
url = {https://pjreddie.com/yolo/. https://pjreddie.com/media/files/papers/YOLOv3.pdf},
year = {2018}
}
@inproceedings{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524v5},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524v5},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - Unknown - Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5).pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://www.cs.berkeley.edu/˜rbg/rcnn.},
year = {2014}
}
@article{Bailey2006a,
author = {Bailey, T and Durrant-Whyte, H},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey, Durrant-Whyte - 2006 - Simultaneous localisation and mapping (SLAM) Part I - The essential algorithms.pdf:pdf},
journal = {IEEE Robotics and Automation Magazine},
number = {2},
pages = {99--108},
title = {{Simultaneous localisation and mapping (SLAM): Part I - The essential algorithms.}},
volume = {13},
year = {2006}
}
@unpublished{Leutenegger2019,
author = {Leutenegger, Stefan},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leutenegger - 2019 - Representations and Sensors.pdf:pdf},
institution = {Imperial College London},
isbn = {9780470517062},
title = {{Representations and Sensors}},
url = {https://www.imperial.ac.uk/computing/current-students/courses/433/},
year = {2019}
}
@article{Cadena2016,
abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
archivePrefix = {arXiv},
arxivId = {1606.05830},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cadena et al. - 2016 - Past, present, and future of simultaneous localization and mapping Toward the robust-perception age.pdf:pdf},
isbn = {9781479936847},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
pmid = {6576973927449638915},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
volume = {32},
year = {2016}
}
@article{Jin2017,
abstract = {In this paper, we focus on the challenging problem of multi-person pose tracking in the wild. Recent multi-person articulated tracking methods can be categorized into the top-down and bottom-up approaches. We investigate the advantages and disadvantages of both bottom-up and top- down methods on various datasets. We propose a novel bottom-up joint detector, termed as MSPAF to extract multi- scale features and implement a human detector based on recent development of object detection. Incorporating the global context, we use a human detector to rule out bottom- up false alarms which significantly improves the tracking re- sults. Following the commonly used graph partitioning for- mulation, we construct a spatio-temporal graph and solve a minimum cost multicut problem for human pose track- ing. Our proposed method achieves the state-of-the-art per- formance on both ”Multi-Person PoseTrack” dataset and ”ICCV 2017 PoseTrack Challenge” dataset.},
author = {Jin, Sheng and Ma, Xujie and Han, Zhipeng and Wu, Yue and Yang, Wei and Liu, Wentao and Qian, Chen and Ouyang, Wanli},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - Unknown - Towards Multi-Person Pose Tracking Bottom-up and Top-down Methods.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {2},
pages = {4--7},
title = {{Towards Multi-Person Pose Tracking : Bottom-up and Top-down Methods}},
url = {https://posetrack.net/workshops/iccv2017/pdfs/BUTD.pdf},
year = {2017}
}
@techreport{Redmon,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - Unknown - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://pjreddie.com/yolo/}
}
@techreport{Hirabayashi,
abstract = {Vision-based object detection using camera sensors is an essential piece of perception for autonomous vehicles. Various combinations of features and models can be applied to increase the quality and the speed of object detection. A well-known approach uses histograms of oriented gradients (HOG) with deformable models to detect a car in an image [15]. A major challenge of this approach can be found in computational cost introducing a real-time constraint relevant to the real world. In this paper, we present an implementation technique using graphics processing units (GPUs) to accelerate computations of scoring similarity of the input image and the pre-defined models. Our implementation considers the entire program structure as well as the specific algorithm for practical use. We apply the presented technique to the real-world vehicle detection program and demonstrate that our implementation using commodity GPUs can achieve speedups of 3x to 5x in frame-rate over sequential and multithreaded implementations using traditional CPUs.},
author = {Hirabayashi, Manato and Kato, Shinpei and Edahiro, Masato and Takeda, Kazuya and Kawano, Taiki and Mita, Seiichi},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hirabayashi et al. - Unknown - GPU Implementations of Object Detection using HOG Features and Deformable Models.pdf:pdf},
keywords = {Computer Vision,Index Terms-GPGPU,Object Detection},
title = {{GPU Implementations of Object Detection using HOG Features and Deformable Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.709.647{\&}rep=rep1{\&}type=pdf}
}
@article{Murphy-Chutorian2009,
abstract = {The capacity to estimate the head pose of another person is a common human ability that presents a unique challenge for computer vision systems. Compared to face detection and recognition, which have been the primary foci of face-related vision research, identity-invariant head pose estimation has fewer rigorously evaluated systems or generic solutions. In this paper, we discuss the inherent difficulties in head pose estimation and present an organized survey describing the evolution of the field. Our discussion focuses on the advantages and disadvantages of each approach and spans 90 of the most innovative and characteristic papers that have been published on this topic. We compare these systems by focusing on their ability to estimate coarse and fine head pose, highlighting approaches that are well suited for unconstrained environments.},
author = {Murphy-Chutorian, Erik and Trivedi, Mohan Manubhai},
doi = {10.1109/TPAMI.2008.106},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy-Chutorian, Manubhai Trivedi - 2008 - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Head Pose Estimation in Com.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Face analysis,Facial landmarks,Gesture analysis,Head pose estimation,Human-computer interfaces},
number = {4},
pages = {607--626},
title = {{Head pose estimation in computer vision: A survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.8306{\&}rep=rep1{\&}type=pdf},
volume = {31},
year = {2009}
}
@article{Milan,
abstract = {Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods. The first release of the benchmark focuses on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. This paper accompanies a new release of the MOTChallenge benchmark. Unlike the initial release, all videos of MOT16 have been carefully annotated following a consistent protocol. Moreover, it not only offers a significant increase in the number of labeled boxes, but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest.},
archivePrefix = {arXiv},
arxivId = {1603.00831},
author = {Milan, Anton and Leal-Taixe, Laura and Reid, Ian and Roth, Stefan and Schindler, Konrad},
eprint = {1603.00831},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milan et al. - Unknown - MOT16 A Benchmark for Multi-Object Tracking.pdf:pdf},
keywords = {Index Terms-multiple people tracking,benchmark,dataset !,evaluation metrics},
title = {{MOT16: A Benchmark for Multi-Object Tracking}},
url = {http://www.motchallenge.net/ http://arxiv.org/abs/1603.00831},
year = {2016}
}
@misc{darknet13,
author = {Redmon, Joseph},
howpublished = {$\backslash$url{\{}http://pjreddie.com/darknet/{\}}},
title = {{Darknet: Open Source Neural Networks in C}}
}
@article{Rena,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497v3},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks(3).pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Convolutional Neural Network !,Index Terms-Object Detection,Region Proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.}},
url = {http://image-net.org/challenges/LSVRC/2015/results http://www.ncbi.nlm.nih.gov/pubmed/27295650},
volume = {39},
year = {2017}
}
@article{Valenti2012,
abstract = {Head pose and eye location for gaze estimation have been separately studied in numerous works in the literature. Previous research shows that satisfactory accuracy in head pose and eye location estimation can be achieved in constrained settings. However, in the presence of nonfrontal faces, eye locators are not adequate to accurately locate the center of the eyes. On the other hand, head pose estimation techniques are able to deal with these conditions; hence, they may be suited to enhance the accuracy of eye localization. Therefore, in this paper, a hybrid scheme is proposed to combine head pose and eye location information to obtain enhanced gaze estimation. To this end, the transformation matrix obtained from the head pose is used to normalize the eye regions, and in turn, the transformation matrix generated by the found eye location is used to correct the pose estimation procedure. The scheme is designed to enhance the accuracy of eye location estimations, particularly in low-resolution videos, to extend the operative range of the eye locators, and to improve the accuracy of the head pose tracker. These enhanced estimations are then combined to obtain a novel visual gaze estimation system, which uses both eye location and head information to refine the gaze estimates. From the experimental results, it can be derived that the proposed unified scheme improves the accuracy of eye estimations by 16{\%} to 23{\%}. Furthermore, it considerably extends its operating range by more than 15° by overcoming the problems introduced by extreme head poses. Moreover, the accuracy of the head pose tracker is improved by 12{\%} to 24{\%}. Finally, the experimentation on the proposed combined gaze estimation system shows that it is accurate (with a mean error between 2° and 5°) and that it can be used in cases where classic approaches would fail without imposing restraints on the position of the head.},
author = {Valenti, Roberto and Sebe, Nicu and Gevers, Theo},
doi = {10.1109/TIP.2011.2162740},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valenti, Sebe, Gevers - 2012 - Combining Head Pose and Eye Location Information for Gaze Estimation.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Eye center location,gaze estimation,head pose estimation},
number = {2},
pages = {802--815},
title = {{Combining head pose and eye location information for gaze estimation}},
url = {http://ieeexplore.ieee.org.},
volume = {21},
year = {2012}
}
@inproceedings{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
author = {Dalal, Navneet and Triggs, Bill and Dalal, Navneet and Triggs, Bill},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal, Triggs - 2005 - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
keywords = {feature extraction,gradient methods,object detec},
pages = {886--893},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://lear.inrialpes.fr},
year = {2005}
}
@article{Shao,
abstract = {Human detection has witnessed impressive progress in recent years. However, the occlusion issue of detecting human in highly crowded environments is far from solved. To make matters worse, crowd scenarios are still under-represented in current human detection benchmarks. In this paper, we introduce a new dataset, called CrowdHuman, to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. There are a total of {\$}470K{\$} human instances from the train and validation subsets, and {\$}{\~{}}22.6{\$} persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. Baseline performance of state-of-the-art detection frameworks on CrowdHuman is presented. The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks.},
archivePrefix = {arXiv},
arxivId = {1805.00123},
author = {Shao, Shuai and Zhao, Zijian and Li, Boxun and Xiao, Tete and Yu, Gang and Zhang, Xiangyu and Sun, Jian},
eprint = {1805.00123},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shao et al. - Unknown - CrowdHuman A Benchmark for Detecting Human in a Crowd.pdf:pdf},
title = {{CrowdHuman: A Benchmark for Detecting Human in a Crowd}},
url = {https://arxiv.org/pdf/1805.00123.pdf http://arxiv.org/abs/1805.00123},
year = {2018}
}
@article{Hou2010,
abstract = {In this paper, our focus is to segment the foreground area for human detection. It is assumed that the foreground region has been detected. Accurate foreground contours are not required. The developed approach adopts a modified ISM (Implicit Shape Model) to collect some typical local patches of human being and their location information. Individuals are detected by grouping some local patches in the foreground area. The method can get good results in crowded scenes. Some examples based on CAVIAR dataset have been shown. A main contribution of the paper is that ISM model and joint occlusion analysis are combined for individual segmentation. There are mainly two advantages: First, with more sufficient information inside the foreground region, even the individuals inside a dense area can also be handled. Secondly, the method does not require an accurate foreground contour. A rough foreground area can be easily obtained in most situations.},
author = {Hou, Ya Li and Pang, Grantham K.H.},
doi = {10.1109/ICIP.2010.5651982},
file = {:home/aufar/Downloads/Human{\_}detection{\_}in{\_}crowded{\_}scenes.pdf:pdf},
isbn = {9781424479948},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Human detection,Implicit shape model,Occlusions},
number = {September 2010},
pages = {721--724},
title = {{Human detection in crowded scenes}},
url = {https://www.researchgate.net/publication/221119054{\_}Human{\_}detection{\_}in{\_}crowded{\_}scenes},
year = {2010}
}
@misc{Detectron2018,
author = {Girshick, Ross and Radosavovic, Ilija and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and He, Kaiming},
howpublished = {$\backslash$url{\{}https://github.com/facebookresearch/detectron{\}}},
title = {{Detectron}},
year = {2018}
}
@inproceedings{Papandreou2017,
abstract = {We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5{\%} absolute improvement compared to the previous best performing method on the same dataset.},
archivePrefix = {arXiv},
arxivId = {1701.01779},
author = {Papandreou, George and Zhu, Tyler and Kanazawa, Nori and Toshev, Alexander and Tompson, Jonathan and Bregler, Chris and Murphy, Kevin},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.395},
eprint = {1701.01779},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papandreou et al. - 2017 - Towards Accurate Multi-person Pose Estimation in the Wild.pdf:pdf},
isbn = {9781538604571},
month = {jan},
pages = {3711--3719},
title = {{Towards accurate multi-person pose estimation in the wild}},
url = {http://arxiv.org/abs/1701.01779},
volume = {2017-Janua},
year = {2017}
}
@article{Kairy2014,
abstract = {Power wheelchairs (PWCs) can have a positive impact on user well-being, self-esteem, pain, activity and participation. Newly developed intelligent power wheelchairs (IPWs), allowing autonomous or collaboratively-controlled navigation, could enhance mobility of individuals not able to use, or having difficulty using, standard PWCs. The objective of this study was to explore the perspectives of PWC users (PWUs) and their caregivers regarding if and how IPWs could impact on current challenges faced by PWUs, as well as inform current development of IPWs. A qualitative exploratory study using individual interviews was conducted with PWUs (n = 12) and caregivers (n = 4). A semi-structured interview guide and video were used to facilitate informed discussion regarding IPWs. Thematic analysis revealed three main themes: (1) "challenging situations that may be overcome by an IPW" described how the IPW features of obstacle avoidance, path following, and target following could alleviate PWUs' identified mobility difficulties; (2) "cautious optimism concerning IPW use revealed participants" addresses concerns regarding using an IPW as well as technological suggestions; (3) "defining the potential IPW user" revealed characteristics of PWUs that would benefit from IPW use. Findings indicate  how IPW use may help overcome PWC difficulties and confirm the importance of user input in the ongoing development of IPWs. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Kairy, Dahlia and Rushton, Paula W. and Archambault, Philippe and Pituch, Evelina and Torkia, Caryne and {El Fathi}, Anas and Stone, Paula and Routhier, Fran{\c{c}}ois and Forget, Robert and Demers, Louise and Pineau, Joelle and Gourdeau, Richard},
doi = {10.3390/ijerph110202244},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kairy et al. - 2014 - Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair us.pdf:pdf},
isbn = {1660-4601},
issn = {16617827},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Disability,Intelligent power wheelchair,Mobility,Navigation,Obstacle-avoidance,Path following,Safety,User-centered design},
number = {2},
pages = {2244--2261},
pmid = {24566051},
title = {{Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair use: A qualitative study}},
volume = {11},
year = {2014}
}
@inproceedings{Kazemi2014,
abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with ef- ficient feature selection. Different regularization strategies and its importance to combat overfitting are also investi- gated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
author = {Kazemi, Vahid and Sullivan, Josephine},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.241},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kazemi, Kth - Unknown - One Millisecond Face Alignment with an Ensemble of Regression Trees.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
keywords = {Decision Trees,Face Alignment,Gradient Boosting,Real-Time},
pages = {1867--1874},
title = {{One Millisecond Face Alignment with an Ensemble of Regression Trees}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2014/papers/Kazemi{\_}One{\_}Millisecond{\_}Face{\_}2014{\_}CVPR{\_}paper.pdf},
year = {2014}
}
@article{Milgram1994,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Milgram, Paul and Kishino, Fumio},
doi = {10.1.1.102.4646},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milgram, Kishino - 1994 - A TAXONOMY OF MIXED REALITY VISUAL DISPLAYS.pdf:pdf},
isbn = {0916-8532},
issn = {0916-8532},
journal = {IEICE Transactions on Information Systems},
number = {12},
pages = {1--15},
pmid = {25246403},
title = {{A TAXONOMY OF MIXED REALITY VISUAL DISPLAYS}},
volume = {E77},
year = {1994}
}
@misc{Microsofta,
author = {Microsoft},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microsoft - 2018 - Windows Mixed Reality Documentation.pdf:pdf},
title = {{Windows Mixed Reality Documentation}},
year = {2018}
}
@inproceedings{Piccardi2004,
abstract = {Background subtraction is a widely used approoch for detecting moving objects @om static cameras. Mony different methods have been proposed over the recent years and both the novice and the exprt can be confused about iheir benefits and limitations. In order to overcome this problem, this poper provides a review of ihe main methods and an original categorisotion based on speed, memoy requirements and accuracy, Such o review can effectively guide ihe designer to select the most suitoble meihod for a given application in a principled way. Methods reviewed include parametric and non-porametric background density estimates and spatial correlation approaches.},
author = {Piccardi, Massimo},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piccardi - Unknown - Background subtraction techniques a review.pdf:pdf},
keywords = {background subtraction,moving object detection,parametric and non-parametric approach'zs,spatial correlation},
title = {{Background subtraction techniques: a review*}},
url = {http://profs.sci.univr.it/{~}cristanm/teaching/sar{\_}files/lezione4/Piccardi.pdf},
year = {2004}
}
@inproceedings{Lin,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312v3},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Microsoft COCO Common Objects in Context.pdf:pdf},
issn = {16113349},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
url = {https://arxiv.org/pdf/1405.0312.pdf},
volume = {8693 LNCS},
year = {2014}
}
@article{Zeng2017,
abstract = {{\textcopyright} 2017 by the authors. Background subtraction based on change detection is the first step in many computer vision systems. Many background subtraction methods have been proposed to detect foreground objects through background modeling. However, most of these methods are pixel-based, which only use pixel-by-pixel comparisons, and a few others are spatial-based, which take the neighborhood of each analyzed pixel into consideration. In this paper, inspired by a illumination- invariant feature based on locality-sensitive histograms proposed for object tracking, we first develop a novel texture descriptor named the Local Similarity Statistical Descriptor (LSSD), which calculates the similarity between the current pixel and its neighbors. The LSSD descriptor shows good performance in illumination variation and dynamic background scenes. Then, we model each background pixel representation with a combination of color features and LSSD features. These features are then embedded in a low-cost and highly efficient background modeling framework. The color and texture features have their own merits and demerits; they can compensate each other, resulting in better performance. Both quantitative and qualitative evaluations carried out on the change detection dataset are provided to demonstrate the effectiveness of our method.},
author = {Zeng, Dongdong and Zhu, Ming and Zhou, Tongxue and Xu, Fang and Yang, Hang},
doi = {10.3390/app7100989},
file = {:home/aufar/Downloads/fyp{\_}papers/applsci-07-00989-v2.pdf:pdf},
isbn = {8613514499},
journal = {Applied Sciences},
keywords = {background subtraction,descriptor,local similarity statistical,locality-sensitive histograms,video surveillance},
number = {10},
pages = {989},
title = {{Robust Background Subtraction via the Local Similarity Statistical Descriptor}},
url = {https://www.mdpi.com/2076-3417/7/10/989},
volume = {7},
year = {2017}
}
@inproceedings{Wojke2018,
abstract = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45{\%}, achieving overall competitive performance at high frame rates.},
archivePrefix = {arXiv},
arxivId = {1703.07402v1},
author = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296962},
eprint = {1703.07402v1},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wojke, Bewley, Paulus - Unknown - SIMPLE ONLINE AND REALTIME TRACKING WITH A DEEP ASSOCIATION METRIC(2).pdf:pdf},
isbn = {9781509021758},
issn = {15224880},
keywords = {Computer Vision,Data Association,Multiple Object Tracking},
pages = {3645--3649},
title = {{Simple online and realtime tracking with a deep association metric}},
url = {https://arxiv.org/pdf/1703.07402.pdf},
volume = {2017-Septe},
year = {2018}
}
@article{Jia,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
eprint = {1408.5093},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia et al. - Unknown - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
keywords = {Computer Vision,D22 [Software Engineering]: [Design Tools and Tech,Design,Experimentation Keywords Open Source,I51 [Pattern Recognition]: [Applications-Computer,I51 [Pattern Recognition]: [Models-Neural Nets] Ge,Machine Learning * Corresponding Authors,Neural Networks,Parallel Computation},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://demo.caffe.berkeleyvision.org/ http://arxiv.org/abs/1408.5093},
year = {2014}
}
@article{Chacon-Quesada,
abstract = {In this paper we present a novel augmented reality head mounted display user interface for controlling a robotic wheelchair for people with limited mobility. To lower the cognitive requirements needed to control the wheelchair, we propose integration of a smart wheelchair with an eye-tracking enabled head-mounted display. We propose a novel platform that integrates multiple user interface interaction methods for aiming at and selecting affordances derived by on-board perception capabilities such as laser-scanner readings and cameras. We demonstrate the effectiveness of the approach by evaluating our platform in two realistic scenarios: 1) Door detection, where the affordance corresponds to a Door object and the Go-Through action and 2) People detection, where the affordance corresponds to a Person and the Approach action. To the best of our knowledge, this is the first demonstration of a augmented reality head-mounted display user interface for controlling a smart wheelchair.},
author = {Chac{\'{o}}n-Quesada, Rodrigo and Demiris, Yiannis},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chac{\'{o}}n-Quesada, Demiris - 2018 - Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances.pdf:pdf},
keywords = {Index terms-Affordances,augmented reality,head mounted display,smart wheelchairs,user interface},
pages = {1--4},
title = {{Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances}},
url = {http://www.imperial.ac.uk/personal-robotics/robots/},
year = {2018}
}
@article{Zolotas2018,
author = {Zolotas, Mark and Elsdon, Joshua and Demiris, Yiannis},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zolotas, Elsdon, Demiris - 2018 - Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance.pdf:pdf},
title = {{Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance}},
year = {2018}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {https://github.com/},
volume = {39},
year = {2017}
}
@article{Microsoft2015,
author = {Microsoft},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microsoft - 2015 - Microsoft HoloLens HoloLens Device Specifications.pdf:pdf},
title = {{Microsoft HoloLens HoloLens Device Specifications}},
year = {2015}
}
@article{Taketomi2017,
abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
doi = {10.1186/s41074-017-0027-2},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taketomi, Uchiyama, Ikeda - 2017 - Visual SLAM algorithms a survey from 2010 to 2016.pdf:pdf},
isbn = {81-7319-221-9},
issn = {1882-6695},
journal = {IPSJ Transactions on Computer Vision and Applications},
keywords = {augmented reality,computer vision,robotics,survey,visual slam},
number = {1},
pages = {16},
publisher = {IPSJ Transactions on Computer Vision and Applications},
title = {{Visual SLAM algorithms: a survey from 2010 to 2016}},
url = {http://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2},
volume = {9},
year = {2017}
}
@inproceedings{Kocabas,
abstract = {In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, keypoint detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with 23 frames/sec. Source code is available at: https://github.com/mkocabas/pose-residual-network},
archivePrefix = {arXiv},
arxivId = {1807.04067v1},
author = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01252-6_26},
eprint = {1807.04067v1},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kocabas, Salih Karagoz, Akbas - Unknown - MultiPoseNet Fast Multi-Person Pose Estimation using Pose Residual Network.pdf:pdf},
isbn = {9783030012519},
issn = {16113349},
keywords = {Multi-person pose estimation,Multi-task learning,MultiPoseNet,Pose residual network,Semantic segmentation},
pages = {437--453},
title = {{MultiPoseNet: Fast Multi-Person Pose Estimation Using Pose Residual Network}},
url = {https://github.com/mkocabas/pose-residual-network},
volume = {11215 LNCS},
year = {2018}
}
@article{Patacchiola2017a,
abstract = {Head pose estimation is an old problem that is recently receiving new attention because of possible applications in human-robot interaction, augmented reality and driving assistance. However, most of the existing work has been tested in controlled environments and is not robust enough for real-world applications. In order to handle these limitations we propose an approach based on Convolutional Neural Networks (CNNs) supplemented with the most recent techniques adopted from the deep learning community. We evaluate the performance of four architectures on recently released in-the-wild datasets. Moreover, we investigate the use of dropout and adaptive gradient methods giving a contribution to their ongoing validation. The results show that joining CNNs and adaptive gradient methods leads to the state-of-the-art in unconstrained head pose estimation.},
author = {Patacchiola, Massimiliano and Cangelosi, Angelo},
doi = {10.1016/j.patcog.2017.06.009},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patacchiola, Cangelosi - 2017 - Head pose estimation in the wild using Convolutional Neural Networks and adaptive gradient methods.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Adaptive gradient,Convolutional Neural Networks,Deep learning,Head pose estimation},
pages = {132--143},
title = {{Head pose estimation in the wild using Convolutional Neural Networks and adaptive gradient methods}},
url = {http://dx.doi.org/10.1016/j.patcog.2017.06.009},
volume = {71},
year = {2017}
}
@article{Nister2004,
abstract = {DAVID AND HENRIK STEWENIUS Department of Computer Science, Center for Visualization and Virtual Environments, University of Kentucky, Lexington, KY},
author = {Nist{\'{e}}r, David},
doi = {10.1109/CVPR.2004.1315081},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nist{\'{e}}r - 2004 - A Minimal Solution to the Generalised 3-Point Pose Problem.pdf:pdf},
isbn = {0769521584},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
number = {1},
pages = {560--567},
title = {{A Minimal Solution to the Generalised 3-Point Pose Problem}},
volume = {27},
year = {2004}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385v1},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/},
volume = {2016-Decem},
year = {2016}
}
@techreport{Viola2001,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classi-fiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differenc-ing or skin color detection.},
author = {Viola, Paul and Jones, Michael},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2001 - Rapid Object Detection using a Boosted Cascade of Simple Features.pdf:pdf},
title = {{Rapid Object Detection using a Boosted Cascade of Simple Features}},
url = {https://www.cs.cmu.edu/{~}efros/courses/LBMV07/Papers/viola-cvpr-01.pdf},
year = {2001}
}
@inproceedings{Ruiz2018,
abstract = {Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.},
archivePrefix = {arXiv},
arxivId = {1710.00925v5},
author = {Ruiz, Nataniel and Chong, Eunji and Rehg, James M},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2018.00281},
eprint = {1710.00925v5},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruiz, Chong, Rehg - Unknown - Fine-Grained Head Pose Estimation Without Keypoints.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
pages = {2155--2164},
title = {{Fine-grained head pose estimation without keypoints}},
url = {https://github.com/natanielruiz/deep-head-pose},
volume = {2018-June},
year = {2018}
}
@article{Kalman1961,
abstract = {A nonlinear differential equation of the Riccati type is derived for the covariance matrix of the optimal filtering error. The solution of this “variance equation” completely specifies the optimal filter for either finite or infinite smoothing intervals and stationary or nonstationary statistics. The variance equation is closely related to the Hamiltonian (canonical) differential equations of the calculus of variations. Analytic solutions are available in some cases. The significance of the variance equation is illustrated by examples which duplicate, simplify, or extend earlier results in this field. The Duality Principle relating stochastic estimation and deterministic control problems plays an important role in the proof of theoretical results. In several examples, the estimation problem and its dual are discussed side-by-side. Properties of the variance equation are of great interest in the theory of adaptive systems. Some aspects of this are considered briefly.},
author = {Kalman, R. E. and Bucy, R. S.},
doi = {10.1115/1.3658902},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalman, Bucy - 1961 - A New Approach to Linear Filtering and Prediction Problems.pdf:pdf},
issn = {00219223},
journal = {Journal of Basic Engineering},
number = {1},
pages = {95},
title = {{New Results in Linear Filtering and Prediction Theory}},
url = {https://pdfs.semanticscholar.org/5c2f/635fd11d2d001b7f9921007c6d3cf201eebf.pdf http://link.aip.org/link/JBAEAI/v83/i1/p95/s1{\&}Agg=doi http://fluidsengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1430804},
volume = {83},
year = {1961}
}
