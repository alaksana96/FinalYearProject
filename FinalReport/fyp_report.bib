Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


@INPROCEEDINGS{Piccardi2004, 
author={M. {Piccardi}}, 
booktitle={2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)}, 
title={Background subtraction techniques: a review}, 
year={2004}, 
volume={4}, 
number={}, 
pages={3099-3104 vol.4}, 
keywords={object detection;feature extraction;image motion analysis;background subtraction technique;static cameras;object detection;spatial correlation;background density estimation;Subtraction techniques;Object detection;Cameras;Computer vision;Information technology;Australia;Videos;Layout;Geometry;Filters}, 
doi={10.1109/ICSMC.2004.1400815}, 
ISSN={1062-922X}, 
month={Oct},}
@article{Cao2017,
  author    = {Zhe Cao and
               Gines Hidalgo and
               Tomas Simon and
               Shih{-}En Wei and
               Yaser Sheikh},
  title     = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity
               Fields},
  journal   = {CoRR},
  volume    = {abs/1812.08008},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.08008},
  archivePrefix = {arXiv},
  eprint    = {1812.08008},
  timestamp = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-08008},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Bewley2016,
abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
archivePrefix = {arXiv},
arxivId = {1602.00763v2},
author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2016.7533003},
eprint = {1602.00763v2},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bewley et al. - 2017 - SIMPLE ONLINE AND REALTIME TRACKING.pdf:pdf},
isbn = {9781467399616},
issn = {15224880},
keywords = {Computer Vision,Data Association,Detection,Multiple Object Tracking},
pages = {3464--3468},
title = {{Simple online and realtime tracking}},
url = {https://github.com/abewley/sort},
volume = {2016-Augus},
year = {2016}
}
@article{Pishchulin,
  author    = {Leonid Pishchulin and
               Eldar Insafutdinov and
               Siyu Tang and
               Bjoern Andres and
               Mykhaylo Andriluka and
               Peter V. Gehler and
               Bernt Schiele},
  title     = {DeepCut: Joint Subset Partition and Labeling for Multi Person Pose
               Estimation},
  journal   = {CoRR},
  volume    = {abs/1511.06645},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06645},
  archivePrefix = {arXiv},
  eprint    = {1511.06645},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PishchulinITAAG15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Insafutdinov,
  author    = {Eldar Insafutdinov and
               Leonid Pishchulin and
               Bjoern Andres and
               Mykhaylo Andriluka and
               Bernt Schiele},
  title     = {DeeperCut: {A} Deeper, Stronger, and Faster Multi-Person Pose Estimation
               Model},
  journal   = {CoRR},
  volume    = {abs/1605.03170},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.03170},
  archivePrefix = {arXiv},
  eprint    = {1605.03170},
  timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/InsafutdinovPAA16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{Dicle2013, 
author={C. {Dicle} and O. I. {Camps} and M. {Sznaier}}, 
booktitle={2013 IEEE International Conference on Computer Vision}, 
title={The Way They Move: Tracking Multiple Targets with Similar Appearance}, 
year={2013}, 
volume={}, 
number={}, 
pages={2304-2311}, 
keywords={computational complexity;object detection;target tracking;nonstationary cameras;complex target motions;target misidentification minimization;missing data recovery;generalized linear assignment;computational efficiency;multiple target tracking;Target tracking;Trajectory;Heuristic algorithms;Dynamics;IP networks;Complexity theory;Multitarget tracking;tracking;motion dynamics;Hankel;Rank Estimation;Generalized Linear Assignment}, 
doi={10.1109/ICCV.2013.286}, 
ISSN={1550-5499}, 
month={Dec},}
@article{Ren,
  author    = {Shaoqing Ren and
               Kaiming He and
               Ross B. Girshick and
               Jian Sun},
  title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
               Networks},
  journal   = {CoRR},
  volume    = {abs/1506.01497},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01497},
  archivePrefix = {arXiv},
  eprint    = {1506.01497},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RenHG015},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Redmon2018,
  author    = {Joseph Redmon and
               Ali Farhadi},
  title     = {YOLOv3: An Incremental Improvement},
  journal   = {CoRR},
  volume    = {abs/1804.02767},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.02767},
  archivePrefix = {arXiv},
  eprint    = {1804.02767},
  timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-02767},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{Girshick2014, 
author={R. {Girshick} and J. {Donahue} and T. {Darrell} and J. {Malik}}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, 
year={2014}, 
volume={}, 
number={}, 
pages={580-587}, 
keywords={image segmentation;neural nets;object detection;rich feature hierarchy;semantic segmentation;object detection performance;canonical PASCAL VOC dataset;low-level image feature;detection algorithm;mean average precision;mAP;high-capacity convolutional neural network;bottom-up region proposal;segment objects;labeled training data;supervised pretraining;auxiliary task;domain-specific fine-tuning;performance boost;R-CNN;image features;source code;Proposals;Feature extraction;Training;Visualization;Object detection;Vectors;Support vector machines}, 
doi={10.1109/CVPR.2014.81}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{Bailey2006a, 
author={H. {Durrant-Whyte} and T. {Bailey}}, 
journal={IEEE Robotics Automation Magazine}, 
title={Simultaneous localization and mapping: part I}, 
year={2006}, 
volume={13}, 
number={2}, 
pages={99-110}, 
keywords={mobile robots;path planning;simultaneous localization and mapping problem;mobile robots;SLAM problem;Simultaneous localization and mapping;Mobile robots;Robotics and automation;History;Artificial intelligence;Navigation;Vehicles;Buildings;Bayesian methods;Particle filters}, 
doi={10.1109/MRA.2006.1638022}, 
ISSN={1070-9932}, 
month={June},}
@unpublished{Leutenegger2019,
author = {Leutenegger, Stefan},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leutenegger - 2019 - Representations and Sensors.pdf:pdf},
institution = {Imperial College London},
isbn = {9780470517062},
title = {{Representations and Sensors}},
url = {https://www.imperial.ac.uk/computing/current-students/courses/433/},
year = {2019}
}
@ARTICLE{Cadena2016, 
author={C. {Cadena} and L. {Carlone} and H. {Carrillo} and Y. {Latif} and D. {Scaramuzza} and J. {Neira} and I. {Reid} and J. J. {Leonard}}, 
journal={IEEE Transactions on Robotics}, 
title={Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age}, 
year={2016}, 
volume={32}, 
number={6}, 
pages={1309-1332}, 
keywords={SLAM (robots);simultaneous-localization-and-mapping;SLAM community;long-term mapping;semantic representations;SLAM users;critical eye;Graph theory;Simultaneous location and mapping;Service robots;Robustness;Localization;Factor graphs;localization;mapping;maximum a posteriori estimation;perception;robots;sensing;simultaneous localization and mapping (SLAM)}, 
doi={10.1109/TRO.2016.2624754}, 
ISSN={1552-3098}, 
month={Dec},}
@article{Jin2017,
  author    = {Guanghan Ning and
               Ping Liu and
               Xiaochuan Fan and
               Chi Zhang},
  title     = {A Top-down Approach to Articulated Human Pose Estimation and Tracking},
  journal   = {CoRR},
  volume    = {abs/1901.07680},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.07680},
  archivePrefix = {arXiv},
  eprint    = {1901.07680},
  timestamp = {Mon, 18 Feb 2019 14:51:21 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-07680},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Redmon,
  author    = {Joseph Redmon and
               Santosh Kumar Divvala and
               Ross B. Girshick and
               Ali Farhadi},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  journal   = {CoRR},
  volume    = {abs/1506.02640},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02640},
  archivePrefix = {arXiv},
  eprint    = {1506.02640},
  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RedmonDGF15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{Hirabayashi, 
author={M. {Hirabayashi} and S. {Kato} and M. {Edahiro} and K. {Takeda} and T. {Kawano} and S. {Mita}}, 
booktitle={2013 IEEE 1st International Conference on Cyber-Physical Systems, Networks, and Applications (CPSNA)}, 
title={GPU implementations of object detection using HOG features and deformable models}, 
year={2013}, 
volume={}, 
number={}, 
pages={106-111}, 
keywords={automobiles;cameras;feature extraction;graphics processing units;image sensors;multi-threading;object detection;vision-based object detection speed;camera sensors;autonomous vehicles;histograms of oriented gradients;HOG features;deformable models;car detection;computational cost;real-time constraint;graphic processing units;scoring similarity;input image models;predefined model;real-world vehicle detection program;commodity GPU implementation;frame rate;sequential implementation;multithreaded implementation;Graphics processing units;Object detection;Vehicles;Multicore processing;Programming;Instruction sets;Shape;GPGPU;Computer Vision;Object Detection}, 
doi={10.1109/CPSNA.2013.6614255}, 
ISSN={}, 
month={Aug},}
@ARTICLE{Murphy-Chutorian2009, 
author={E. {Murphy-Chutorian} and M. M. {Trivedi}}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Head Pose Estimation in Computer Vision: A Survey}, 
year={2009}, 
volume={31}, 
number={4}, 
pages={607-626}, 
keywords={computer vision;pose estimation;pose estimation;computer vision;human ability;face detection;image recognition;generic solution;Computer vision;Head;Humans;Cameras;Face detection;Focusing;Neck;Eyes;Face recognition;Evolution (biology);Introductory and Survey;Computer vision;Modeling and recovery of physical attributes;Human-centered computing;Vision I/O;Face and gesture recognition;Evaluation/methodology;Introductory and Survey;Computer vision;Modeling and recovery of physical attributes;Human-centered computing;Vision I/O;Face and gesture recognition;Evaluation/methodology;Artificial Intelligence;Head;Humans;Nonlinear Dynamics;Video Recording;Visual Perception}, 
doi={10.1109/TPAMI.2008.106}, 
ISSN={0162-8828}, 
month={April},
}
@article{Milan,
  author    = {Anton Milan and
               Laura Leal{-}Taix{\'{e}} and
               Ian D. Reid and
               Stefan Roth and
               Konrad Schindler},
  title     = {{MOT16:} {A} Benchmark for Multi-Object Tracking},
  journal   = {CoRR},
  volume    = {abs/1603.00831},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.00831},
  archivePrefix = {arXiv},
  eprint    = {1603.00831},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MilanL0RS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{darknet13,
author = {Redmon, Joseph},
howpublished = {$\backslash$url{\{}http://pjreddie.com/darknet/{\}}},
title = {{Darknet: Open Source Neural Networks in C}},
year = {2013}
}
@ARTICLE{Rena, 
author={S. {Ren} and K. {He} and R. {Girshick} and J. {Sun}}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
year={2017}, 
volume={39}, 
number={6}, 
pages={1137-1149}, 
keywords={graphics processing units;neural nets;object detection;faster-R-CNN;real-time object detection;region proposal networks;RPN;full-image convolutional features;high-quality region proposals;attention mechanisms;deep VGG-16 model;GPU;object detection accuracy;PASCAL VOC 2007;PASCAL VOC 2012;MS COCO datasets;COCO 2015 competitions;ILSVRC;Proposals;Object detection;Convolutional codes;Feature extraction;Search problems;Detectors;Training;Object detection;region proposal;convolutional neural network}, 
doi={10.1109/TPAMI.2016.2577031}, 
ISSN={0162-8828}, 
month={June},}
@article{Valenti2012,
abstract = {Head pose and eye location for gaze estimation have been separately studied in numerous works in the literature. Previous research shows that satisfactory accuracy in head pose and eye location estimation can be achieved in constrained settings. However, in the presence of nonfrontal faces, eye locators are not adequate to accurately locate the center of the eyes. On the other hand, head pose estimation techniques are able to deal with these conditions; hence, they may be suited to enhance the accuracy of eye localization. Therefore, in this paper, a hybrid scheme is proposed to combine head pose and eye location information to obtain enhanced gaze estimation. To this end, the transformation matrix obtained from the head pose is used to normalize the eye regions, and in turn, the transformation matrix generated by the found eye location is used to correct the pose estimation procedure. The scheme is designed to enhance the accuracy of eye location estimations, particularly in low-resolution videos, to extend the operative range of the eye locators, and to improve the accuracy of the head pose tracker. These enhanced estimations are then combined to obtain a novel visual gaze estimation system, which uses both eye location and head information to refine the gaze estimates. From the experimental results, it can be derived that the proposed unified scheme improves the accuracy of eye estimations by 16{\%} to 23{\%}. Furthermore, it considerably extends its operating range by more than 15° by overcoming the problems introduced by extreme head poses. Moreover, the accuracy of the head pose tracker is improved by 12{\%} to 24{\%}. Finally, the experimentation on the proposed combined gaze estimation system shows that it is accurate (with a mean error between 2° and 5°) and that it can be used in cases where classic approaches would fail without imposing restraints on the position of the head.},
author = {Valenti, Roberto and Sebe, Nicu and Gevers, Theo},
doi = {10.1109/TIP.2011.2162740},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Valenti, Sebe, Gevers - 2012 - Combining Head Pose and Eye Location Information for Gaze Estimation.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Eye center location,gaze estimation,head pose estimation},
number = {2},
pages = {802--815},
title = {{Combining head pose and eye location information for gaze estimation}},
url = {http://ieeexplore.ieee.org.},
volume = {21},
year = {2012}
}
@INPROCEEDINGS{Dalal2005, 
author={N. {Dalal} and B. {Triggs}}, 
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
title={Histograms of oriented gradients for human detection}, 
year={2005}, 
volume={1}, 
number={}, 
pages={886-893 vol. 1}, 
keywords={object detection;support vector machines;object recognition;feature extraction;gradient methods;histograms of oriented gradients;human detection;robust visual object recognition;linear SVM;edge based descriptors;gradient based descriptors;fine-scale gradients;fine orientation binning;coarse spatial binning;contrast normalization;overlapping descriptor;pedestrian database;Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases}, 
doi={10.1109/CVPR.2005.177}, 
ISSN={1063-6919}, 
month={June},}
@article{Shao,
  author    = {Shuai Shao and
               Zijian Zhao and
               Boxun Li and
               Tete Xiao and
               Gang Yu and
               Xiangyu Zhang and
               Jian Sun},
  title     = {CrowdHuman: {A} Benchmark for Detecting Human in a Crowd},
  journal   = {CoRR},
  volume    = {abs/1805.00123},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.00123},
  archivePrefix = {arXiv},
  eprint    = {1805.00123},
  timestamp = {Thu, 14 Mar 2019 14:56:07 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-00123},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{Hou2010, 
author={Y. {Hou} and G. K. H. {Pang}}, 
booktitle={2010 IEEE International Conference on Image Processing}, 
title={Human detection in crowded scenes}, 
year={2010}, 
volume={}, 
number={}, 
pages={721-724}, 
keywords={image motion analysis;image segmentation;object detection;video surveillance;human detection;crowded scenes;ISM;foreground region;image segmentation;ISM model;CAVIAR dataset;implicit shape model;Humans;Training;Shape;Detectors;Head;Video sequences;Image segmentation;Human detection;Occlusions;Implicit Shape Model}, 
doi={10.1109/ICIP.2010.5651982}, 
ISSN={2381-8549}, 
month={Sep.},}
@misc{Detectron2018,
author = {Girshick, Ross and Radosavovic, Ilija and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and He, Kaiming},
howpublished = {$\backslash$url{\{}https://github.com/facebookresearch/detectron{\}}},
title = {{Detectron}},
year = {2018}
}
@inproceedings{Papandreou2017,
abstract = {We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5{\%} absolute improvement compared to the previous best performing method on the same dataset.},
archivePrefix = {arXiv},
arxivId = {1701.01779},
author = {Papandreou, George and Zhu, Tyler and Kanazawa, Nori and Toshev, Alexander and Tompson, Jonathan and Bregler, Chris and Murphy, Kevin},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.395},
eprint = {1701.01779},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papandreou et al. - 2017 - Towards Accurate Multi-person Pose Estimation in the Wild.pdf:pdf},
isbn = {9781538604571},
month = {Jan},
pages = {3711--3719},
title = {{Towards accurate multi-person pose estimation in the wild}},
url = {http://arxiv.org/abs/1701.01779},
volume = {2017-January},
year = {2017}
}
@article{Kairy2014,
abstract = {Power wheelchairs (PWCs) can have a positive impact on user well-being, self-esteem, pain, activity and participation. Newly developed intelligent power wheelchairs (IPWs), allowing autonomous or collaboratively-controlled navigation, could enhance mobility of individuals not able to use, or having difficulty using, standard PWCs. The objective of this study was to explore the perspectives of PWC users (PWUs) and their caregivers regarding if and how IPWs could impact on current challenges faced by PWUs, as well as inform current development of IPWs. A qualitative exploratory study using individual interviews was conducted with PWUs (n = 12) and caregivers (n = 4). A semi-structured interview guide and video were used to facilitate informed discussion regarding IPWs. Thematic analysis revealed three main themes: (1) "challenging situations that may be overcome by an IPW" described how the IPW features of obstacle avoidance, path following, and target following could alleviate PWUs' identified mobility difficulties; (2) "cautious optimism concerning IPW use revealed participants" addresses concerns regarding using an IPW as well as technological suggestions; (3) "defining the potential IPW user" revealed characteristics of PWUs that would benefit from IPW use. Findings indicate  how IPW use may help overcome PWC difficulties and confirm the importance of user input in the ongoing development of IPWs. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Kairy, Dahlia and Rushton, Paula W. and Archambault, Philippe and Pituch, Evelina and Torkia, Caryne and {El Fathi}, Anas and Stone, Paula and Routhier, Fran{\c{c}}ois and Forget, Robert and Demers, Louise and Pineau, Joelle and Gourdeau, Richard},
doi = {10.3390/ijerph110202244},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kairy et al. - 2014 - Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair us.pdf:pdf},
isbn = {1660-4601},
issn = {16617827},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Disability,Intelligent power wheelchair,Mobility,Navigation,Obstacle-avoidance,Path following,Safety,User-centered design},
number = {2},
pages = {2244--2261},
pmid = {24566051},
title = {{Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair use: A qualitative study}},
volume = {11},
year = {2014}
}
@inproceedings{Kazemi2014,
abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with ef- ficient feature selection. Different regularization strategies and its importance to combat overfitting are also investi- gated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
author = {Kazemi, Vahid and Sullivan, Josephine},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.241},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kazemi, Kth - Unknown - One Millisecond Face Alignment with an Ensemble of Regression Trees.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
keywords = {Decision Trees,Face Alignment,Gradient Boosting,Real-Time},
pages = {1867--1874},
title = {{One Millisecond Face Alignment with an Ensemble of Regression Trees}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2014/papers/Kazemi{\_}One{\_}Millisecond{\_}Face{\_}2014{\_}CVPR{\_}paper.pdf},
year = {2014}
}
@article{Milgram1994,
author = {Milgram, Paul and Kishino, Fumio},
year = {1994},
month = {12},
pages = {1321-1329},
title = {A Taxonomy of Mixed Reality Visual Displays},
volume = {vol. E77-D, no. 12},
journal = {IEICE Trans. Information Systems}
}
@misc{Microsofta,
author = {Microsoft},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microsoft - 2018 - Windows Mixed Reality Documentation.pdf:pdf},
title = {{Windows Mixed Reality Documentation}},
year = {2018}
}
@article{Lin,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{\'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  archivePrefix = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zeng2017,
  title={Robust Background Subtraction via the Local Similarity Statistical Descriptor},
  author={Dongdong Zeng and Ming Zhu and Tongxue Zhou and Fang Xu and Hang Yang},
  year={2017}
}
@INPROCEEDINGS{Wojke2018, 
author={N. {Wojke} and A. {Bewley} and D. {Paulus}}, 
booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, 
title={Simple online and realtime tracking with a deep association metric}, 
year={2017}, 
volume={}, 
number={}, 
pages={3645-3649}, 
keywords={computational complexity;learning (artificial intelligence);object detection;object tracking;query processing;target tracking;simple online tracking;computational complexity;realtime tracking;measurement-to-track associations;online application;deep association metric;identity switches;SORT;simple algorithms;multiple object tracking;Kalman filters;Tracking;Extraterrestrial measurements;Standards;Uncertainty;Cameras;Computer Vision;Multiple Object Tracking;Data Association}, 
doi={10.1109/ICIP.2017.8296962}, 
ISSN={2381-8549}, 
month={Sep.},
}
@article{Jia,
  author    = {Yangqing Jia and
               Evan Shelhamer and
               Jeff Donahue and
               Sergey Karayev and
               Jonathan Long and
               Ross B. Girshick and
               Sergio Guadarrama and
               Trevor Darrell},
  title     = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  journal   = {CoRR},
  volume    = {abs/1408.5093},
  year      = {2014},
  url       = {http://arxiv.org/abs/1408.5093},
  archivePrefix = {arXiv},
  eprint    = {1408.5093},
  timestamp = {Mon, 13 Aug 2018 16:46:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JiaSDKLGGD14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Chacon-Quesada,
abstract = {In this paper we present a novel augmented reality head mounted display user interface for controlling a robotic wheelchair for people with limited mobility. To lower the cognitive requirements needed to control the wheelchair, we propose integration of a smart wheelchair with an eye-tracking enabled head-mounted display. We propose a novel platform that integrates multiple user interface interaction methods for aiming at and selecting affordances derived by on-board perception capabilities such as laser-scanner readings and cameras. We demonstrate the effectiveness of the approach by evaluating our platform in two realistic scenarios: 1) Door detection, where the affordance corresponds to a Door object and the Go-Through action and 2) People detection, where the affordance corresponds to a Person and the Approach action. To the best of our knowledge, this is the first demonstration of a augmented reality head-mounted display user interface for controlling a smart wheelchair.},
author = {Chac{\'{o}}n-Quesada, Rodrigo and Demiris, Yiannis},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chac{\'{o}}n-Quesada, Demiris - 2018 - Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances.pdf:pdf},
keywords = {Index terms-Affordances,augmented reality,head mounted display,smart wheelchairs,user interface},
pages = {1--4},
title = {{Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances}},
url = {http://www.imperial.ac.uk/personal-robotics/robots/},
year = {2018}
}
@INPROCEEDINGS{Zolotas2018, 
author={M. {Zolotas} and J. {Elsdon} and Y. {Demiris}}, 
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance}, 
year={2018}, 
volume={}, 
number={}, 
pages={1823-1829}, 
keywords={augmented reality;handicapped aids;human-robot interaction;medical robotics;mobile robots;wheelchairs;robotic wheelchair assistance;visual feedback;wheelchair navigation;head-mounted aid;Microsoft Hololens;mental model;severely disabled individuals;robotic wheelchairs;head-mounted augmented reality;augmented information acquisition;assistive navigation;immersive wheelchair training regime;learning curve;Wheelchairs;Mobile robots;Navigation;Visualization;Collision avoidance;Trajectory}, 
doi={10.1109/IROS.2018.8594002}, 
ISSN={2153-0866}, 
month={Oct},}


@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {https://github.com/},
volume = {39},
year = {2017}
}
@article{Microsoft2015,
author = {Microsoft},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microsoft - 2015 - Microsoft HoloLens HoloLens Device Specifications.pdf:pdf},
title = {{Microsoft HoloLens HoloLens Device Specifications}},
year = {2015}
}
@Article{Taketomi2017,
author="Taketomi, Takafumi
and Uchiyama, Hideaki
and Ikeda, Sei",
title="Visual SLAM algorithms: a survey from 2010 to 2016",
journal="IPSJ Transactions on Computer Vision and Applications",
year="2017",
month="Jun",
day="02",
volume="9",
number="1",
pages="16",
abstract="SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.",
issn="1882-6695",
doi="10.1186/s41074-017-0027-2",
url="https://doi.org/10.1186/s41074-017-0027-2"
}

@inproceedings{Kocabas,
abstract = {In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, keypoint detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with 23 frames/sec. Source code is available at: https://github.com/mkocabas/pose-residual-network},
archivePrefix = {arXiv},
arxivId = {1807.04067v1},
author = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01252-6_26},
eprint = {1807.04067v1},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kocabas, Salih Karagoz, Akbas - Unknown - MultiPoseNet Fast Multi-Person Pose Estimation using Pose Residual Network.pdf:pdf},
isbn = {9783030012519},
issn = {16113349},
keywords = {Multi-person pose estimation,Multi-task learning,MultiPoseNet,Pose residual network,Semantic segmentation},
pages = {437--453},
title = {{MultiPoseNet: Fast Multi-Person Pose Estimation Using Pose Residual Network}},
url = {https://github.com/mkocabas/pose-residual-network},
volume = {11215 LNCS},
year = {2018}
}
@article{Patacchiola2017a,
author = {Patacchiola, Massimiliano and Cangelosi, Angelo},
year = {2017},
month = {06},
pages = {},
title = {Head Pose Estimation in the Wild using Convolutional Neural Networks and Adaptive Gradient Methods},
volume = {71},
journal = {Pattern Recognition},
doi = {10.1016/j.patcog.2017.06.009}
}
@INPROCEEDINGS{Nister2004, 
author={D. {Nister}}, 
booktitle={Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.}, 
title={A minimal solution to the generalised 3-point pose problem}, 
year={2004}, 
volume={1}, 
number={}, 
pages={I-I}, 
keywords={image processing;cameras;computational geometry;algebra;calibration;minimal solution;three point pose problem;image projection;central perspective projection;three back projected rays;numerical algorithm;octic polynomial;reflection symmetry;computational geometry;algebra;calibration;ruled quartic surface;multicamera rig;generalised 3-point pose problem;Cameras;Polynomials;Reflection;Computer vision;Geometry;Collaboration;Government;Computer architecture;Solid modeling;Mirrors}, 
doi={10.1109/CVPR.2004.1315081}, 
ISSN={1063-6919}, 
month={June},}
@article{He2016,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{Viola2001, 
author={P. {Viola} and M. {Jones}}, 
booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}, 
title={Rapid object detection using a boosted cascade of simple features}, 
year={2001}, 
volume={1}, 
number={}, 
pages={I-I}, 
keywords={object detection;image classification;image representation;learning (artificial intelligence);feature extraction;rapid object detection;boosted simple feature cascade;machine learning;visual object detection;image processing;image representation;integral image;AdaBoost;classifiers;background regions;object specific focus-of-attention mechanism;statistical guarantees;face detection;real-time applications;Object detection;Face detection;Pixel;Detectors;Filters;Machine learning;Image representation;Focusing;Skin;Robustness}, 
doi={10.1109/CVPR.2001.990517}, 
ISSN={1063-6919}, 
month={Dec},}
@inproceedings{Ruiz2018,
abstract = {Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.},
archivePrefix = {arXiv},
arxivId = {1710.00925v5},
author = {Ruiz, Nataniel and Chong, Eunji and Rehg, James M},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2018.00281},
eprint = {1710.00925v5},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruiz, Chong, Rehg - Unknown - Fine-Grained Head Pose Estimation Without Keypoints.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
pages = {2155--2164},
title = {{Fine-grained head pose estimation without keypoints}},
url = {https://github.com/natanielruiz/deep-head-pose},
volume = {2018-June},
year = {2018}
}
@article{Kalman1961,
abstract = {A nonlinear differential equation of the Riccati type is derived for the covariance matrix of the optimal filtering error. The solution of this “variance equation” completely specifies the optimal filter for either finite or infinite smoothing intervals and stationary or nonstationary statistics. The variance equation is closely related to the Hamiltonian (canonical) differential equations of the calculus of variations. Analytic solutions are available in some cases. The significance of the variance equation is illustrated by examples which duplicate, simplify, or extend earlier results in this field. The Duality Principle relating stochastic estimation and deterministic control problems plays an important role in the proof of theoretical results. In several examples, the estimation problem and its dual are discussed side-by-side. Properties of the variance equation are of great interest in the theory of adaptive systems. Some aspects of this are considered briefly.},
author = {Kalman, R. E. and Bucy, R. S.},
doi = {10.1115/1.3658902},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalman, Bucy - 1961 - A New Approach to Linear Filtering and Prediction Problems.pdf:pdf},
issn = {00219223},
journal = {Journal of Basic Engineering},
number = {1},
pages = {95},
title = {{New Results in Linear Filtering and Prediction Theory}},
url = {https://pdfs.semanticscholar.org/5c2f/635fd11d2d001b7f9921007c6d3cf201eebf.pdf http://link.aip.org/link/JBAEAI/v83/i1/p95/s1{\&}Agg=doi http://fluidsengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1430804},
volume = {83},
year = {1961}
}
