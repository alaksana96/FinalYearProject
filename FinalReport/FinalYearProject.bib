Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Liu2014,
abstract = {{\textcopyright} 2014 The Author(s). Licensee InTech.Pose estimation methods in robotics applications frequently suffer from inaccuracy due to a lack of correspondence and real-time constraints, and instability from a wide range of viewpoints, etc. In this paper, we present a novel approach for estimating the poses of all the cameras in a multi-camera system in which each camera is placed rigidly using only a few coplanar points simultaneously. Instead of solving the orientation and translation for the multi-camera system from the overlapping point correspondences among all the cameras directly, we employ homography, which can map image points with 3D coplanar-referenced points. In our method, we first establish the corresponding relations between each camera by their Euclidean geometries and optimize the homographies of the cameras; then, we solve the orientation and translation for the optimal homographies. The results from simulations and real case experiments show that our approach is accurate and robust for implementation in robotics applications. Finally, a practical implementation in a ping-pong robot is described in order to confirm the validity of our approach.},
author = {Liu, Yong and Xiong, Rong and Li, Yi},
doi = {10.5772/58868},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Xiong, Li - 2014 - Robust and accurate multiple-camera pose estimation toward robotic applications.pdf:pdf},
issn = {17298814},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Coplanar points,Multi-camera system,Ping-pong robot,Pose estimation},
title = {{Robust and accurate multiple-camera pose estimation toward robotic applications}},
volume = {11},
year = {2014}
}
@article{Grigorescu2013,
abstract = {In this paper, a new object tracking system is proposed to improve the object manipulation capabilities of service robots. The goal is to continuously track the state of the visualized environment in order to send visual information in real time to the path planning and decision modules of the robot; that is, to adapt the movement of the robotic system according to the state variations appearing in the imaged scene. The tracking approach is based on a probabilistic collaborative tracking framework developed around a 2D patch-based tracking system and a 2D-3D point features tracker. The real-time visual information is composed of RGB-D data streams acquired from state-of-the-art structured light sensors. For performance evaluation, the accuracy of the developed tracker is compared to a traditional marker-based tracking system which delivers 3D information with respect to the position of the marker.},
author = {Grigorescu, Sorin M. and Pozna, Claudiu},
doi = {10.5772/55952},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grigorescu, Pozna - 2013 - Towards a stable robotic object manipulation through 2D-3D features tracking Regular paper.pdf:pdf},
isbn = {1729-8806},
issn = {17298806},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Mobile manipulation,Object tracking,Robot vision,Service robotics},
pages = {1--8},
title = {{Towards a stable robotic object manipulation through 2D-3D features tracking: Regular paper}},
volume = {10},
year = {2013}
}
@article{Dalal2004,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, Navneet and Triggs, William},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal, Triggs - 2004 - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {1063-6919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR05},
number = {3},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of Oriented Gradients for Human Detection}},
volume = {1},
year = {2004}
}
@article{Martinez-Martin2017,
abstract = {Technological advances are being made to assist humans in performing ordinary tasks in everyday settings. A key issue is the interaction with objects of varying size, shape, and degree of mobility. Autonomous assistive robots must be provided with the ability to process visual data in real time so that they can react adequately for quickly adapting to changes in the environment. Reliable object detection and recognition is usually a necessary early step to achieve this goal. In spite of significant research achievements, this issue still remains a challenge when real-life scenarios are considered. In this article, we present a vision system for assistive robots that is able to detect and recognize objects from a visual input in ordinary environments in real time. The system computes color, motion, and shape cues, combining them in a probabilistic manner to accurately achieve object detection and recognition, taking some inspiration from vision science. In addition, with the purpose of processing the input visual data in real time, a graphical processing unit (GPU) has been employed. The presented approach has been implemented and evaluated on a humanoid robot torso located at realistic scenarios. For further experimental validation, a public image repository for object recognition has been used, allowing a quantitative comparison with respect to other state-of-the-art techniques when realworld scenes are considered. Finally, a temporal analysis of the performance is provided with respect to image resolution and the number of target objects in the scene. {\textcopyright} 1994-2011 IEEE.},
author = {Martinez-Martin, Ester and {Del Pobil}, Angel P.},
doi = {10.1109/MRA.2016.2615329},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martinez-Martin, Del Pobil - 2017 - Object detection and recognition for assistive robots Experimentation and implementation.pdf:pdf},
isbn = {9780470976371},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {123--138},
title = {{Object detection and recognition for assistive robots: Experimentation and implementation}},
volume = {24},
year = {2017}
}
@article{Haddad2019,
abstract = {Pedestrian trajectory prediction is essential for collision avoidance in autonomous driving and robot navigation. However, predicting a pedestrian's trajectory in crowded environments is non-trivial as it is influenced by other pedestrians' motion and static structures that are present in the scene. Such human-human and human-space interactions lead to non-linearities in the trajectories. In this paper, we present a new spatio-temporal graph based Long Short-Term Memory (LSTM) network for predicting pedestrian trajectory in crowded environments, which takes into account the interaction with static (physical objects) and dynamic (other pedestrians) elements in the scene. Our results are based on two widely-used datasets to demonstrate that the proposed method outperforms the state-of-the-art approaches in human trajectory prediction. In particular, our method leads to a reduction in Average Displacement Error (ADE) and Final Displacement Error (FDE) of up to 55{\%} and 61{\%} respectively over state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {1902.05437},
author = {Haddad, Sirin and Wu, Meiqing and Wei, He and Lam, Siew Kei},
doi = {10.3217/978-3-85125-652-9},
eprint = {1902.05437},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haddad et al. - 2019 - Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model.pdf:pdf},
title = {{Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model}},
url = {https://arxiv.org/pdf/1902.05437.pdf http://arxiv.org/abs/1902.05437{\%}0Ahttp://dx.doi.org/10.3217/978-3-85125-652-9},
year = {2019}
}
@article{Zolotas2018,
author = {Zolotas, Mark and Elsdon, Joshua and Demiris, Yiannis},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zolotas, Elsdon, Demiris - 2018 - Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance.pdf:pdf},
title = {{Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance}},
year = {2018}
}
@article{Emharraf2015,
abstract = {this paper presents an approach for unknown indoor environment exploration using a simultaneous localization and mapping system. The approach addresses the problem of unknown indoor environments exploration, based on robot mobile moving and sonar scanning. The measurements given by the localization system (odometry for the test system), update for the robot self-localization. The map building process maintaining two grid maps: (1) map grid models the environment occupancy (OM), (2) map grid memorize the robot trajectory(TM). The use of two grid maps provides an efficacy description and use of the environment information over time. Results in simulation and real robots experiments using random exploration show the fusibility of our approach.},
author = {Emharraf, Mohamed and Rahmoun, Mohammed and Saber, Mohammed and Azizi, Mostafa},
doi = {10.1109/EITech.2015.7162945},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Emharraf et al. - 2015 - Mobile robot Simultaneous localization and mapping of unknown indoor environment.pdf:pdf},
isbn = {9781479974795},
journal = {Proceedings of 2015 International Conference on Electrical and Information Technologies, ICEIT 2015},
keywords = {autonomous robot mobile,map building,self-localization,unknown indoor environment},
pages = {1--6},
publisher = {IEEE},
title = {{Mobile robot: Simultaneous localization and mapping of unknown indoor environment}},
year = {2015}
}
@article{Chacon-Quesada,
abstract = {In this paper we present a novel augmented reality head mounted display user interface for controlling a robotic wheelchair for people with limited mobility. To lower the cognitive requirements needed to control the wheelchair, we propose integration of a smart wheelchair with an eye-tracking enabled head-mounted display. We propose a novel platform that integrates multiple user interface interaction methods for aiming at and selecting affordances derived by on-board perception capabilities such as laser-scanner readings and cameras. We demonstrate the effectiveness of the approach by evaluating our platform in two realistic scenarios: 1) Door detection, where the affordance corresponds to a Door object and the Go-Through action and 2) People detection, where the affordance corresponds to a Person and the Approach action. To the best of our knowledge, this is the first demonstration of a augmented reality head-mounted display user interface for controlling a smart wheelchair.},
author = {Chac{\'{o}}n-Quesada, Rodrigo and Demiris, Yiannis},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chac{\'{o}}n-Quesada, Demiris - 2018 - Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances.pdf:pdf},
keywords = {Index terms-Affordances,augmented reality,head mounted display,smart wheelchairs,user interface},
pages = {1--4},
title = {{Augmented Reality Control of Smart Wheelchair Using Eye-Gaze-Enabled Selection of Affordances}},
url = {http://www.imperial.ac.uk/personal-robotics/robots/},
year = {2018}
}
@article{Hasanuzzaman2006,
abstract = {Achieving natural interactions by means of vision and speech between humans and robots is one of the major goals that many researchers are working on. This paper aims to describe a gesture-based human-robot interaction (HRI) system using a knowledge-based software platform. A frame-based knowledge model is defined for the gesture interpretation and HRI. In this knowledge model, necessary frames are defined for the known users, robots, poses, gestures and robot behaviors. First, the system identifies the user using the eigenface method. Then, face and hand poses are segmented from the camera frame buffer using the person's specific skin color information and classified by the subspace method. The system is capable of recognizing static gestures comprised of the face and hand poses, and dynamic gestures of face in motion. The system combines computer vision and knowledge-based approaches in order to improve the adaptability to different people.},
author = {Hasanuzzaman, Md and Zhang, T. and Ampornaramveth, V. and Ueno, H.},
doi = {10.1108/01439910610638216},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasanuzzaman et al. - 2006 - Gesture-based human-robot interaction using a knowledge-based software platform.pdf:pdf},
issn = {0143991X},
journal = {Industrial Robot},
keywords = {Man machine interface,Robotics},
number = {1 SPEC. ISS.},
pages = {37--49},
title = {{Gesture-based human-robot interaction using a knowledge-based software platform}},
volume = {33},
year = {2006}
}
@article{Versini2017,
author = {Versini, Alessandro},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Versini - 2017 - Augmented Reality for Driving Applications.pdf:pdf},
journal = {Electronic Engineering},
pages = {1--67},
title = {{Augmented Reality for Driving Applications}},
year = {2017}
}
@article{Bailey2006,
abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity; data association; and environment representation},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Bailey, Tim and Durrant-Whyte, Hugh},
doi = {10.1109/MRA.2006.1678144},
eprint = {there is not},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey, Durrant-Whyte - 2006 - Simultaneous localization and mapping (SLAM) Part II.pdf:pdf},
isbn = {1610-7438},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {3},
pages = {108--117},
pmid = {1638022},
publisher = {IEEE},
title = {{Simultaneous localization and mapping (SLAM): Part II}},
volume = {13},
year = {2006}
}
@article{Cadena2016,
abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
archivePrefix = {arXiv},
arxivId = {1606.05830},
author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
doi = {10.1109/TRO.2016.2624754},
eprint = {1606.05830},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cadena et al. - 2016 - Past, present, and future of simultaneous localization and mapping Toward the robust-perception age.pdf:pdf},
isbn = {9781479936847},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Factor graphs,localization,mapping,maximum a posteriori estimation,perception,robots,sensing,simultaneous localization and mapping (SLAM)},
number = {6},
pages = {1309--1332},
pmid = {6576973927449638915},
title = {{Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age}},
volume = {32},
year = {2016}
}
@misc{Microsofta,
author = {Microsoft},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microsoft - 2018 - Windows Mixed Reality Documentation.pdf:pdf},
title = {{Windows Mixed Reality Documentation}},
year = {2018}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that canbe usedtoperformreliablematchingbetweendifferent views of an object or scene. The features are invariant to image scale and rotation, and are shown toprovide robust matching across a a substantial range ofaffine dis- tortion, change in 3Dviewpoint, addition ofnoise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be cor- rectly matched with high probability against a large database offeatures from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual fea- tures to a database offeatures fromknown objects using a fast nearest-neighbor algorithm, followedbyaHoughtransformtoidentifyclustersbelonging toasin- gle object, andfinallyperforming verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects amongclutter andocclusionwhileachievingnear real-timeperformance},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G},
doi = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
eprint = {0112017},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
isbn = {1568811012},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {SIFT},
pages = {1--28},
pmid = {20064111},
primaryClass = {cs},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
year = {2004}
}
@article{Kanaujia,
abstract = {Recent studies based upon the output of the ECMWF operational forecasting model indicates that if, after the first day of a forecast, a perfect model could be substituted for the present model, forecasts as good as these presently produced at seven days would be realized at ten days. These studies do not reveal how much improvement in one-day forecasting is possible. The author hypothesises that if all other imperfections in the forecasting procedure could be removed, the inevitable initial uncertainties in observing the small-scale features would, after D days, lead to error fields with amplitudes and spectra resembling those of the errors in present one-day forecasts. The appropriate value of D is highly dependent upon the spectrum of actual atmospheric motions. Estimates with a crude model place D at about four days, thereby implying that the present forecasting success at one week may some day be realized at nearly two weeks (6 Refs.)},
author = {Kanaujia, Atul and Wang, Ping and Haering, Niels},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanaujia, Wang, Haering - Unknown - Markov Logic Networks for Scene Interpretation and Complex Event Recognition in Videos.pdf:pdf},
isbn = {0094-243X},
title = {{Markov Logic Networks for Scene Interpretation and Complex Event Recognition in Videos}}
}
@article{Biswas2012,
abstract = {The sheer volume of data generated by depth cameras provides a challenge to process in real time, in particular when used for indoor mobile robot localization and navigation. We introduce the Fast Sampling Plane Filtering (FSPF) algorithm to reduce the volume of the 3D point cloud by sampling points from the depth image, and classifying local grouped sets of points as belonging to planes in 3D (the “plane filtered” points) or points that do not correspond to planes within a specified error margin (the “outlier” points). We then introduce a localization algorithm based on an observation model that down-projects the plane filtered points on to 2D, and assigns correspondences for each point to lines in the 2D map. The full sampled point cloud (consisting of both plane filtered as well as outlier points) is processed for obstacle avoidance for autonomous navigation. All our algorithms process only the depth information, and do not require additional RGB data. The FSPF, localization and obstacle avoidance algorithms run in real time at full camera frame rates (30Hz) with low CPU requirements (16$\backslash${\%}). We provide experimental results demonstrating the effectiveness of our approach for indoor mobile robot localization and navigation. We further compare the accuracy and robustness in localization using depth cam- eras with FSPF vs. alternative approaches that simulate laser rangefinder scans from the 3D data.},
author = {Biswas, Joydeep and Veloso, Manuela},
doi = {10.1109/ICRA.2012.6224766},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Biswas, Veloso - 2012 - Depth camera based indoor mobile robot localization and navigation.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1697--1702},
pmid = {7084453},
publisher = {IEEE},
title = {{Depth camera based indoor mobile robot localization and navigation}},
year = {2012}
}
@article{VanderMeulen2017,
abstract = {Anthracnose symptoms were observed on strawberries at several locations$\backslash$nnear Tucuman, Argentina [date not given]. The causal organism was$\backslash$nidentified as Colletotrichum acutatum. This is the first report of$\backslash$nC. acutatum causing strawberry anthracnose in northwestern Argentina.},
author = {van der Meulen, Hidde and Kun, Andrew L. and Shaer, Orit},
doi = {10.4038/cjsbs.v38i1.1326},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Meulen, Kun, Shaer - 2017 - What Are We Missing Adding Eye- Tracking to the HoloLens to Improve Gaze Estimation Accuracy.pdf:pdf},
isbn = {9781450346917},
issn = {0069-2379},
journal = {Proceedings of the Interactive Surfaces and Spaces - ISS '17},
keywords = {Augmented Reality,Eye-tracking},
pages = {396--400},
title = {{What Are We Missing? Adding Eye- Tracking to the HoloLens to Improve Gaze Estimation Accuracy}},
url = {http://dl.acm.org/citation.cfm?doid=3132272.3132278},
year = {2017}
}
@article{H.Montenegro-Couto2018,
author = {Montenegro-Couto, E. H. and {A. Hernandez-Ossa}, K. and {L. C. Bissoli}, A. and Sime, M. and {F. Bastos-Filho}, T.},
doi = {10.29327/cobecseb.78867},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montenegro-Couto et al. - 2018 - Towards an Assistive Interface To Command Robotic Wheelchairs and Interact With Environment Through Eye.pdf:pdf},
isbn = {978-85-5722-065-2},
journal = {Anais do V Congresso Brasileiro de Eletromiografia e Cinesiologia e X Simp{\'{o}}sio de Engenharia Biom{\'{e}}dica},
number = {January},
title = {{Towards an Assistive Interface To Command Robotic Wheelchairs and Interact With Environment Through Eye Gaze}},
url = {https://www.even3.com.br/anais/cobecseb/78867},
year = {2018}
}
@unpublished{Leutenegger2019,
author = {Leutenegger, Stefan},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leutenegger - 2019 - Representations and Sensors.pdf:pdf},
institution = {Imperial College London},
isbn = {9780470517062},
title = {{Representations and Sensors}},
year = {2019}
}
@article{Baldini2017,
author = {Baldini, Filippo},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldini - 2017 - Machine Learning for Humanoid Robot Programming through Virtual Reality Headsets.pdf:pdf},
journal = {Electronic Engineering},
pages = {1--67},
title = {{Machine Learning for Humanoid Robot Programming through Virtual Reality Headsets}},
year = {2017}
}
@misc{Zeller,
author = {Zeller, Matt},
title = {{HoloLens hardware details - Mixed Reality | Microsoft Docs}},
url = {https://docs.microsoft.com/en-us/windows/mixed-reality/hololens-hardware-details},
urldate = {2019-01-27},
year = {2018}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@misc{Microsoft,
author = {Microsoft},
title = {{Gaze - Mixed Reality | Microsoft Docs}},
url = {https://docs.microsoft.com/en-us/windows/mixed-reality/gaze},
urldate = {2019-01-27},
year = {2018}
}
@article{Microsoft2015,
author = {Microsoft},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Microsoft - 2015 - Microsoft HoloLens HoloLens Device Specifications.pdf:pdf},
title = {{Microsoft HoloLens HoloLens Device Specifications}},
year = {2015}
}
@article{Arai2011,
abstract = {The distributed multipole analysis procedure, for describing a molecular charge distribution in terms of multipole moments on the individual atoms (or other sites) of the molecule, is not stable with respect to a change of basis set, and indeed, the calculated moments change substantially and unpredictably when the basis set is improved, even though the resulting electrostatic potential changes very little. A revised procedure is proposed, which uses grid-based quadrature for partitioning the contributions to the charge density from diffuse basis functions. The resulting procedure is very stable, and the calculated multipole moments converge rapidly to stable values as the size of the basis is increased.},
author = {Arai, Kohei and Mardiyanto, Ronny},
doi = {10.1021/ct050190+},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arai, Mardiyanto - 2011 - A prototype of electric wheelchair controlled by eye-only for paralyzed user.pdf:pdf},
isbn = {S1549-9618(05)00190-6},
issn = {09153942},
journal = {Journal of Robotics and Mechatronics},
keywords = {Computer input by eye-only,Eye gaze,Hand-free controller,Paralysis,Wheelchair},
number = {1},
pmid = {26631656},
title = {{A prototype of electric wheelchair controlled by eye-only for paralyzed user}},
volume = {23},
year = {2011}
}
@article{Wastlund2010,
abstract = {Individuals with severe multiple disabilities have little or no opportunity to express their own wishes, make choices and move independently. Because of this, the objective of this work has been to develop a prototype for a gaze-driven device to manoeuvre powered wheelchairs or other moving platforms. The prototype has the same capabilities as a normal powered wheelchair, with two exceptions. Firstly, the prototype is controlled by eye movements instead of by a normal joystick. Secondly, the prototype is equipped with a sensor that stops all motion when the machine approaches an obstacle. The prototype has been evaluated in a preliminary clinical test with two users. Both users clearly communicated that they appreciated and had mastered the ability to control a powered wheelchair with their eye movements.},
author = {W{\"{a}}stlund, Erik and Sponseller, Kay and Pettersson, Ola},
doi = {10.1145/1743666.1743699},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/W{\"{a}}stlund, Sponseller, Pettersson - 2010 - What you see is where you go.pdf:pdf},
isbn = {9781605589947},
journal = {Proceedings of the 2010 Symposium on Eye-Tracking Research {\&} Applications - ETRA '10},
keywords = {communication and concentration,eyes-only interaction,facili-,interaction,is motivating and also,move and explore independently,smart wheelchair,the ability to},
number = {January},
pages = {133},
title = {{What you see is where you go}},
url = {http://portal.acm.org/citation.cfm?doid=1743666.1743699},
year = {2010}
}
@inproceedings{Brazil2017,
abstract = {Pedestrian detection is a critical problem in computer vision with significant impact on safety in urban autonomous driving. In this work, we explore how semantic segmentation can be used to boost pedestrian detection accuracy while having little to no impact on network efficiency. We propose a segmentation infusion network to enable joint supervision on semantic segmentation and pedestrian detection. When placed properly, the additional supervision helps guide features in shared layers to become more sophisticated and helpful for the downstream pedestrian detector. Using this approach, we find weakly annotated boxes to be sufficient for considerable performance gains. We provide an in-depth analysis to demonstrate how shared layers are shaped by the segmentation supervision. In doing so, we show that the resulting feature maps become more semantically meaningful and robust to shape and occlusion. Overall, our simultaneous detection and segmentation framework achieves a considerable gain over the state-of-the-art on the Caltech pedestrian dataset, competitive performance on KITTI, and executes 2x faster than competitive methods.},
archivePrefix = {arXiv},
arxivId = {1706.08564},
author = {Brazil, Garrick and Yin, Xi and Liu, Xiaoming},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.530},
eprint = {1706.08564},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brazil, Yin, Liu - 2017 - Illuminating Pedestrians via Simultaneous Detection and Segmentation.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {4960--4969},
title = {{Illuminating Pedestrians via Simultaneous Detection and Segmentation}},
url = {https://arxiv.org/pdf/1706.08564v1.pdf},
volume = {2017-Octob},
year = {2017}
}
@article{Raymond2018,
author = {Raymond, Lou-ann and Piccini, Margherita and Subramanian, Mahendran and Pavel, Orlov and Zito, Giuseppe},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raymond et al. - 2018 - Natural Gaze Data-Driven Wheelchair.pdf:pdf},
title = {{Natural Gaze Data-Driven Wheelchair}},
year = {2018}
}
@article{Fox1997,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fox, Dieter and Burgard, Wolfram and Thrun, Sebastian},
doi = {10.1109/100.580977},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fox, Burgard, Thrun - 1997 - The Dynamic Window Approach to Collision Avoidance.pdf:pdf},
isbn = {1070-9932 VO - 4},
issn = {10709932},
journal = {IEEE Robotics {\&} Automation Magazine ( Volume: 4 , Issue: 1 , Mar 1997 )},
number = {March},
pages = {23--33},
pmid = {568982},
title = {{The Dynamic Window Approach to Collision Avoidance}},
url = {https://ieeexplore.ieee.org/abstract/document/580977/authors{\#}authors},
volume = {4},
year = {1997}
}
@article{Milgram1994,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Milgram, Paul and Kishino, Fumio},
doi = {10.1.1.102.4646},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milgram, Kishino - 1994 - A TAXONOMY OF MIXED REALITY VISUAL DISPLAYS.pdf:pdf},
isbn = {0916-8532},
issn = {0916-8532},
journal = {IEICE Transactions on Information Systems},
number = {12},
pages = {1--15},
pmid = {25246403},
title = {{A TAXONOMY OF MIXED REALITY VISUAL DISPLAYS}},
volume = {E77},
year = {1994}
}
@inproceedings{Hasan2018,
abstract = {In this paper we show the importance of the head pose estimation in the task of trajectory forecasting. This cue, when produced by an oracle and injected in a novel socially-based energy minimization approach, allows to get state-of-the-art performances on four different forecasting benchmarks, without relying on additional information such as expected destination and desired speed, which are supposed to be know beforehand for most of the current forecasting techniques. Our approach uses the head pose estimation for two aims: 1) to define a view frustum of attention , highlighting the people a given subject is more interested about, in order to avoid collisions; 2) to give a short-time estimation of what would be the desired destination point. Moreover, we show that when the head pose estimation is given by a real detector, though the performance decreases, it still remains at the level of the top score forecasting systems.},
author = {Hasan, Irtiza and Setti, Francesco and Tsesmelis, Theodore and {Del Bue}, Alessio and Cristani, Marco and Galasso, Fabio},
booktitle = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
doi = {10.1109/WACV.2018.00134},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasan et al. - 2018 - 'Seeing is Believing' Pedestrian Trajectory Forecasting Using Visual Frustum of Attention.pdf:pdf},
isbn = {9781538648865},
pages = {1178--1185},
title = {{'Seeing is Believing': Pedestrian Trajectory Forecasting Using Visual Frustum of Attention}},
url = {https://irtizahasan.com/WACV{\_}2018{\_}Seeing{\_}is{\_}believing.pdf},
volume = {2018-Janua},
year = {2018}
}
@article{Quattoni2009,
abstract = {Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task.},
author = {Quattoni, Ariadna and Torralba, Antonio},
doi = {10.1109/CVPR.2009.5206537},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quattoni, Torralba - 2009 - Recognizing Indoor Scenes.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {00208868},
journal = {International Surgery},
number = {3},
pages = {182--186},
publisher = {IEEE},
title = {{Recognizing Indoor Scenes}},
volume = {56},
year = {2009}
}
@misc{PupilLabs,
author = {{Pupil Labs}},
booktitle = {Https://Pupil-Labs.Com/Vr-Ar/},
title = {{Pupil Labs - VR AR}},
url = {https://pupil-labs.com/vr-ar/},
urldate = {2019-01-27}
}
@misc{Vidanapathirana,
author = {Vidanapathirana, Madhawa},
title = {{Real-time Human Detection in Computer Vision — Part 2}},
url = {https://medium.com/@madhawavidanapathirana/real-time-human-detection-in-computer-vision-part-2-c7eda27115c6},
urldate = {2019-01-25},
year = {2018}
}
@article{Mather2015,
abstract = {The Population Reference Bureau INFORMS people around the world about population, health, and the environment, and EMPOWERS them to use that information to ADVANCE the well-being of current and future generations.},
author = {Mather, Mark and Jacobsen, Linda A. and Pollard, Kelvin M.},
doi = {10.1093/oxfordhb/9780199640935.013.0031},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mather, Jacobsen, Pollard - 2015 - Aging in the United States.pdf:pdf},
isbn = {9780191749506},
issn = {00987921},
journal = {Population Bulletin},
keywords = {Deregulation,Financial innovation,Large banks,Small banks,Technological change,Us banking industry},
number = {2},
title = {{Aging in the United States}},
volume = {70},
year = {2015}
}
@article{Fransen2010,
abstract = {Human-robot interaction necessitates more than robust people detection and tracking. It relies on the integration of disparate scene information from tracking and recognition systems combined and infused with current and prior knowledge to facililtate robotic understanding and interaction with humans and the environment. In this work we will discuss our efforts in the development and integration of visual scene processing systems for the purpose of enhancing human robotic interaction. Our latest efforts in integrating 3D scene information for the production of novel information sources will be discussed and demonstrated. We show the integration of facial pose and pointing gestures to localize the diectic gesture to a single point in space. Additionally, we will discuss our efforts in integrating Markov logic networks for high level reasoning with computer vision systems to facilitate scene understanding.},
author = {Fransen, Benjamin R. and Lawson, Wallace E. and Bugajska, Magdalena D.},
doi = {10.1109/CVPRW.2010.5543749},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fransen, Lawson, Bugajska - 2010 - Integrating vision for human-robot interaction.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
pages = {9--16},
publisher = {IEEE},
title = {{Integrating vision for human-robot interaction}},
year = {2010}
}
@article{Kairy2014,
abstract = {Power wheelchairs (PWCs) can have a positive impact on user well-being, self-esteem, pain, activity and participation. Newly developed intelligent power wheelchairs (IPWs), allowing autonomous or collaboratively-controlled navigation, could enhance mobility of individuals not able to use, or having difficulty using, standard PWCs. The objective of this study was to explore the perspectives of PWC users (PWUs) and their caregivers regarding if and how IPWs could impact on current challenges faced by PWUs, as well as inform current development of IPWs. A qualitative exploratory study using individual interviews was conducted with PWUs (n = 12) and caregivers (n = 4). A semi-structured interview guide and video were used to facilitate informed discussion regarding IPWs. Thematic analysis revealed three main themes: (1) "challenging situations that may be overcome by an IPW" described how the IPW features of obstacle avoidance, path following, and target following could alleviate PWUs' identified mobility difficulties; (2) "cautious optimism concerning IPW use revealed participants" addresses concerns regarding using an IPW as well as technological suggestions; (3) "defining the potential IPW user" revealed characteristics of PWUs that would benefit from IPW use. Findings indicate how IPW use may help overcome PWC difficulties and confirm the importance of user input in the ongoing development of IPWs.},
author = {Kairy, Dahlia and Rushton, Paula W. and Archambault, Philippe and Pituch, Evelina and Torkia, Caryne and {El Fathi}, Anas and Stone, Paula and Routhier, Fran{\c{c}}ois and Forget, Robert and Demers, Louise and Pineau, Joelle and Gourdeau, Richard},
doi = {10.3390/ijerph110202244},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kairy et al. - 2014 - Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair us.pdf:pdf},
isbn = {1660-4601},
issn = {16617827},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Disability,Intelligent power wheelchair,Mobility,Navigation,Obstacle-avoidance,Path following,Safety,User-centered design},
number = {2},
pages = {2244--2261},
pmid = {24566051},
title = {{Exploring powered wheelchair users and their caregivers' perspectives on potential intelligent power wheelchair use: A qualitative study}},
volume = {11},
year = {2014}
}
@article{Bailey2006a,
author = {Bailey, T and Durrant-Whyte, H},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey, Durrant-Whyte - 2006 - Simultaneous localisation and mapping (SLAM) Part I - The essential algorithms.pdf:pdf},
journal = {IEEE Robotics and Automation Magazine},
number = {2},
pages = {99--108},
title = {{Simultaneous localisation and mapping (SLAM): Part I - The essential algorithms.}},
volume = {13},
year = {2006}
}
@article{Nister2004,
abstract = {DAVID AND HENRIK STEWENIUS Department of Computer Science, Center for Visualization and Virtual Environments, University of Kentucky, Lexington, KY},
author = {Nist{\'{e}}r, David},
doi = {10.1109/CVPR.2004.1315081},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nist{\'{e}}r - 2004 - A Minimal Solution to the Generalised 3-Point Pose Problem.pdf:pdf},
isbn = {0769521584},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
number = {1},
pages = {560--567},
title = {{A Minimal Solution to the Generalised 3-Point Pose Problem}},
volume = {27},
year = {2004}
}
@inproceedings{Leonard,
abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of `which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning},
author = {Leonard, J.J. and Durrant-Whyte, H.F.},
booktitle = {Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91},
doi = {10.1109/IROS.1991.174711},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leonard, Durrant-Whyte - Unknown - Simultaneous map building and localization for an autonomous mobile robot.pdf:pdf},
isbn = {0-7803-0067-X},
issn = {1476-5535},
pages = {1442--1447},
pmid = {21086107},
title = {{Simultaneous map building and localization for an autonomous mobile robot}},
url = {http://ieeexplore.ieee.org/document/174711/}
}
@article{Shinde2018,
abstract = {Human action recognition in video analytics has been widely studied in recent years. Yet, most of these methods assign a single action label to video after either analyzing a complete video or using classifier for each frame. But when compared to human vision strategy, it can be deduced that we (human) require just an instance of visual data for recognition of scene. It turns out that small group of frames or even single frame from the video are enough for precise recognition. In this paper, we present an approach to detect, localize and recognize actions of interest in almost real-time from frames obtained by a continuous stream of video data that can be captured from a surveillance camera. The model takes input frames after a specified period and is able to give action label based on a single frame. Combining results over specific time we predicted the action label for the stream of video. We demonstrate that YOLO is effective method and comparatively fast for recognition and localization in Liris Human Activities dataset.},
author = {Shinde, Shubham and Kothari, Ashwin and Gupta, Vikram},
doi = {10.1016/j.procs.2018.07.112},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shinde, Kothari, Gupta - 2018 - YOLO based Human Action Recognition and Localization.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Convolutional Neural Network,Liris Human Activities dataset,You Only Look Once (YOLO),action label,human action recognition,video analytics},
number = {2018},
pages = {831--838},
publisher = {Elsevier B.V.},
title = {{YOLO based Human Action Recognition and Localization}},
url = {https://doi.org/10.1016/j.procs.2018.07.112},
volume = {133},
year = {2018}
}
@article{Carlson2012,
abstract = {Powered wheelchair users often struggle to drive safely and effectively and, in more critical cases, can only get around when accompanied by an assistant. To address these issues, we propose a collaborative control mechanism that assists users as and when they require help. The system uses a multiple-hypothesis method to predict the driver's intentions and, if necessary, adjusts the control signals to achieve the desired goal safely. The main emphasis of this paper is on a comprehensive evaluation, where we not only look at the system performance but also, perhaps more importantly, characterize the user performance in an experiment that combines eye tracking with a secondary task. Without assistance, participants experienced multiple collisions while driving around the predefined route. Conversely, when they were assisted by the collaborative controller, not only did they drive more safely but also they were able to pay less attention to their driving, resulting in a reduced cognitive workload. We discuss the importance of these results and their implications for other applications of shared control, such as brain-machine interfaces, where it could be used to compensate for both the low frequency and the low resolution of the user input.},
author = {Carlson, Tom and Demiris, Yiannis},
doi = {10.1109/TSMCB.2011.2181833},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlson, Demiris - 2012 - Collaborative control for a robotic wheelchair Evaluation of performance, attention, and workload.pdf:pdf},
isbn = {1083-4419 VO  - 42},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Collision avoidance,human factors,human robot interaction,intelligent robots,rehabilitation robotics,wheelchairs},
number = {3},
pages = {876--888},
pmid = {22275718},
title = {{Collaborative control for a robotic wheelchair: Evaluation of performance, attention, and workload}},
volume = {42},
year = {2012}
}
@article{Wang2011,
abstract = {This paper presents algorithms for improving the detection of moving objects in robot visual simultaneous localization and mapping (SLAM). The method of speeded-up robust feature (SURF) is employed in the algorithm to provide a robust detection for image features as well as a better description of landmarks in the map of a visual SLAM system. Meanwhile, a moving object detection (MOD) is designed based on the correspondence constraint of the essential matrix for the feature points on image plane. Experiments are carried out on a handheld camera sensor to verify the performances of the proposed algorithms. The results show that the integration of SURF and MOD is efficient to improve the robustness of robot SLAM.},
author = {Wang, Yin Tien and Feng, Ying Chieh and Hung, Duen Yan},
doi = {10.1109/IMTC.2011.5944059},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Feng, Hung - 2011 - Detection and tracking of moving objects in SLAM using vision sensors.pdf:pdf},
isbn = {9781424479351},
issn = {10915281},
journal = {Conference Record - IEEE Instrumentation and Measurement Technology Conference},
keywords = {Moving Object Detection (MOD),Simultaneous Localization and Mapping (SLAM),Speeded Up Robust Features (SURF),Vision Sensor},
pages = {1078--1082},
publisher = {IEEE},
title = {{Detection and tracking of moving objects in SLAM using vision sensors}},
year = {2011}
}
@article{Rosslin2010,
abstract = {Smart Home technology started for more than a decade to introduce the concept of networking devices and equipment in the house. According to the Smart Homes Association the best definition of smart home technology is: the integration of technology and services through home networking for a better quality of living. Many tools that are used in computer systems can also be integrated in Smart Home Systems. In this paper, we present the Technologies and tools that can be integrated or applied in smart home systems},
author = {Rosslin, John Robles and Tai-hoon, Kim},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosslin, Tai-hoon - 2010 - Applications, Systems and Methods in Smart Home Technology A Review.pdf:pdf},
journal = {International Journal of Advanced Science and Technology},
keywords = {automation,smart home,ubiquitous,x10},
number = {January 2010},
pages = {37--48},
title = {{Applications, Systems and Methods in Smart Home Technology : A Review}},
volume = {15},
year = {2010}
}
@article{Taketomi2017,
abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
doi = {10.1186/s41074-017-0027-2},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taketomi, Uchiyama, Ikeda - 2017 - Visual SLAM algorithms a survey from 2010 to 2016.pdf:pdf},
isbn = {81-7319-221-9},
issn = {1882-6695},
journal = {IPSJ Transactions on Computer Vision and Applications},
keywords = {augmented reality,computer vision,robotics,survey,visual slam},
number = {1},
pages = {16},
publisher = {IPSJ Transactions on Computer Vision and Applications},
title = {{Visual SLAM algorithms: a survey from 2010 to 2016}},
url = {http://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2},
volume = {9},
year = {2017}
}
@article{WorldHealthOrganisation&TheWorldbank2011,
abstract = {http://www.larchetoronto.org/wordpress/wp-content/uploads/2012/01/launch-of-World-Report-on-Disability-Jan-27-121.pdf},
archivePrefix = {arXiv},
arxivId = {NBK304079},
author = {{World Health Organisation {\&} The World bank}},
doi = {10.1080/09687599.2011.589198},
eprint = {NBK304079},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/World Health Organisation {\&} The World bank - 2011 - The world report on disability.pdf:pdf},
isbn = {9241564180},
issn = {09687599},
journal = {Disability and Society},
keywords = {Convention on the rights of persons with disabilit,Disability data,Disability politics,World health organization,World report on disability},
number = {5},
pages = {655--658},
pmid = {21780912},
title = {{The world report on disability}},
volume = {26},
year = {2011}
}
@article{Thaler2013,
abstract = {People can direct their gaze at a visual target for extended periods of time. Yet, even during fixation the eyes make small, involuntary movements (e.g. tremor, drift, and microsaccades). This can be a problem during experiments that require stable fixation. The shape of a fixation target can be easily manipulated in the context of many experimental paradigms. Thus, from a purely methodological point of view, it would be good to know if there was a particular shape of a fixation target that minimizes involuntary eye movements during fixation, because this shape could then be used in experiments that require stable fixation. Based on this methodological motivation, the current experiments tested if the shape of a fixation target can be used to reduce eye movements during fixation. In two separate experiments subjects directed their gaze at a fixation target for 17. s on each trial. The shape of the fixation target varied from trial to trial and was drawn from a set of seven shapes, the use of which has been frequently reported in the literature. To determine stability of fixation we computed spatial dispersion and microsaccade rate. We found that only a target shape which looks like a combination of bulls eye and cross hair resulted in combined low dispersion and microsaccade rate. We recommend the combination of bulls eye and cross hair as fixation target shape for experiments that require stable fixation. {\textcopyright} 2012 Elsevier Ltd.},
author = {Thaler, L and Sch{\"{u}}tz, A C and Goodale, M A and Gegenfurtner, K R},
doi = {10.1016/j.visres.2012.10.012},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thaler et al. - 2013 - What is the best fixation target The effect of target shape on stability of fixational eye movements(2).pdf:pdf},
issn = {00426989},
journal = {Vision Research},
keywords = {Dispersion,Drift,Eye movements,Microsaccade,Ocular fixation,Slow control},
pages = {31--42},
pmid = {23099046},
title = {{What is the best fixation target? The effect of target shape on stability of fixational eye movements}},
url = {http://dx.doi.org/10.1016/j.visres.2012.10.012},
volume = {76},
year = {2013}
}
@article{Thrun2005,
abstract = {Probabilistic robotics is a new and growing area in robotics, concerned with perception and control in the face of uncertainty. Building on the field of mathematical statistics, probabilistic robotics endows robots with a new level of robustness in real-world situations.{\textless}br {\textless}br This book introduces the reader to a wealth of techniques and algorithms in the field. All algorithms are based on a single overarching mathematical foundation. Each chapter provides example implementations in pseudo code, detailed mathematical derivations, discussions from a practitioner's perspective, and extensive lists of exercises and class projects. The book's Web site, http://www.probabilistic-robotics.org, has additional material.{\textless}br {\textless}br The book is relevant for anyone involved in robotic software development and scientific research. It will also be of interest to applied statisticians and engineers dealing with real-world sensor data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
doi = {10.1145/504729.504754},
eprint = {arXiv:1011.1669v3},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun, Burgard, Fox - 2005 - Probabilistic robotics (intelligent robotics and autonomous agents series).pdf:pdf},
isbn = {0262201623},
issn = {00010782},
journal = {Intelligent robotics and autonomous agents, The MIT {\ldots}},
number = {3},
pages = {52},
pmid = {20926156},
title = {{Probabilistic robotics (intelligent robotics and autonomous agents series)}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Probabilistic+Robotics+(Intelligent+Robotics+and+Autonomous+Agents){\#}0},
volume = {45},
year = {2005}
}
@article{Espinace2010,
abstract = {Scene recognition is a highly valuable perceptual ability for an indoor$\backslash$nmobile robot, however, current approaches for scene recognition present$\backslash$na significant drop in performance for the case of indoor scenes.$\backslash$nWe believe that this can be explained by the high appearance variability$\backslash$nof indoor environments. This stresses the need to include high-level$\backslash$nsemantic information in the recognition process. In this work we$\backslash$npropose a new approach for indoor scene recognition based on a generative$\backslash$nprobabilistic hierarchical model that uses common objects as an intermediate$\backslash$nsemantic representation. Under this model, we use object classifiers$\backslash$nto associate low-level visual features to objects, and at the same$\backslash$ntime, we use contextual relations to associate objects to scenes.$\backslash$nAs a further contribution, we improve the performance of current$\backslash$nstate-of-the-art category-level object classifiers by including geometrical$\backslash$ninformation obtained from a 3D range sensor that facilitates the$\backslash$nimplementation of a focus of attention mechanism within a Monte Carlo$\backslash$nsampling scheme. We test our approach using real data, showing significant$\backslash$nadvantages with respect to previous state-of-the-art methods.},
author = {Espinace, P. and Kollar, T. and Soto, A. and Roy, N.},
doi = {10.1109/ROBOT.2010.5509682},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Espinace et al. - 2010 - Indoor scene recognition through object detection.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1406--1413},
publisher = {IEEE},
title = {{Indoor scene recognition through object detection}},
year = {2010}
}
@inproceedings{Wang2013,
abstract = {Under the structure of robot technology middleware(RTM), this paper presents a distributed method for mobile robot simultaneous localization and mapping(SLAM) to address the problem of 3D modeling in complex indoor environment.We integrate the image feature and depth information to establish the correspondence-based iterative closest point (ICP) algorithm for localizing the robot precisely. With the introduction of keyframe selection mechanism, a vision-based loop closure detect algorithm and tree-based network optimizer(TORO) are used to efficiently achieve globally consistent and accuracy maps during the map building. Experimental results verify the feasibility and effectiveness of the proposed algorithm in the indoor environment. {\textcopyright} 2013 IEEE.},
author = {Wang, Ke and Jia, Songmin and Guo, Bing and Li, Yuchen},
booktitle = {2013 IEEE International Conference on Information and Automation, ICIA 2013},
doi = {10.1109/ICInfA.2013.6720481},
file = {:home/aufar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2013 - Mobile robot 3D map building based on RTM.pdf:pdf},
isbn = {9781479913343},
keywords = {3D map building,RTM,SLAM,keyframe,loop closure,mobile robot},
pages = {1224--1229},
title = {{Mobile robot 3D map building based on RTM}},
year = {2013}
}
